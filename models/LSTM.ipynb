{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b680a614",
   "metadata": {},
   "source": [
    "# LSTM "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdb1a44",
   "metadata": {},
   "source": [
    "## Outline\n",
    "1. Imports\n",
    "2. Data\n",
    "3. Model\n",
    "5. Train\n",
    "6. Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b23d55",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0baed98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc101f4",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7846f422",
   "metadata": {},
   "source": [
    "### Ingestion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d4e3df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent Directory /deep-co-training\n",
      "Path: /deep-co-training/cleaned_datasets/india/daily_cases_india.csv\n",
      "Exists: True\n"
     ]
    }
   ],
   "source": [
    "# Get the path of the current working directory\n",
    "curPath = os.getcwd()\n",
    "# Appened the parent directory to the current path to step out of the current folder\n",
    "parentDir = os.path.abspath(os.path.join(curPath, os.pardir))\n",
    "print(\"Parent Directory\", parentDir)\n",
    "# Save the path to all of the datasets\n",
    "india_cases_path = os.path.join(parentDir, \"cleaned_datasets/india/daily_cases_india.csv\")\n",
    "india_vacc_path = os.path.join(parentDir, \"cleaned_datasets/india/daily_vacc_india.csv\")\n",
    "usa_cases_path = os.path.join(parentDir, \"cleaned_datasets/usa/daily_cases_usa.csv\")\n",
    "usa_vacc_path = os.path.join(parentDir, \"cleaned_datasets/usa/vacc_usa.csv\")\n",
    "\n",
    "# Quick check to make sure the path exists\n",
    "print(\"Path:\", india_cases_path)\n",
    "print(\"Exists:\", os.path.exists(india_cases_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8bb50f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India Cases:\n",
      "          Date  Confirmed  Deaths  Recovered  Active\n",
      "0  2020-01-30        NaN     NaN        NaN     NaN\n",
      "1  2020-01-31        0.0     0.0        0.0     0.0\n",
      "2  2020-02-01        0.0     0.0        0.0     0.0\n",
      "3  2020-02-02        1.0     0.0        0.0     0.0\n",
      "4  2020-02-03        1.0     0.0        0.0     0.0 \n",
      "\n",
      "India Vacc:\n",
      "    Updated On  Total_Doses  First_Dose  Second_Dose\n",
      "0  2021-01-16          NaN         NaN          NaN\n",
      "1  2021-01-17      20656.0     20656.0          0.0\n",
      "2  2021-01-18      81690.0     81690.0          0.0\n",
      "3  2021-01-19     192152.0    192152.0          0.0\n",
      "4  2021-01-20     111510.0    111510.0          0.0 \n",
      "\n",
      "USA Cases:\n",
      "          Date  Confirmed  Deaths  Recovered\n",
      "0  2020-04-12        NaN     NaN        NaN\n",
      "1  2020-04-13    25322.0  1546.0    11785.0\n",
      "2  2020-04-14    26713.0  2305.0     6484.0\n",
      "3  2020-04-15    29380.0  2478.0     6093.0\n",
      "4  2020-04-16    31542.0  4616.0     5234.0 \n",
      "\n",
      "USA Vacc:\n",
      "          date  total_doses  people_vacc  people_fully_vacc  daily_vacc\n",
      "0  2020-12-20     556208.0          0.0                0.0         0.0\n",
      "1  2020-12-21     614117.0          0.0                0.0     57909.0\n",
      "2  2020-12-22          0.0          0.0                0.0    127432.0\n",
      "3  2020-12-23    1008025.0          0.0                0.0    150606.0\n",
      "4  2020-12-24          0.0          0.0                0.0    191001.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the data as a pandas dataframe\n",
    "india_cases_df = pd.read_csv(india_cases_path)\n",
    "india_vacc_df =  pd.read_csv(india_vacc_path)\n",
    "\n",
    "usa_cases_df = pd.read_csv(usa_cases_path)\n",
    "usa_vacc_df = pd.read_csv(usa_vacc_path)\n",
    "\n",
    "# Visualize the datasets\n",
    "print('India Cases:\\n',india_cases_df.head(),'\\n')\n",
    "print('India Vacc:\\n',india_vacc_df.head(),'\\n')\n",
    "\n",
    "print('USA Cases:\\n',usa_cases_df.head(),'\\n')\n",
    "print('USA Vacc:\\n',usa_vacc_df.head(),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01edd055",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "438c9fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India Cases:\n",
      "    Confirmed\n",
      "1        0.0\n",
      "2        0.0\n",
      "3        1.0\n",
      "4        1.0\n",
      "5        0.0 \n",
      "\n",
      "USA Cases:\n",
      "    Confirmed\n",
      "1    25322.0\n",
      "2    26713.0\n",
      "3    29380.0\n",
      "4    31542.0\n",
      "5    32022.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select only the Confirmed column for univariate analysis\n",
    "# Selecting from the first index because the 0th index is NaN\n",
    "india_cases_df = india_cases_df[[\"Confirmed\"]][1:]\n",
    "usa_cases_df = usa_cases_df[[\"Confirmed\"]][1:]\n",
    "\n",
    "# Visualize the datasets\n",
    "print('India Cases:\\n',india_cases_df.head(),'\\n')\n",
    "print('USA Cases:\\n',usa_cases_df.head(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55483a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India Cases:\n",
      "    Confirmed\n",
      "1  -0.687995\n",
      "2  -0.687995\n",
      "3  -0.687983\n",
      "4  -0.687983\n",
      "5  -0.687995 \n",
      "\n",
      "USA Cases:\n",
      "    Confirmed\n",
      "1  -0.817861\n",
      "2  -0.797170\n",
      "3  -0.757499\n",
      "4  -0.725340\n",
      "5  -0.718200 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalize the data\n",
    "india_cases_mean = india_cases_df.mean()\n",
    "india_cases_std = india_cases_df.std()\n",
    "\n",
    "usa_cases_mean = usa_cases_df.mean()\n",
    "usa_cases_std = usa_cases_df.std()\n",
    "\n",
    "\n",
    "india_cases_df = (india_cases_df-india_cases_mean)/india_cases_std\n",
    "usa_cases_df = (usa_cases_df-usa_cases_mean)/usa_cases_std\n",
    "\n",
    "# Visualize the datasets\n",
    "print('India Cases:\\n',india_cases_df.head(),'\\n')\n",
    "print('USA Cases:\\n',usa_cases_df.head(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b5cc363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India Cases:\n",
      "      Confirmed\n",
      "113  -0.605466\n",
      "118  -0.596356\n",
      "420   0.054845\n",
      "124  -0.577156\n",
      "355  -0.514392\n",
      "..         ...\n",
      "55   -0.686475\n",
      "271  -0.136463\n",
      "386  -0.512168\n",
      "260   0.093723\n",
      "489   0.997701\n",
      "\n",
      "[496 rows x 1 columns] \n",
      "\n",
      "USA Cases:\n",
      "      Confirmed\n",
      "528   0.775194\n",
      "493   1.165165\n",
      "23   -0.836469\n",
      "114  -0.366324\n",
      "440  -1.066016\n",
      "..         ...\n",
      "239   1.654677\n",
      "82   -0.385512\n",
      "166  -0.403525\n",
      "250   2.501734\n",
      "302   0.083444\n",
      "\n",
      "[438 rows x 1 columns] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create train test splits\n",
    "india_cases_train, india_cases_test = train_test_split(india_cases_df, test_size=0.2)\n",
    "india_vacc_train, india_vacc_test = train_test_split(india_vacc_df, test_size=0.2)\n",
    "\n",
    "usa_cases_train, usa_cases_test = train_test_split(usa_cases_df, test_size=0.2)\n",
    "usa_vacc_train, usa_vacc_test = train_test_split(usa_vacc_df, test_size=0.2)\n",
    "\n",
    "# Visualize splits\n",
    "print('India Cases:\\n',india_cases_train,'\\n')\n",
    "print('USA Cases:\\n',usa_cases_train,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01face15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India Cases:\n",
      " [-6.05465670e-01 -5.96355767e-01  5.48452939e-02 -5.77155859e-01\n",
      " -5.14391762e-01 -3.47837589e-01 -6.87982552e-01 -1.93346184e-01\n",
      " -1.52483553e-01 -6.87995117e-01  3.71668903e-01 -6.87982552e-01\n",
      " -4.55259062e-01 -3.60779935e-01 -6.87831767e-01 -6.64460151e-01\n",
      " -6.87995117e-01 -5.77645909e-01  3.62374124e+00  4.05419526e-01\n",
      " -4.54291527e-01  9.05157440e-01  3.91714297e+00 -2.45040177e-01\n",
      "  3.74747258e+00 -3.46518223e-01  6.13250989e-01 -1.08127745e-01\n",
      " -1.33723434e-01 -3.81977739e-01 -4.80616008e-01  8.94377979e-02\n",
      " -2.24747080e-01 -4.13793293e-01 -3.01320535e-01  8.18483055e-02\n",
      " -2.68612838e-01  3.40955169e+00  1.77759887e-01  4.36369708e-02\n",
      "  2.26100014e+00 -6.87995117e-01  1.21203090e-01 -1.68001803e-01\n",
      "  4.71676799e-01 -1.60688749e-01  1.82019552e-01 -5.63736028e-01\n",
      " -1.68768292e-01 -2.99209550e-01 -6.57913586e-01  3.66027045e-01\n",
      "  1.49752737e+00 -2.02619438e-01 -4.51074789e-01 -6.87366848e-01\n",
      " -3.77479331e-01 -1.44779333e-02 -6.80393059e-01 -1.18129791e-01\n",
      " -2.55708188e-01 -1.02234580e-01 -5.08762470e-01 -6.52736648e-01\n",
      "  3.98056211e-01 -2.08449776e-01 -6.81146982e-01 -3.18208412e-01\n",
      " -2.29735538e-01  3.02647245e-01 -5.68988359e-01  1.08285875e-01\n",
      " -2.27448638e-01 -6.87957421e-01 -6.42923082e-01 -1.48827026e-01\n",
      " -2.35503050e-01  1.62870999e+00 -1.11206265e-01 -6.87517632e-01\n",
      " -6.70780540e-01 -5.73524463e-01  1.49574308e+00 -5.52526066e-02\n",
      " -3.54937031e-01 -4.97704932e-01 -4.06140973e-01 -4.62810858e-01\n",
      "  3.71657976e-02 -6.87969986e-01 -6.87995117e-01 -6.85984655e-01\n",
      " -3.47297277e-01 -6.81297767e-01 -3.75745308e-01 -1.06808380e-01\n",
      " -1.27038649e-01 -3.67992466e-01  1.23116634e+00 -2.41283127e-01\n",
      " -4.77499793e-01 -3.83309670e-01 -4.69269466e-01 -6.72313517e-01\n",
      " -8.60503644e-02  3.59216607e-01 -6.65364859e-01 -3.04147746e-01\n",
      "  2.62339840e+00 -5.09579220e-01 -4.51351227e-01  1.83177206e-02\n",
      " -1.60060480e-01  4.75622330e-01 -2.86870342e-01  3.26562778e+00\n",
      " -4.94324843e-01 -6.87995117e-01 -4.92100770e-01  2.96590730e-01\n",
      " -1.63603919e-01  1.90023702e-01  1.23138160e-01 -1.46791434e-01\n",
      " -5.81918140e-01 -3.56495139e-01  1.53873090e-01  9.70233567e-01\n",
      "  2.67115943e+00 -2.06615230e-01 -1.11470138e-01 -2.91921627e-01\n",
      " -5.83250070e-01 -3.71774646e-01 -3.03871308e-01  4.51643643e+00\n",
      " -1.73805372e-02  1.92098630e-02 -1.83871884e-01  2.04312894e+00\n",
      "  2.10547838e+00 -4.80817055e-01 -6.73042309e-01 -2.23815603e-02\n",
      "  1.96704553e+00 -4.60159562e-01 -2.30602550e-01 -3.80243716e-01\n",
      " -3.57638589e-01  2.33827726e+00 -4.36423550e-01  1.62781948e-01\n",
      " -5.44020939e-01 -6.77503021e-01 -5.26140397e-01  7.66272243e-01\n",
      " -1.62372511e-01 -4.76419170e-01 -6.40711574e-01 -1.07072253e-01\n",
      " -6.87819202e-01 -4.24725177e-01 -3.86300231e-01  2.46655891e-01\n",
      "  1.93717761e+00 -1.21509880e-01 -6.40410005e-01  2.54506579e+00\n",
      "  4.50830826e-01 -1.26850168e-01  4.84820192e-01 -6.80443321e-01\n",
      " -5.23614755e-01 -1.11344484e-01 -1.59983449e-02  4.02944145e-01\n",
      " -5.38102643e-01 -3.92344181e-01 -6.68631859e-01 -5.15321601e-01\n",
      " -4.00976600e-01 -6.87919725e-01 -6.87706113e-01  3.59291999e-01\n",
      "  3.02077869e+00  7.80284286e-02 -1.93409011e-01 -5.22219997e-01\n",
      "  3.44502541e-01 -3.72955793e-01 -1.10603126e-01 -5.66952766e-01\n",
      " -6.81637032e-01  5.30621019e-01  1.16566463e-01  1.65374023e+00\n",
      " -2.28787125e-04  5.42080650e-01 -1.70841580e-01 -6.33297997e-01\n",
      " -6.87856898e-01 -2.28051777e-01 -7.11341602e-01 -2.28466434e-01\n",
      " -1.46653215e-01 -3.18912073e-01 -2.39888369e-01 -1.71557807e-01\n",
      "  3.22197563e+00 -6.87995117e-01 -6.45725163e-01 -2.45970015e-01\n",
      " -2.41936527e-01  3.80181787e+00 -6.87995117e-01 -1.29375811e-01\n",
      " -6.87995117e-01 -5.41620951e-01  4.68912415e-01 -5.01763551e-01\n",
      "  4.31945053e-01 -6.49256036e-01 -6.76409832e-01 -6.83959989e-02\n",
      "  3.99011180e-01 -5.29872316e-01  9.80537182e-01 -2.48520788e-01\n",
      " -6.82504044e-01 -2.15624611e-01  1.10019898e-01 -4.81520716e-01\n",
      " -4.99652566e-01 -4.10739904e-01 -1.37053261e-01  3.49528531e+00\n",
      " -6.87995117e-01 -2.74518569e-01 -6.86738578e-01  2.47397249e-01\n",
      "  4.16921715e+00  5.53552846e-01 -8.59733331e-03  2.32695748e-01\n",
      " -5.48934005e-01 -5.08146766e-01  3.69004877e+00 -5.90475167e-01\n",
      " -5.49285836e-01  2.84772821e+00 -4.02094920e-01 -1.13505730e-01\n",
      " -4.82236943e-01 -4.50886308e-01 -2.21417253e-01 -4.59732339e-01\n",
      " -6.87856898e-01 -6.48879074e-01  9.70901171e-02 -4.94638978e-01\n",
      " -6.87995117e-01 -5.24092239e-01  2.82881895e-01 -1.62636384e-01\n",
      " -5.40816766e-01  3.32721674e-03 -5.92232681e-02 -7.60860144e-02\n",
      " -2.96985477e-01 -4.77675708e-01 -6.87995117e-01 -6.87693548e-01\n",
      " -6.77339671e-01 -1.70364096e-01 -6.10114863e-01 -1.50623876e-01\n",
      " -1.76395480e-01  1.78018570e+00 -1.38473149e-01 -3.28738204e-01\n",
      "  5.14951984e-01 -6.87995117e-01 -4.83041128e-01  5.25205338e-01\n",
      " -2.81993334e-02 -2.28164865e-01 -5.01411720e-01 -6.87995117e-01\n",
      "  2.17328283e-01 -1.00060768e-01 -6.87743809e-01 -4.87351055e-01\n",
      " -5.22094343e-01 -6.85142775e-01 -6.71785771e-01 -2.73576165e-01\n",
      " -1.31587318e-01 -6.67802544e-01 -4.52620331e-01 -3.53454316e-01\n",
      "  2.59652104e+00 -4.60071604e-01 -6.87982552e-01 -5.25612651e-01\n",
      " -1.47909753e-01 -6.87530198e-01  3.93682036e+00 -4.37868570e-01\n",
      " -1.19650203e-01  3.86980917e+00 -5.56837632e-01 -2.02129388e-01\n",
      " -2.93668215e-01 -6.66546005e-01 -5.34772816e-01 -7.33467605e-02\n",
      "  9.43006017e-02  8.36200248e-02  9.78189094e-02 -6.87969986e-01\n",
      "  4.38094389e+00 -3.07841969e-01 -3.95410135e-01 -1.52207115e-01\n",
      " -6.75002509e-01  8.26498132e-01  4.24376758e+00  2.56853793e+00\n",
      " -4.42266454e-01  1.21732475e-02  4.04703299e-01  4.11381637e+00\n",
      " -6.87995117e-01 -1.98598515e-01 -6.87995117e-01  1.13881077e+00\n",
      " -1.41287795e-01 -4.86541407e-03 -1.50623876e-01 -5.36469143e-01\n",
      " -3.30962277e-01 -1.39465815e-01 -6.66156478e-01  2.41114556e-01\n",
      " -6.87995117e-01  1.83436764e+00 -7.71792028e-02 -6.74072671e-01\n",
      " -4.75363678e-01 -3.14941412e-01  2.98538364e-01 -1.96512661e-01\n",
      "  3.10464553e-02  1.39224201e+00 -5.42111001e-01 -6.87995117e-01\n",
      " -5.99646258e-02  4.52966942e-01 -5.00356228e-01 -6.57310448e-01\n",
      " -6.50763882e-01 -3.24817804e-01 -6.86700882e-01  3.37091313e+00\n",
      " -4.00825816e-01 -3.69839577e-01 -8.14891298e-02 -2.09517834e-01\n",
      " -6.87995117e-01 -5.05231597e-01 -5.17281801e-01 -3.10644050e-01\n",
      " -6.87995117e-01 -4.52343893e-01  2.55451660e-01 -5.28552951e-01\n",
      " -1.64659411e-01 -4.58249624e-01 -5.70871527e-02 -4.75062108e-01\n",
      " -2.34598342e-01 -5.98617536e-01 -5.59602016e-01 -6.87995117e-01\n",
      "  4.49435905e+00 -1.97065538e-01 -6.77829721e-01  7.81792132e-02\n",
      " -6.87919725e-01 -6.26877086e-01 -5.44297378e-01  5.23609534e-01\n",
      "  5.76534935e-01 -4.26647681e-01 -3.82191350e-01  1.23298833e+00\n",
      " -3.45538123e-01 -5.71023951e-01 -1.60486064e-02 -4.89173035e-01\n",
      " -3.54170542e-01  3.45785303e+00  3.93281365e-01  2.98689149e-01\n",
      " -1.66456261e-01 -2.13777500e-01 -3.33601008e-01  3.84719148e+00\n",
      "  4.97825365e-01 -5.79493020e-01 -5.25914220e-01  8.52409594e-02\n",
      " -1.07926699e-01 -1.73404919e-01 -3.68784085e-01 -6.07400740e-01\n",
      " -4.47393131e-01 -1.00450295e-01  7.53896978e-02 -1.24073218e-01\n",
      " -5.55003086e-01  1.34427994e+00 -9.41299067e-02  9.14217082e-01\n",
      " -6.14575574e-01 -8.91146793e-03 -3.87054154e-01 -5.41407339e-01\n",
      " -6.87932290e-01 -6.87115540e-01 -6.87995117e-01 -1.15315145e-01\n",
      "  3.70600681e+00  2.57336304e+00 -5.35413651e-01 -5.61713001e-01\n",
      " -3.09864996e-01 -3.39668450e-02 -5.50102586e-01 -1.14083738e-01\n",
      " -1.99641442e-01 -4.90704373e-02 -3.75343216e-01 -4.79698735e-01\n",
      "  1.97814241e-01 -5.33616801e-01 -4.32339800e-01 -2.37262204e-01\n",
      " -1.30393607e-01 -6.04699182e-01 -5.73386243e-01 -5.86152674e-01\n",
      " -6.87882028e-01 -4.97529016e-01 -4.09433104e-01 -4.08402743e-01\n",
      "  2.64574129e-01  3.01667145e-01 -1.28973718e-01 -6.69737613e-01\n",
      " -2.47564180e-02 -3.87793873e-02 -2.84897577e-01 -4.70664224e-01\n",
      " -6.87995117e-01  2.72904979e-01 -2.09593226e-01 -6.45976471e-01\n",
      "  3.27715187e-01 -3.59850096e-01  3.88694999e-01 -5.81049489e-02\n",
      " -5.26140397e-01 -2.55029657e-01 -3.98149389e-01 -6.24539924e-01\n",
      " -1.32228153e-01 -6.18219536e-01 -5.96267809e-01 -6.81523944e-01\n",
      "  1.87460364e-01  1.90589144e-01  1.43444914e+00 -7.67142836e-02\n",
      " -9.78869568e-02 -4.96448393e-01 -3.15858685e-01  3.23166518e-01\n",
      " -6.38588024e-01  9.18629171e-02 -6.87165802e-01 -6.86474705e-01\n",
      " -1.36462688e-01 -5.12167689e-01  9.37225940e-02  9.97701498e-01] \n",
      "\n",
      "USA Cases:\n",
      " [ 7.75193522e-01  1.16516453e+00 -8.36469462e-01 -3.66323619e-01\n",
      " -1.06601642e+00 -3.02972110e-01 -4.97503933e-01 -9.97116632e-01\n",
      " -5.75551683e-01 -7.03950421e-01 -6.60322815e-01  2.23446482e+00\n",
      "  9.57870179e-01 -5.64514628e-01  1.65626760e-01 -1.33816590e-01\n",
      " -8.03789663e-01  4.67627289e-02 -5.00716877e-01 -5.03096835e-01\n",
      " -1.85179060e-01 -6.76610653e-01 -2.60668355e-01  2.29872368e+00\n",
      " -1.79437411e-01  1.85448963e+00  9.31155149e-01  2.07860731e+00\n",
      "  1.32239051e+00 -7.97170404e-01  1.80686072e+00 -5.73841088e-01\n",
      " -8.64389345e-01 -4.87493235e-01 -8.76839501e-01  2.33842436e+00\n",
      "  1.14988817e+00 -6.02370836e-01  2.14497839e+00  1.96932261e+00\n",
      " -8.07880216e-01 -3.36291523e-01 -1.03393161e+00 -6.15609353e-01\n",
      " -8.79264083e-01 -3.23857518e-02  7.33722752e-01 -1.84807192e-01\n",
      " -7.77149007e-01  1.61882916e+00 -1.02060384e+00  2.08858826e+00\n",
      "  4.51578723e-01  1.82595989e+00 -1.00904617e+00 -7.08011225e-01\n",
      " -4.34851537e-01 -9.13907348e-01 -8.93023216e-01 -9.23888297e-01\n",
      " -1.01083114e+00 -9.43241607e-02 -4.44966359e-01 -5.50710871e-01\n",
      "  1.79841187e+00 -2.73416006e-01  9.52678895e-01 -4.21137028e-01\n",
      "  1.42083152e+00 -8.67096547e-01  1.65060160e+00 -7.56978862e-01\n",
      " -7.51430585e-01 -3.72883379e-01 -1.76878956e-01 -4.87136241e-01\n",
      "  4.52486082e-01 -3.14024040e-01 -5.68025066e-01 -3.10379730e-01\n",
      " -6.78038628e-01 -3.50244027e-01 -3.33674845e-02 -4.90527681e-01\n",
      "  1.00725431e+00 -3.11584583e-01  2.15860365e+00 -4.30850232e-01\n",
      " -8.81867162e-01 -7.57499478e-01 -4.39358583e-01 -6.85832991e-01\n",
      "  2.46024719e-01 -9.06767474e-01 -4.25078834e-01 -8.28318106e-01\n",
      "  1.28591765e+00  2.46752221e+00 -1.09484366e+00 -3.41378684e-01\n",
      " -3.47908693e-01  2.24486226e+00  8.99114964e-01  1.49996513e+00\n",
      " -7.99401615e-01 -5.10623452e-01  2.10407286e+00 -9.13327233e-01\n",
      " -5.22181123e-01 -6.70110393e-01 -4.49458530e-01 -3.14009166e-01\n",
      "  8.52110792e-01  1.80541787e+00 -2.50598158e-01 -5.09284726e-01\n",
      " -4.36279512e-01 -2.35946541e-01  9.62734218e-01 -6.95620568e-01\n",
      " -5.95245836e-01  5.04532790e-01 -8.58082456e-01 -5.11203567e-01\n",
      "  1.79744501e+00 -3.24361983e-01  1.62208673e+00 -2.46195235e-01\n",
      " -5.42291769e-01 -7.77506001e-01 -8.19452762e-01  1.38490903e+00\n",
      " -5.54474180e-01  1.14185581e+00  1.28325507e+00 -2.78131298e-01\n",
      " -6.61126051e-01  2.94189120e-01  1.30385658e+00 -5.06294903e-01\n",
      " -8.73537309e-01 -5.92644033e-02  2.09958069e+00 -2.64119295e-01\n",
      "  1.50067784e-01 -6.11340303e-01 -5.19191301e-01 -2.13247691e-01\n",
      " -2.05602075e-01 -9.89828010e-01 -9.42972586e-01 -5.07068390e-01\n",
      "  6.01099589e-01 -8.57204847e-01 -6.64115873e-01  2.38184372e+00\n",
      " -8.34654744e-01  6.49189616e-01  4.50818835e-02 -4.22625778e-02\n",
      " -4.81736711e-01 -9.77734848e-01 -8.72555576e-01 -1.73234646e-01\n",
      " -7.87949342e-02 -9.22638819e-01 -6.53108567e-01 -1.79496910e-01\n",
      " -1.12591699e+00 -6.83988523e-01 -3.66293870e-01 -8.25834024e-01\n",
      " -6.24325949e-01 -1.11781026e+00 -8.52400306e-01 -3.78089537e-01\n",
      "  1.34684458e+00 -4.85827264e-01 -3.86553263e-01 -7.29817591e-01\n",
      " -7.06434503e-01  9.60116264e-01 -4.58591619e-01  1.52238136e+00\n",
      " -6.51011229e-01  2.92619963e+00 -1.00126668e+00 -3.19393821e-01\n",
      " -3.96653210e-01 -7.59329071e-01 -3.14232287e-01  6.39045045e-01\n",
      " -6.93746351e-01 -1.24624002e-01 -3.14157913e-01 -5.68471308e-01\n",
      " -6.19238789e-01 -1.00785619e+00  1.19635685e+00 -9.38346542e-01\n",
      " -7.88438933e-01  4.50537491e-01  1.42997949e+00 -4.17343970e-01\n",
      "  1.51600010e+00  1.66638242e-01 -1.84539446e-01 -3.30311878e-01\n",
      " -2.73029263e-01  5.96205800e-01 -9.00817578e-01 -3.62277690e-01\n",
      " -5.61257060e-01 -2.88707236e-01  6.08834452e-01 -1.03338124e+00\n",
      " -7.02091079e-01 -2.56384431e-01 -1.02067822e+00 -8.21654223e-01\n",
      " -5.77916767e-01 -5.17436082e-01  1.06877622e+00  1.80938943e+00\n",
      " -2.08353902e-01 -7.14154492e-01  1.17932528e+00  2.05461436e+00\n",
      " -9.08254947e-01  1.79832262e+00 -4.62191305e-01  1.55504628e+00\n",
      "  6.54007756e-02 -5.45430339e-01 -1.13863489e+00 -7.75483036e-01\n",
      " -5.29975486e-01 -3.70339798e-01 -1.77548320e-01 -8.65504950e-01\n",
      " -8.16135695e-01  8.93432814e-01  2.65954020e+00  2.99425028e-01\n",
      " -8.52654453e-02 -4.44088749e-01 -7.25786537e-01 -2.83039961e-01\n",
      "  6.77287996e-01 -1.76209593e-01 -5.69408417e-01 -7.20104387e-01\n",
      "  3.21529016e+00 -7.58049843e-01 -7.18200420e-01 -1.64756045e-01\n",
      " -1.74068907e-02  1.39732944e+00 -4.69524551e-01  7.39018159e-01\n",
      " -3.22816286e-02 -1.06744567e-01 -3.36440271e-01 -9.17804529e-01\n",
      " -4.52864845e-01 -9.81751028e-01  3.72281495e-01 -8.35829848e-01\n",
      "  7.64468836e-01 -3.61816573e-01  1.84208410e+00  9.78277043e-02\n",
      " -6.02608832e-01 -8.18322282e-01 -3.80365372e-01 -1.10665420e+00\n",
      "  1.13767601e+00 -2.00395917e-01 -5.09701218e-01 -8.51582196e-01\n",
      " -8.31828544e-01 -3.97858064e-01  1.68877018e+00  1.34928403e+00\n",
      " -1.00318553e+00 -7.51861952e-01 -1.78143309e-01 -5.16945216e-01\n",
      "  2.28994759e+00 -3.59110647e-02 -6.18837171e-01  2.34595098e+00\n",
      " -8.26667010e-01  1.13620341e+00 -4.49146160e-01 -1.94951763e-01\n",
      " -2.23139391e-01  4.64058628e-01  2.10074092e+00 -5.89563686e-01\n",
      " -3.70116677e-01 -6.84330642e-01 -1.11012002e+00 -1.07149032e+00\n",
      " -7.45391441e-01  6.04312532e-01 -7.67703548e-01  3.33845172e-01\n",
      " -1.98462201e-01  1.56171017e+00 -8.22442584e-01 -6.55875268e-01\n",
      " -8.37674316e-01 -9.53057658e-01 -6.00719740e-01 -9.10307661e-01\n",
      " -5.16781594e-01  6.36025473e-01  2.67224323e+00  2.19709947e+00\n",
      " -5.93550116e-01 -8.64389345e-01 -7.56978862e-01  1.58525688e+00\n",
      " -8.12089766e-01 -1.09149684e+00  1.55754524e+00 -9.09370553e-01\n",
      "  1.54894764e+00  2.05550684e+00 -3.78387032e-01  1.95904417e+00\n",
      " -2.45674619e-01 -2.90209585e-01 -8.28392479e-01 -2.12518829e-01\n",
      " -2.90849199e-01 -8.17266175e-01 -4.50246891e-01 -8.22026091e-01\n",
      " -7.47622652e-01 -3.72675132e-01 -1.03634132e+00  7.56183607e-01\n",
      " -7.61323561e-02  7.11767639e-01 -5.93713738e-01  5.77551603e-02\n",
      "  2.07842881e+00 -6.75733043e-01 -7.71035490e-01  7.58459442e-01\n",
      " -1.76730209e-01 -7.87263829e-01  2.22411200e+00  1.49548783e+00\n",
      " -8.70562361e-01 -1.02980417e-03  1.35303247e+00 -5.32310820e-01\n",
      "  2.29667097e+00 -8.65966067e-01 -1.06528756e+00  3.31264532e+00\n",
      " -1.66853383e-01  1.90079469e+00 -4.67204092e-01 -4.98619539e-01\n",
      "  2.45849324e+00 -3.47566575e-01 -8.98392996e-01  1.23248632e-01\n",
      " -4.77512285e-01 -5.69096047e-01  1.38065486e+00 -2.83977070e-01\n",
      "  1.10327074e+00  4.35826375e-01 -6.94817332e-01  1.65626888e+00\n",
      " -5.98265408e-01  2.61798018e+00 -1.92066064e-01  2.21911409e+00\n",
      " -7.90180553e-02 -8.49618730e-01 -9.07213716e-01  4.11729300e-01\n",
      " -7.25340295e-01 -8.58528698e-01  1.12809668e+00 -4.57684260e-01\n",
      "  9.88838107e-02  6.45575055e-01  1.15991374e+00 -3.36975761e-01\n",
      "  8.41281983e-01 -2.23258389e-01  2.06621538e-01 -5.34720528e-01\n",
      " -4.34330921e-01 -6.61676416e-01 -5.65555859e-01 -1.39811109e-01\n",
      " -7.34027142e-01 -8.38730422e-01  1.66120729e+00 -2.22574151e-01\n",
      " -2.38742992e-01 -8.18233033e-01 -6.08573602e-01  1.44503272e+00\n",
      "  2.65531578e+00 -2.91191318e-01 -8.69387257e-01 -7.88037315e-01\n",
      " -6.34663892e-01 -4.60748456e-01 -6.56604130e-01  2.19607184e-01\n",
      " -8.52192060e-01 -7.11060546e-01  3.44852478e-01 -2.59984117e-01\n",
      " -7.64029488e-01 -4.83551429e-01  1.32961963e+00 -2.45183753e-01\n",
      "  2.42020567e+00 -1.06830713e+00  1.81034141e+00 -8.75322278e-01\n",
      " -9.02528173e-01  1.65467728e+00 -3.85512031e-01 -4.03525339e-01\n",
      "  2.50173411e+00  8.34438327e-02] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to numpy array\n",
    "india_cases_train, india_cases_test = india_cases_train.to_numpy().flatten(), india_cases_test.to_numpy()\n",
    "usa_cases_train, usa_cases_test = usa_cases_train.to_numpy().flatten(), usa_cases_test.to_numpy()\n",
    "\n",
    "# Visualize outputs\n",
    "print('India Cases:\\n',india_cases_train,'\\n')\n",
    "print('USA Cases:\\n',usa_cases_train,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "613a19ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate data preparation\n",
    "from numpy import array\n",
    "\n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66ca8149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.60546567 -0.59635577  0.05484529] -0.5771558587770755\n",
      "[-0.59635577  0.05484529 -0.57715586] -0.5143917623597827\n",
      "[ 0.05484529 -0.57715586 -0.51439176] -0.34783758858376324\n",
      "[-0.57715586 -0.51439176 -0.34783759] -0.687982551549953\n",
      "[-0.51439176 -0.34783759 -0.68798255] -0.19334618408914545\n"
     ]
    }
   ],
   "source": [
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "# split into samples\n",
    "india_cases_train_X, india_cases_train_y = split_sequence(india_cases_train, n_steps)\n",
    "india_cases_test_X, india_cases_test_y = split_sequence(india_cases_test, n_steps)\n",
    "\n",
    "\n",
    "# summarize the data\n",
    "for i in range(0,5):\n",
    "    print(india_cases_train_X[i], india_cases_train_y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "409c73a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India Cases:\n",
      " [[[-0.60546567]\n",
      "  [-0.59635577]\n",
      "  [ 0.05484529]]\n",
      "\n",
      " [[-0.59635577]\n",
      "  [ 0.05484529]\n",
      "  [-0.57715586]]\n",
      "\n",
      " [[ 0.05484529]\n",
      "  [-0.57715586]\n",
      "  [-0.51439176]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.6871658 ]\n",
      "  [-0.68647471]\n",
      "  [-0.13646269]]\n",
      "\n",
      " [[-0.68647471]\n",
      "  [-0.13646269]\n",
      "  [-0.51216769]]\n",
      "\n",
      " [[-0.13646269]\n",
      "  [-0.51216769]\n",
      "  [ 0.09372259]]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reshape the data into [samples, timesteps, features]\n",
    "# univariate\n",
    "n_features = 1\n",
    "\n",
    "india_cases_train_X = india_cases_train_X.reshape((india_cases_train_X.shape[0], \n",
    "                                                   india_cases_train_X.shape[1], n_features))\n",
    "india_cases_test_X = india_cases_test_X.reshape((india_cases_test_X.shape[0], \n",
    "                                                 india_cases_test_X.shape[1], n_features))\n",
    "\n",
    "# Visualize outputs\n",
    "print('India Cases:\\n',india_cases_train_X,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4e40f6",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6119e325",
   "metadata": {},
   "source": [
    "### Univariate non-stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c7cc198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14d80147",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                10400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 10,451\n",
      "Trainable params: 10,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model_uni = Sequential()\n",
    "model_uni.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
    "model_uni.add(Dense(1))\n",
    "model_uni.compile(optimizer='adam',loss='mse')\n",
    "model_uni.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acc2a67",
   "metadata": {},
   "source": [
    "### Univariate stacked "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8645ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 3, 50)             10400     \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 30,651\n",
      "Trainable params: 30,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model_uni_stacked = Sequential()\n",
    "model_uni_stacked.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "model_uni_stacked.add(LSTM(50, activation='relu'))\n",
    "model_uni_stacked.add(Dense(1))\n",
    "model_uni_stacked.compile(optimizer='adam',loss='mse')\n",
    "model_uni_stacked.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b14488",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "811b9422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard callback\n",
    "logdir = os.path.join(parentDir+\"/logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b88b80",
   "metadata": {},
   "source": [
    "### Univariate non-stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7019359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.8336 - val_loss: 0.9653\n",
      "Epoch 2/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8424 - val_loss: 0.9868\n",
      "Epoch 3/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8355 - val_loss: 1.0087\n",
      "Epoch 4/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8374 - val_loss: 0.9847\n",
      "Epoch 5/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8344 - val_loss: 0.9965\n",
      "Epoch 6/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8271 - val_loss: 0.9825\n",
      "Epoch 7/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8296 - val_loss: 1.0078\n",
      "Epoch 8/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8464 - val_loss: 1.0023\n",
      "Epoch 9/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8332 - val_loss: 0.9617\n",
      "Epoch 10/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8475 - val_loss: 1.0040\n",
      "Epoch 11/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8294 - val_loss: 1.0109\n",
      "Epoch 12/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8287 - val_loss: 1.0055\n",
      "Epoch 13/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8312 - val_loss: 1.0005\n",
      "Epoch 14/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8281 - val_loss: 0.9722\n",
      "Epoch 15/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8472 - val_loss: 1.0060\n",
      "Epoch 16/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8252 - val_loss: 0.9930\n",
      "Epoch 17/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8311 - val_loss: 1.0170\n",
      "Epoch 18/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8286 - val_loss: 1.0017\n",
      "Epoch 19/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8307 - val_loss: 0.9821\n",
      "Epoch 20/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8345 - val_loss: 0.9908\n",
      "Epoch 21/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8327 - val_loss: 0.9841\n",
      "Epoch 22/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8326 - val_loss: 0.9934\n",
      "Epoch 23/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8371 - val_loss: 1.0127\n",
      "Epoch 24/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8944 - val_loss: 1.0235\n",
      "Epoch 25/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8659 - val_loss: 0.9991\n",
      "Epoch 26/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8362 - val_loss: 0.9962\n",
      "Epoch 27/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8316 - val_loss: 1.0030\n",
      "Epoch 28/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8293 - val_loss: 0.9640\n",
      "Epoch 29/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8343 - val_loss: 1.0035\n",
      "Epoch 30/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8257 - val_loss: 0.9914\n",
      "Epoch 31/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8284 - val_loss: 0.9927\n",
      "Epoch 32/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.8251 - val_loss: 0.9951\n",
      "Epoch 33/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8229 - val_loss: 0.9880\n",
      "Epoch 34/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8282 - val_loss: 1.0323\n",
      "Epoch 35/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8329 - val_loss: 0.9554\n",
      "Epoch 36/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8297 - val_loss: 1.0243\n",
      "Epoch 37/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8215 - val_loss: 0.9914\n",
      "Epoch 38/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8236 - val_loss: 0.9888\n",
      "Epoch 39/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8384 - val_loss: 0.9726\n",
      "Epoch 40/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8212 - val_loss: 1.0246\n",
      "Epoch 41/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8150 - val_loss: 0.9900\n",
      "Epoch 42/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8201 - val_loss: 0.9999\n",
      "Epoch 43/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8339 - val_loss: 0.9923\n",
      "Epoch 44/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8250 - val_loss: 1.0329\n",
      "Epoch 45/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8211 - val_loss: 0.9834\n",
      "Epoch 46/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8319 - val_loss: 0.9998\n",
      "Epoch 47/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8210 - val_loss: 1.0162\n",
      "Epoch 48/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8260 - val_loss: 1.0467\n",
      "Epoch 49/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8201 - val_loss: 0.9953\n",
      "Epoch 50/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8171 - val_loss: 0.9829\n",
      "Epoch 51/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8241 - val_loss: 1.0434\n",
      "Epoch 52/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8160 - val_loss: 0.9924\n",
      "Epoch 53/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8180 - val_loss: 1.0340\n",
      "Epoch 54/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8185 - val_loss: 1.0139\n",
      "Epoch 55/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8151 - val_loss: 1.0381\n",
      "Epoch 56/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8235 - val_loss: 0.9874\n",
      "Epoch 57/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8196 - val_loss: 0.9947\n",
      "Epoch 58/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8261 - val_loss: 1.0461\n",
      "Epoch 59/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8158 - val_loss: 0.9964\n",
      "Epoch 60/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8207 - val_loss: 1.0099\n",
      "Epoch 61/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8192 - val_loss: 1.0257\n",
      "Epoch 62/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8125 - val_loss: 1.0091\n",
      "Epoch 63/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8147 - val_loss: 0.9932\n",
      "Epoch 64/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8213 - val_loss: 1.0432\n",
      "Epoch 65/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8208 - val_loss: 1.0696\n",
      "Epoch 66/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8208 - val_loss: 0.9904\n",
      "Epoch 67/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8217 - val_loss: 1.0691\n",
      "Epoch 68/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8465 - val_loss: 1.0528\n",
      "Epoch 69/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8140 - val_loss: 1.0090\n",
      "Epoch 70/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8199 - val_loss: 1.0700\n",
      "Epoch 71/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8271 - val_loss: 1.0421\n",
      "Epoch 72/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8181 - val_loss: 1.0237\n",
      "Epoch 73/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8133 - val_loss: 1.0254\n",
      "Epoch 74/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8192 - val_loss: 1.0406\n",
      "Epoch 75/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8196 - val_loss: 1.0341\n",
      "Epoch 76/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8177 - val_loss: 0.9864\n",
      "Epoch 77/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8308 - val_loss: 1.0385\n",
      "Epoch 78/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8162 - val_loss: 1.0279\n",
      "Epoch 79/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8154 - val_loss: 1.0006\n",
      "Epoch 80/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8108 - val_loss: 1.0449\n",
      "Epoch 81/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8161 - val_loss: 1.0426\n",
      "Epoch 82/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8100 - val_loss: 1.0078\n",
      "Epoch 83/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8108 - val_loss: 1.0346\n",
      "Epoch 84/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8049 - val_loss: 1.0263\n",
      "Epoch 85/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8144 - val_loss: 1.0133\n",
      "Epoch 86/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8060 - val_loss: 1.0414\n",
      "Epoch 87/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8109 - val_loss: 1.0121\n",
      "Epoch 88/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8161 - val_loss: 1.0172\n",
      "Epoch 89/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8072 - val_loss: 1.0277\n",
      "Epoch 90/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8003 - val_loss: 1.0268\n",
      "Epoch 91/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8191 - val_loss: 1.0312\n",
      "Epoch 92/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8324 - val_loss: 1.0243\n",
      "Epoch 93/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8170 - val_loss: 0.9976\n",
      "Epoch 94/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8252 - val_loss: 1.1579\n",
      "Epoch 95/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8397 - val_loss: 1.0236\n",
      "Epoch 96/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8028 - val_loss: 1.0299\n",
      "Epoch 97/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8063 - val_loss: 1.0208\n",
      "Epoch 98/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8274 - val_loss: 1.0691\n",
      "Epoch 99/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8138 - val_loss: 1.0328\n",
      "Epoch 100/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8183 - val_loss: 1.0047\n",
      "Epoch 101/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8190 - val_loss: 1.0542\n",
      "Epoch 102/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8092 - val_loss: 1.0369\n",
      "Epoch 103/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8238 - val_loss: 1.0799\n",
      "Epoch 104/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8339 - val_loss: 1.0379\n",
      "Epoch 105/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8025 - val_loss: 1.0383\n",
      "Epoch 106/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8028 - val_loss: 1.0360\n",
      "Epoch 107/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8041 - val_loss: 1.0101\n",
      "Epoch 108/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8026 - val_loss: 1.0318\n",
      "Epoch 109/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8066 - val_loss: 1.0474\n",
      "Epoch 110/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8350 - val_loss: 1.0867\n",
      "Epoch 111/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8098 - val_loss: 1.0104\n",
      "Epoch 112/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8241 - val_loss: 1.0324\n",
      "Epoch 113/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8088 - val_loss: 1.0396\n",
      "Epoch 114/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8096 - val_loss: 1.0517\n",
      "Epoch 115/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8083 - val_loss: 1.0434\n",
      "Epoch 116/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7975 - val_loss: 1.0042\n",
      "Epoch 117/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8101 - val_loss: 1.0295\n",
      "Epoch 118/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8217 - val_loss: 1.0285\n",
      "Epoch 119/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8141 - val_loss: 1.0832\n",
      "Epoch 120/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8121 - val_loss: 1.0560\n",
      "Epoch 121/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8052 - val_loss: 1.0020\n",
      "Epoch 122/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8076 - val_loss: 1.0235\n",
      "Epoch 123/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8210 - val_loss: 1.0152\n",
      "Epoch 124/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8067 - val_loss: 1.0392\n",
      "Epoch 125/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8010 - val_loss: 1.0322\n",
      "Epoch 126/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8044 - val_loss: 1.0065\n",
      "Epoch 127/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8048 - val_loss: 1.0911\n",
      "Epoch 128/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8003 - val_loss: 1.0085\n",
      "Epoch 129/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7943 - val_loss: 1.0422\n",
      "Epoch 130/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8006 - val_loss: 1.0428\n",
      "Epoch 131/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8052 - val_loss: 1.0403\n",
      "Epoch 132/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8066 - val_loss: 1.0331\n",
      "Epoch 133/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7999 - val_loss: 1.0386\n",
      "Epoch 134/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7944 - val_loss: 1.0421\n",
      "Epoch 135/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7952 - val_loss: 1.0218\n",
      "Epoch 136/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8010 - val_loss: 1.0278\n",
      "Epoch 137/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8042 - val_loss: 1.0367\n",
      "Epoch 138/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7965 - val_loss: 1.0351\n",
      "Epoch 139/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7879 - val_loss: 1.0417\n",
      "Epoch 140/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7911 - val_loss: 1.0093\n",
      "Epoch 141/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8048 - val_loss: 1.0431\n",
      "Epoch 142/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7939 - val_loss: 1.0835\n",
      "Epoch 143/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8144 - val_loss: 1.0496\n",
      "Epoch 144/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7970 - val_loss: 1.0598\n",
      "Epoch 145/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8208 - val_loss: 1.0160\n",
      "Epoch 146/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8063 - val_loss: 1.1014\n",
      "Epoch 147/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8025 - val_loss: 1.0296\n",
      "Epoch 148/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8043 - val_loss: 1.0258\n",
      "Epoch 149/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8107 - val_loss: 1.0708\n",
      "Epoch 150/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8133 - val_loss: 1.0925\n",
      "Epoch 151/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8059 - val_loss: 1.0260\n",
      "Epoch 152/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7958 - val_loss: 1.0531\n",
      "Epoch 153/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8039 - val_loss: 1.0556\n",
      "Epoch 154/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7993 - val_loss: 1.0513\n",
      "Epoch 155/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8050 - val_loss: 1.0267\n",
      "Epoch 156/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7955 - val_loss: 1.0336\n",
      "Epoch 157/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8073 - val_loss: 1.0399\n",
      "Epoch 158/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8106 - val_loss: 1.0692\n",
      "Epoch 159/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7967 - val_loss: 1.0791\n",
      "Epoch 160/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7950 - val_loss: 1.0393\n",
      "Epoch 161/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7980 - val_loss: 1.0162\n",
      "Epoch 162/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7955 - val_loss: 1.0668\n",
      "Epoch 163/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7977 - val_loss: 1.0222\n",
      "Epoch 164/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7938 - val_loss: 1.0423\n",
      "Epoch 165/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8159 - val_loss: 1.0623\n",
      "Epoch 166/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8076 - val_loss: 1.1328\n",
      "Epoch 167/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8194 - val_loss: 1.0702\n",
      "Epoch 168/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.7891 - val_loss: 1.0187\n",
      "Epoch 169/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7954 - val_loss: 1.0908\n",
      "Epoch 170/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7972 - val_loss: 1.0513\n",
      "Epoch 171/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8012 - val_loss: 1.0240\n",
      "Epoch 172/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7956 - val_loss: 1.0160\n",
      "Epoch 173/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7881 - val_loss: 1.0821\n",
      "Epoch 174/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7985 - val_loss: 1.0652\n",
      "Epoch 175/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7969 - val_loss: 1.0502\n",
      "Epoch 176/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7913 - val_loss: 1.0418\n",
      "Epoch 177/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7934 - val_loss: 1.0450\n",
      "Epoch 178/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7866 - val_loss: 1.0776\n",
      "Epoch 179/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7847 - val_loss: 1.0258\n",
      "Epoch 180/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7944 - val_loss: 1.0298\n",
      "Epoch 181/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7962 - val_loss: 1.0220\n",
      "Epoch 182/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8179 - val_loss: 1.1301\n",
      "Epoch 183/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8143 - val_loss: 1.0848\n",
      "Epoch 184/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7932 - val_loss: 1.0582\n",
      "Epoch 185/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7932 - val_loss: 1.0333\n",
      "Epoch 186/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7821 - val_loss: 1.0383\n",
      "Epoch 187/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7881 - val_loss: 1.0562\n",
      "Epoch 188/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7843 - val_loss: 1.0570\n",
      "Epoch 189/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7979 - val_loss: 1.0364\n",
      "Epoch 190/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7974 - val_loss: 1.0695\n",
      "Epoch 191/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7771 - val_loss: 1.0348\n",
      "Epoch 192/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7864 - val_loss: 1.0673\n",
      "Epoch 193/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7855 - val_loss: 1.1086\n",
      "Epoch 194/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7864 - val_loss: 1.0447\n",
      "Epoch 195/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7795 - val_loss: 1.0394\n",
      "Epoch 196/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7931 - val_loss: 1.1198\n",
      "Epoch 197/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7923 - val_loss: 1.0940\n",
      "Epoch 198/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7875 - val_loss: 1.0478\n",
      "Epoch 199/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7817 - val_loss: 1.0916\n",
      "Epoch 200/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7753 - val_loss: 1.0350\n",
      "Epoch 201/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7793 - val_loss: 1.0516\n",
      "Epoch 202/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7807 - val_loss: 1.0921\n",
      "Epoch 203/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7947 - val_loss: 1.0284\n",
      "Epoch 204/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8015 - val_loss: 1.0934\n",
      "Epoch 205/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7874 - val_loss: 1.0922\n",
      "Epoch 206/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7793 - val_loss: 1.0465\n",
      "Epoch 207/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7822 - val_loss: 1.0627\n",
      "Epoch 208/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7849 - val_loss: 1.1089\n",
      "Epoch 209/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8043 - val_loss: 1.1229\n",
      "Epoch 210/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8051 - val_loss: 1.0696\n",
      "Epoch 211/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7889 - val_loss: 1.0137\n",
      "Epoch 212/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7939 - val_loss: 1.0936\n",
      "Epoch 213/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7809 - val_loss: 1.0399\n",
      "Epoch 214/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7792 - val_loss: 1.0840\n",
      "Epoch 215/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7843 - val_loss: 1.1107\n",
      "Epoch 216/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7923 - val_loss: 1.0419\n",
      "Epoch 217/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7866 - val_loss: 1.0502\n",
      "Epoch 218/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7747 - val_loss: 1.0403\n",
      "Epoch 219/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7844 - val_loss: 1.1162\n",
      "Epoch 220/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7888 - val_loss: 1.0717\n",
      "Epoch 221/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8009 - val_loss: 1.0356\n",
      "Epoch 222/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7921 - val_loss: 1.0813\n",
      "Epoch 223/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7792 - val_loss: 1.0972\n",
      "Epoch 224/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7816 - val_loss: 1.0550\n",
      "Epoch 225/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7769 - val_loss: 1.0522\n",
      "Epoch 226/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7848 - val_loss: 1.0580\n",
      "Epoch 227/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7807 - val_loss: 1.0549\n",
      "Epoch 228/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7922 - val_loss: 1.0746\n",
      "Epoch 229/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7721 - val_loss: 1.0778\n",
      "Epoch 230/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7785 - val_loss: 1.1754\n",
      "Epoch 231/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8090 - val_loss: 1.0875\n",
      "Epoch 232/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7761 - val_loss: 1.0349\n",
      "Epoch 233/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8060 - val_loss: 1.0619\n",
      "Epoch 234/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7711 - val_loss: 1.0994\n",
      "Epoch 235/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7803 - val_loss: 1.1060\n",
      "Epoch 236/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7889 - val_loss: 1.1038\n",
      "Epoch 237/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7894 - val_loss: 1.0509\n",
      "Epoch 238/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8021 - val_loss: 1.0284\n",
      "Epoch 239/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7876 - val_loss: 1.1441\n",
      "Epoch 240/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7929 - val_loss: 1.0763\n",
      "Epoch 241/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7833 - val_loss: 1.0422\n",
      "Epoch 242/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7809 - val_loss: 1.0741\n",
      "Epoch 243/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7691 - val_loss: 1.1009\n",
      "Epoch 244/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7758 - val_loss: 1.0775\n",
      "Epoch 245/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7667 - val_loss: 1.0563\n",
      "Epoch 246/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7725 - val_loss: 1.0517\n",
      "Epoch 247/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7702 - val_loss: 1.1168\n",
      "Epoch 248/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7835 - val_loss: 1.0539\n",
      "Epoch 249/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7776 - val_loss: 1.0839\n",
      "Epoch 250/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7709 - val_loss: 1.0656\n",
      "Epoch 251/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7815 - val_loss: 1.1099\n",
      "Epoch 252/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7864 - val_loss: 1.0808\n",
      "Epoch 253/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7675 - val_loss: 1.0780\n",
      "Epoch 254/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7695 - val_loss: 1.0644\n",
      "Epoch 255/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7698 - val_loss: 1.0679\n",
      "Epoch 256/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7852 - val_loss: 1.0735\n",
      "Epoch 257/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7725 - val_loss: 1.0879\n",
      "Epoch 258/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7750 - val_loss: 1.0931\n",
      "Epoch 259/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7770 - val_loss: 1.0415\n",
      "Epoch 260/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7716 - val_loss: 1.0835\n",
      "Epoch 261/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7647 - val_loss: 1.0743\n",
      "Epoch 262/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7595 - val_loss: 1.0729\n",
      "Epoch 263/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7674 - val_loss: 1.0916\n",
      "Epoch 264/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7657 - val_loss: 1.0931\n",
      "Epoch 265/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7727 - val_loss: 1.0564\n",
      "Epoch 266/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.7779 - val_loss: 1.1443\n",
      "Epoch 267/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7730 - val_loss: 1.1008\n",
      "Epoch 268/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7769 - val_loss: 1.1175\n",
      "Epoch 269/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7748 - val_loss: 1.0860\n",
      "Epoch 270/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7751 - val_loss: 1.0937\n",
      "Epoch 271/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8007 - val_loss: 1.0779\n",
      "Epoch 272/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7770 - val_loss: 1.0647\n",
      "Epoch 273/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7741 - val_loss: 1.0855\n",
      "Epoch 274/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7672 - val_loss: 1.0527\n",
      "Epoch 275/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7729 - val_loss: 1.1071\n",
      "Epoch 276/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7629 - val_loss: 1.0708\n",
      "Epoch 277/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7638 - val_loss: 1.1109\n",
      "Epoch 278/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7833 - val_loss: 1.0642\n",
      "Epoch 279/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7721 - val_loss: 1.0779\n",
      "Epoch 280/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7680 - val_loss: 1.1380\n",
      "Epoch 281/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7671 - val_loss: 1.1206\n",
      "Epoch 282/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7840 - val_loss: 1.0717\n",
      "Epoch 283/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7810 - val_loss: 1.1013\n",
      "Epoch 284/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7706 - val_loss: 1.0958\n",
      "Epoch 285/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7685 - val_loss: 1.0595\n",
      "Epoch 286/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7843 - val_loss: 1.0786\n",
      "Epoch 287/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7680 - val_loss: 1.0475\n",
      "Epoch 288/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7656 - val_loss: 1.1029\n",
      "Epoch 289/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7755 - val_loss: 1.0841\n",
      "Epoch 290/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7677 - val_loss: 1.0984\n",
      "Epoch 291/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7600 - val_loss: 1.1081\n",
      "Epoch 292/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7611 - val_loss: 1.0776\n",
      "Epoch 293/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7654 - val_loss: 1.0599\n",
      "Epoch 294/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7589 - val_loss: 1.0985\n",
      "Epoch 295/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7630 - val_loss: 1.1038\n",
      "Epoch 296/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7758 - val_loss: 1.1043\n",
      "Epoch 297/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7668 - val_loss: 1.0851\n",
      "Epoch 298/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7619 - val_loss: 1.1249\n",
      "Epoch 299/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7526 - val_loss: 1.0920\n",
      "Epoch 300/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7587 - val_loss: 1.1073\n",
      "Epoch 301/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7656 - val_loss: 1.1405\n",
      "Epoch 302/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7632 - val_loss: 1.0794\n",
      "Epoch 303/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7577 - val_loss: 1.1357\n",
      "Epoch 304/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7603 - val_loss: 1.0876\n",
      "Epoch 305/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7705 - val_loss: 1.0637\n",
      "Epoch 306/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7814 - val_loss: 1.0991\n",
      "Epoch 307/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7763 - val_loss: 1.1412\n",
      "Epoch 308/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7601 - val_loss: 1.1000\n",
      "Epoch 309/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7607 - val_loss: 1.0733\n",
      "Epoch 310/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7815 - val_loss: 1.0432\n",
      "Epoch 311/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7742 - val_loss: 1.1259\n",
      "Epoch 312/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7652 - val_loss: 1.1364\n",
      "Epoch 313/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.7661 - val_loss: 1.0673\n",
      "Epoch 314/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7669 - val_loss: 1.1351\n",
      "Epoch 315/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7588 - val_loss: 1.1117\n",
      "Epoch 316/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7610 - val_loss: 1.1104\n",
      "Epoch 317/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7682 - val_loss: 1.0642\n",
      "Epoch 318/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7796 - val_loss: 1.0571\n",
      "Epoch 319/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7815 - val_loss: 1.0782\n",
      "Epoch 320/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7595 - val_loss: 1.1166\n",
      "Epoch 321/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7513 - val_loss: 1.0748\n",
      "Epoch 322/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7579 - val_loss: 1.1139\n",
      "Epoch 323/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7679 - val_loss: 1.1931\n",
      "Epoch 324/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7850 - val_loss: 1.0705\n",
      "Epoch 325/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7649 - val_loss: 1.0643\n",
      "Epoch 326/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7732 - val_loss: 1.0734\n",
      "Epoch 327/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7541 - val_loss: 1.1012\n",
      "Epoch 328/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7575 - val_loss: 1.0657\n",
      "Epoch 329/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7464 - val_loss: 1.1175\n",
      "Epoch 330/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7502 - val_loss: 1.0819\n",
      "Epoch 331/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7464 - val_loss: 1.1489\n",
      "Epoch 332/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7819 - val_loss: 1.0863\n",
      "Epoch 333/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7708 - val_loss: 1.0953\n",
      "Epoch 334/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7729 - val_loss: 1.0851\n",
      "Epoch 335/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7542 - val_loss: 1.1027\n",
      "Epoch 336/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7572 - val_loss: 1.1774\n",
      "Epoch 337/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7630 - val_loss: 1.0607\n",
      "Epoch 338/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7519 - val_loss: 1.0805\n",
      "Epoch 339/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7538 - val_loss: 1.1140\n",
      "Epoch 340/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7558 - val_loss: 1.1043\n",
      "Epoch 341/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7506 - val_loss: 1.1015\n",
      "Epoch 342/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7649 - val_loss: 1.1198\n",
      "Epoch 343/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7510 - val_loss: 1.0835\n",
      "Epoch 344/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7727 - val_loss: 1.0599\n",
      "Epoch 345/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7602 - val_loss: 1.1162\n",
      "Epoch 346/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7532 - val_loss: 1.0908\n",
      "Epoch 347/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7501 - val_loss: 1.1445\n",
      "Epoch 348/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7658 - val_loss: 1.0911\n",
      "Epoch 349/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7595 - val_loss: 1.0759\n",
      "Epoch 350/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7529 - val_loss: 1.0861\n",
      "Epoch 351/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7576 - val_loss: 1.1409\n",
      "Epoch 352/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7695 - val_loss: 1.1311\n",
      "Epoch 353/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7539 - val_loss: 1.0844\n",
      "Epoch 354/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7478 - val_loss: 1.0722\n",
      "Epoch 355/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7523 - val_loss: 1.1547\n",
      "Epoch 356/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7566 - val_loss: 1.1157\n",
      "Epoch 357/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7567 - val_loss: 1.0931\n",
      "Epoch 358/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7562 - val_loss: 1.1211\n",
      "Epoch 359/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7653 - val_loss: 1.0933\n",
      "Epoch 360/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7567 - val_loss: 1.0767\n",
      "Epoch 361/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7548 - val_loss: 1.2061\n",
      "Epoch 362/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7585 - val_loss: 1.0985\n",
      "Epoch 363/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7525 - val_loss: 1.1215\n",
      "Epoch 364/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7459 - val_loss: 1.1622\n",
      "Epoch 365/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7458 - val_loss: 1.0986\n",
      "Epoch 366/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7530 - val_loss: 1.1828\n",
      "Epoch 367/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7635 - val_loss: 1.0615\n",
      "Epoch 368/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7729 - val_loss: 1.0928\n",
      "Epoch 369/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7492 - val_loss: 1.1087\n",
      "Epoch 370/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7451 - val_loss: 1.0877\n",
      "Epoch 371/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7622 - val_loss: 1.1527\n",
      "Epoch 372/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7553 - val_loss: 1.1033\n",
      "Epoch 373/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7388 - val_loss: 1.1627\n",
      "Epoch 374/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7645 - val_loss: 1.1067\n",
      "Epoch 375/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7509 - val_loss: 1.1322\n",
      "Epoch 376/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7574 - val_loss: 1.1785\n",
      "Epoch 377/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7532 - val_loss: 1.0691\n",
      "Epoch 378/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7552 - val_loss: 1.1051\n",
      "Epoch 379/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7513 - val_loss: 1.1293\n",
      "Epoch 380/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7459 - val_loss: 1.1030\n",
      "Epoch 381/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7409 - val_loss: 1.1319\n",
      "Epoch 382/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7492 - val_loss: 1.1853\n",
      "Epoch 383/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7842 - val_loss: 1.0827\n",
      "Epoch 384/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7555 - val_loss: 1.0856\n",
      "Epoch 385/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7445 - val_loss: 1.1223\n",
      "Epoch 386/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7705 - val_loss: 1.1007\n",
      "Epoch 387/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7443 - val_loss: 1.0768\n",
      "Epoch 388/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7339 - val_loss: 1.1098\n",
      "Epoch 389/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7526 - val_loss: 1.0593\n",
      "Epoch 390/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7378 - val_loss: 1.1353\n",
      "Epoch 391/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7423 - val_loss: 1.1175\n",
      "Epoch 392/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7393 - val_loss: 1.0817\n",
      "Epoch 393/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7629 - val_loss: 1.1249\n",
      "Epoch 394/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7470 - val_loss: 1.1148\n",
      "Epoch 395/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7443 - val_loss: 1.1152\n",
      "Epoch 396/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7404 - val_loss: 1.1019\n",
      "Epoch 397/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7446 - val_loss: 1.0919\n",
      "Epoch 398/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7427 - val_loss: 1.1129\n",
      "Epoch 399/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7429 - val_loss: 1.1458\n",
      "Epoch 400/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7474 - val_loss: 1.1202\n",
      "Epoch 401/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7476 - val_loss: 1.0943\n",
      "Epoch 402/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7468 - val_loss: 1.0994\n",
      "Epoch 403/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7377 - val_loss: 1.1162\n",
      "Epoch 404/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7478 - val_loss: 1.1352\n",
      "Epoch 405/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7427 - val_loss: 1.1305\n",
      "Epoch 406/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7487 - val_loss: 1.1077\n",
      "Epoch 407/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7430 - val_loss: 1.0840\n",
      "Epoch 408/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7311 - val_loss: 1.1447\n",
      "Epoch 409/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7392 - val_loss: 1.1453\n",
      "Epoch 410/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7400 - val_loss: 1.1171\n",
      "Epoch 411/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7431 - val_loss: 1.0946\n",
      "Epoch 412/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7547 - val_loss: 1.1188\n",
      "Epoch 413/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7297 - val_loss: 1.1327\n",
      "Epoch 414/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7388 - val_loss: 1.0831\n",
      "Epoch 415/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7331 - val_loss: 1.1542\n",
      "Epoch 416/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7688 - val_loss: 1.1506\n",
      "Epoch 417/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7670 - val_loss: 1.0795\n",
      "Epoch 418/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7484 - val_loss: 1.0786\n",
      "Epoch 419/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7497 - val_loss: 1.1524\n",
      "Epoch 420/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7334 - val_loss: 1.0955\n",
      "Epoch 421/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7510 - val_loss: 1.1140\n",
      "Epoch 422/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7525 - val_loss: 1.1423\n",
      "Epoch 423/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7362 - val_loss: 1.1097\n",
      "Epoch 424/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7357 - val_loss: 1.1342\n",
      "Epoch 425/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7405 - val_loss: 1.1090\n",
      "Epoch 426/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7364 - val_loss: 1.1220\n",
      "Epoch 427/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7481 - val_loss: 1.0826\n",
      "Epoch 428/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7504 - val_loss: 1.1042\n",
      "Epoch 429/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7375 - val_loss: 1.1587\n",
      "Epoch 430/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7472 - val_loss: 1.1376\n",
      "Epoch 431/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7328 - val_loss: 1.1265\n",
      "Epoch 432/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7396 - val_loss: 1.1497\n",
      "Epoch 433/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7587 - val_loss: 1.1502\n",
      "Epoch 434/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7587 - val_loss: 1.0894\n",
      "Epoch 435/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7351 - val_loss: 1.1543\n",
      "Epoch 436/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.7482 - val_loss: 1.1251\n",
      "Epoch 437/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7560 - val_loss: 1.1059\n",
      "Epoch 438/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7504 - val_loss: 1.0829\n",
      "Epoch 439/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7266 - val_loss: 1.1321\n",
      "Epoch 440/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7353 - val_loss: 1.1397\n",
      "Epoch 441/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7249 - val_loss: 1.1152\n",
      "Epoch 442/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7424 - val_loss: 1.1276\n",
      "Epoch 443/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7336 - val_loss: 1.1135\n",
      "Epoch 444/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7380 - val_loss: 1.2106\n",
      "Epoch 445/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7439 - val_loss: 1.1713\n",
      "Epoch 446/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7407 - val_loss: 1.1137\n",
      "Epoch 447/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7301 - val_loss: 1.1225\n",
      "Epoch 448/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7289 - val_loss: 1.1263\n",
      "Epoch 449/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7193 - val_loss: 1.1359\n",
      "Epoch 450/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7363 - val_loss: 1.1480\n",
      "Epoch 451/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7473 - val_loss: 1.0968\n",
      "Epoch 452/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7305 - val_loss: 1.1711\n",
      "Epoch 453/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7231 - val_loss: 1.0937\n",
      "Epoch 454/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7338 - val_loss: 1.1718\n",
      "Epoch 455/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7369 - val_loss: 1.0932\n",
      "Epoch 456/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7278 - val_loss: 1.1323\n",
      "Epoch 457/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7323 - val_loss: 1.1261\n",
      "Epoch 458/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7250 - val_loss: 1.1409\n",
      "Epoch 459/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7267 - val_loss: 1.1346\n",
      "Epoch 460/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7273 - val_loss: 1.1816\n",
      "Epoch 461/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7535 - val_loss: 1.1902\n",
      "Epoch 462/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7425 - val_loss: 1.1190\n",
      "Epoch 463/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7345 - val_loss: 1.0976\n",
      "Epoch 464/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7267 - val_loss: 1.0884\n",
      "Epoch 465/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7607 - val_loss: 1.1635\n",
      "Epoch 466/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7321 - val_loss: 1.2100\n",
      "Epoch 467/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7389 - val_loss: 1.1512\n",
      "Epoch 468/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7375 - val_loss: 1.0960\n",
      "Epoch 469/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7249 - val_loss: 1.1301\n",
      "Epoch 470/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7462 - val_loss: 1.0965\n",
      "Epoch 471/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7268 - val_loss: 1.1168\n",
      "Epoch 472/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7194 - val_loss: 1.1153\n",
      "Epoch 473/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7222 - val_loss: 1.1586\n",
      "Epoch 474/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7215 - val_loss: 1.1127\n",
      "Epoch 475/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7404 - val_loss: 1.1606\n",
      "Epoch 476/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7355 - val_loss: 1.1092\n",
      "Epoch 477/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7427 - val_loss: 1.1885\n",
      "Epoch 478/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7391 - val_loss: 1.1399\n",
      "Epoch 479/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7238 - val_loss: 1.1787\n",
      "Epoch 480/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7368 - val_loss: 1.1271\n",
      "Epoch 481/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7173 - val_loss: 1.1353\n",
      "Epoch 482/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7284 - val_loss: 1.1543\n",
      "Epoch 483/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7187 - val_loss: 1.0943\n",
      "Epoch 484/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7270 - val_loss: 1.1681\n",
      "Epoch 485/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7400 - val_loss: 1.1722\n",
      "Epoch 486/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7364 - val_loss: 1.1786\n",
      "Epoch 487/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7278 - val_loss: 1.1026\n",
      "Epoch 488/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7168 - val_loss: 1.1481\n",
      "Epoch 489/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7184 - val_loss: 1.1225\n",
      "Epoch 490/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7245 - val_loss: 1.1734\n",
      "Epoch 491/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7289 - val_loss: 1.1654\n",
      "Epoch 492/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7292 - val_loss: 1.1294\n",
      "Epoch 493/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7221 - val_loss: 1.1560\n",
      "Epoch 494/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7332 - val_loss: 1.1865\n",
      "Epoch 495/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7337 - val_loss: 1.1395\n",
      "Epoch 496/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7324 - val_loss: 1.1300\n",
      "Epoch 497/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7347 - val_loss: 1.1038\n",
      "Epoch 498/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7436 - val_loss: 1.2245\n",
      "Epoch 499/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7583 - val_loss: 1.1389\n",
      "Epoch 500/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7402 - val_loss: 1.0926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7f3c75bb70>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_uni.fit(india_cases_train_X,\n",
    "          india_cases_train_y, \n",
    "          validation_data=(india_cases_test_X, india_cases_test_y),\n",
    "          epochs=500,\n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acc2a67",
   "metadata": {},
   "source": [
    "### Univariate stacked "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164cb27e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "16/16 [==============================] - 2s 42ms/step - loss: 1.0392 - val_loss: 0.8707\n",
      "Epoch 2/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0387 - val_loss: 0.8709\n",
      "Epoch 3/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0384 - val_loss: 0.8701\n",
      "Epoch 4/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0385 - val_loss: 0.8707\n",
      "Epoch 5/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0381 - val_loss: 0.8703\n",
      "Epoch 6/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0381 - val_loss: 0.8703\n",
      "Epoch 7/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0381 - val_loss: 0.8705\n",
      "Epoch 8/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0379 - val_loss: 0.8709\n",
      "Epoch 9/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0376 - val_loss: 0.8710\n",
      "Epoch 10/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0377 - val_loss: 0.8706\n",
      "Epoch 11/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0374 - val_loss: 0.8714\n",
      "Epoch 12/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0380 - val_loss: 0.8720\n",
      "Epoch 13/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0371 - val_loss: 0.8719\n",
      "Epoch 14/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0374 - val_loss: 0.8708\n",
      "Epoch 15/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0374 - val_loss: 0.8721\n",
      "Epoch 16/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0372 - val_loss: 0.8716\n",
      "Epoch 17/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0366 - val_loss: 0.8724\n",
      "Epoch 18/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0363 - val_loss: 0.8762\n",
      "Epoch 19/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0367 - val_loss: 0.8752\n",
      "Epoch 20/500\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.0365 - val_loss: 0.8747\n",
      "Epoch 21/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0361 - val_loss: 0.8758\n",
      "Epoch 22/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0365 - val_loss: 0.8749\n",
      "Epoch 23/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0358 - val_loss: 0.8756\n",
      "Epoch 24/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0356 - val_loss: 0.8751\n",
      "Epoch 25/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0357 - val_loss: 0.8742\n",
      "Epoch 26/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0365 - val_loss: 0.8763\n",
      "Epoch 27/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0350 - val_loss: 0.8720\n",
      "Epoch 28/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0346 - val_loss: 0.8745\n",
      "Epoch 29/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0353 - val_loss: 0.8749\n",
      "Epoch 30/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0347 - val_loss: 0.8724\n",
      "Epoch 31/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0331 - val_loss: 0.8739\n",
      "Epoch 32/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0340 - val_loss: 0.8729\n",
      "Epoch 33/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0329 - val_loss: 0.8757\n",
      "Epoch 34/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0336 - val_loss: 0.8809\n",
      "Epoch 35/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0334 - val_loss: 0.8757\n",
      "Epoch 36/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0330 - val_loss: 0.8762\n",
      "Epoch 37/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0330 - val_loss: 0.8819\n",
      "Epoch 38/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0310 - val_loss: 0.8755\n",
      "Epoch 39/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0311 - val_loss: 0.8738\n",
      "Epoch 40/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0317 - val_loss: 0.8749\n",
      "Epoch 41/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0326 - val_loss: 0.8757\n",
      "Epoch 42/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0316 - val_loss: 0.8703\n",
      "Epoch 43/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0311 - val_loss: 0.8721\n",
      "Epoch 44/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0302 - val_loss: 0.8755\n",
      "Epoch 45/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0297 - val_loss: 0.8767\n",
      "Epoch 46/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0297 - val_loss: 0.8730\n",
      "Epoch 47/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 1.0306 - val_loss: 0.8745\n",
      "Epoch 48/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0327 - val_loss: 0.8835\n",
      "Epoch 49/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0287 - val_loss: 0.8732\n",
      "Epoch 50/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0304 - val_loss: 0.8723\n",
      "Epoch 51/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0302 - val_loss: 0.8717\n",
      "Epoch 52/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0299 - val_loss: 0.8828\n",
      "Epoch 53/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0289 - val_loss: 0.8775\n",
      "Epoch 54/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0274 - val_loss: 0.8776\n",
      "Epoch 55/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0271 - val_loss: 0.8774\n",
      "Epoch 56/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0265 - val_loss: 0.8790\n",
      "Epoch 57/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0265 - val_loss: 0.8784\n",
      "Epoch 58/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0276 - val_loss: 0.8744\n",
      "Epoch 59/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0297 - val_loss: 0.8890\n",
      "Epoch 60/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0284 - val_loss: 0.8794\n",
      "Epoch 61/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0259 - val_loss: 0.8774\n",
      "Epoch 62/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0262 - val_loss: 0.8777\n",
      "Epoch 63/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0250 - val_loss: 0.8772\n",
      "Epoch 64/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0244 - val_loss: 0.8821\n",
      "Epoch 65/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0232 - val_loss: 0.8834\n",
      "Epoch 66/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0236 - val_loss: 0.8805\n",
      "Epoch 67/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0224 - val_loss: 0.8840\n",
      "Epoch 68/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0258 - val_loss: 0.8786\n",
      "Epoch 69/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0220 - val_loss: 0.8810\n",
      "Epoch 70/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0243 - val_loss: 0.8871\n",
      "Epoch 71/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0211 - val_loss: 0.8787\n",
      "Epoch 72/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0221 - val_loss: 0.8839\n",
      "Epoch 73/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0203 - val_loss: 0.8848\n",
      "Epoch 74/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0195 - val_loss: 0.8845\n",
      "Epoch 75/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0197 - val_loss: 0.8834\n",
      "Epoch 76/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0207 - val_loss: 0.8936\n",
      "Epoch 77/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0209 - val_loss: 0.8876\n",
      "Epoch 78/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0158 - val_loss: 0.8822\n",
      "Epoch 79/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0220 - val_loss: 0.8887\n",
      "Epoch 80/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0259 - val_loss: 0.8971\n",
      "Epoch 81/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0198 - val_loss: 0.8814\n",
      "Epoch 82/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0153 - val_loss: 0.8919\n",
      "Epoch 83/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0198 - val_loss: 0.8962\n",
      "Epoch 84/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0166 - val_loss: 0.8807\n",
      "Epoch 85/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0124 - val_loss: 0.8943\n",
      "Epoch 86/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0141 - val_loss: 0.8904\n",
      "Epoch 87/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0104 - val_loss: 0.9004\n",
      "Epoch 88/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0127 - val_loss: 0.8930\n",
      "Epoch 89/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0090 - val_loss: 0.8981\n",
      "Epoch 90/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0130 - val_loss: 0.8993\n",
      "Epoch 91/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0067 - val_loss: 0.9047\n",
      "Epoch 92/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0046 - val_loss: 0.9001\n",
      "Epoch 93/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0030 - val_loss: 0.9072\n",
      "Epoch 94/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0042 - val_loss: 0.9005\n",
      "Epoch 95/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0039 - val_loss: 0.9124\n",
      "Epoch 96/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0048 - val_loss: 0.9060\n",
      "Epoch 97/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0013 - val_loss: 0.9083\n",
      "Epoch 98/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0015 - val_loss: 0.9075\n",
      "Epoch 99/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0077 - val_loss: 0.9103\n",
      "Epoch 100/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9978 - val_loss: 0.9056\n",
      "Epoch 101/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9983 - val_loss: 0.9175\n",
      "Epoch 102/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0032 - val_loss: 0.9017\n",
      "Epoch 103/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0016 - val_loss: 0.9209\n",
      "Epoch 104/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9959 - val_loss: 0.9079\n",
      "Epoch 105/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0009 - val_loss: 0.9089\n",
      "Epoch 106/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9995 - val_loss: 0.9293\n",
      "Epoch 107/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0063 - val_loss: 0.9077\n",
      "Epoch 108/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0021 - val_loss: 0.9080\n",
      "Epoch 109/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9936 - val_loss: 0.9108\n",
      "Epoch 110/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9928 - val_loss: 0.9051\n",
      "Epoch 111/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9927 - val_loss: 0.9164\n",
      "Epoch 112/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9893 - val_loss: 0.9125\n",
      "Epoch 113/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9902 - val_loss: 0.9169\n",
      "Epoch 114/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9919 - val_loss: 0.9175\n",
      "Epoch 115/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9876 - val_loss: 0.9128\n",
      "Epoch 116/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9878 - val_loss: 0.9150\n",
      "Epoch 117/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9861 - val_loss: 0.9167\n",
      "Epoch 118/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9885 - val_loss: 0.9171\n",
      "Epoch 119/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9863 - val_loss: 0.9224\n",
      "Epoch 120/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9876 - val_loss: 0.9110\n",
      "Epoch 121/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9891 - val_loss: 0.9234\n",
      "Epoch 122/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9853 - val_loss: 0.9057\n",
      "Epoch 123/500\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.9855 - val_loss: 0.9130\n",
      "Epoch 124/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9840 - val_loss: 0.9131\n",
      "Epoch 125/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9842 - val_loss: 0.9166\n",
      "Epoch 126/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9812 - val_loss: 0.9190\n",
      "Epoch 127/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9809 - val_loss: 0.9174\n",
      "Epoch 128/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9815 - val_loss: 0.9110\n",
      "Epoch 129/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9810 - val_loss: 0.9227\n",
      "Epoch 130/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9814 - val_loss: 0.9106\n",
      "Epoch 131/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9817 - val_loss: 0.9222\n",
      "Epoch 132/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9816 - val_loss: 0.9094\n",
      "Epoch 133/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9770 - val_loss: 0.9201\n",
      "Epoch 134/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9867 - val_loss: 0.9176\n",
      "Epoch 135/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9783 - val_loss: 0.9142\n",
      "Epoch 136/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9786 - val_loss: 0.9108\n",
      "Epoch 137/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9754 - val_loss: 0.9187\n",
      "Epoch 138/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9775 - val_loss: 0.9116\n",
      "Epoch 139/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9774 - val_loss: 0.9198\n",
      "Epoch 140/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9722 - val_loss: 0.9166\n",
      "Epoch 141/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9773 - val_loss: 0.8994\n",
      "Epoch 142/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9858 - val_loss: 0.9212\n",
      "Epoch 143/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9777 - val_loss: 0.9099\n",
      "Epoch 144/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9739 - val_loss: 0.9196\n",
      "Epoch 145/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9816 - val_loss: 0.9046\n",
      "Epoch 146/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9816 - val_loss: 0.9237\n",
      "Epoch 147/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9755 - val_loss: 0.9182\n",
      "Epoch 148/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9749 - val_loss: 0.9136\n",
      "Epoch 149/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9708 - val_loss: 0.9201\n",
      "Epoch 150/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9702 - val_loss: 0.9220\n",
      "Epoch 151/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9699 - val_loss: 0.9228\n",
      "Epoch 152/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9698 - val_loss: 0.9222\n",
      "Epoch 153/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9702 - val_loss: 0.9240\n",
      "Epoch 154/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9849 - val_loss: 0.9169\n",
      "Epoch 155/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9840 - val_loss: 0.9346\n",
      "Epoch 156/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9691 - val_loss: 0.9105\n",
      "Epoch 157/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9703 - val_loss: 0.9272\n",
      "Epoch 158/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9692 - val_loss: 0.9113\n",
      "Epoch 159/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9665 - val_loss: 0.9155\n",
      "Epoch 160/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9678 - val_loss: 0.9208\n",
      "Epoch 161/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9694 - val_loss: 0.9160\n",
      "Epoch 162/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9667 - val_loss: 0.9188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9654 - val_loss: 0.9118\n",
      "Epoch 164/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9662 - val_loss: 0.9192\n",
      "Epoch 165/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9678 - val_loss: 0.9155\n",
      "Epoch 166/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9636 - val_loss: 0.9202\n",
      "Epoch 167/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9631 - val_loss: 0.9162\n",
      "Epoch 168/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9635 - val_loss: 0.9235\n",
      "Epoch 169/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9613 - val_loss: 0.9126\n",
      "Epoch 170/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9622 - val_loss: 0.9230\n",
      "Epoch 171/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9606 - val_loss: 0.9125\n",
      "Epoch 172/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9688 - val_loss: 0.9282\n",
      "Epoch 173/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9590 - val_loss: 0.9121\n",
      "Epoch 174/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9621 - val_loss: 0.9164\n",
      "Epoch 175/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9589 - val_loss: 0.9265\n",
      "Epoch 176/500\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.9597 - val_loss: 0.9113\n",
      "Epoch 177/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9700 - val_loss: 0.9291\n",
      "Epoch 178/500\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.9583 - val_loss: 0.9204\n",
      "Epoch 179/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9584 - val_loss: 0.9259\n",
      "Epoch 180/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9663 - val_loss: 0.9222\n",
      "Epoch 181/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9591 - val_loss: 0.9216\n",
      "Epoch 182/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9579 - val_loss: 0.9233\n",
      "Epoch 183/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9627 - val_loss: 0.9165\n",
      "Epoch 184/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9595 - val_loss: 0.9188\n",
      "Epoch 185/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9643 - val_loss: 0.9284\n",
      "Epoch 186/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9639 - val_loss: 0.9268\n",
      "Epoch 187/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9559 - val_loss: 0.9234\n",
      "Epoch 188/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9556 - val_loss: 0.9302\n",
      "Epoch 189/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9620 - val_loss: 0.9207\n",
      "Epoch 190/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9791 - val_loss: 0.9136\n",
      "Epoch 191/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9742 - val_loss: 0.9249\n",
      "Epoch 192/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9619 - val_loss: 0.9237\n",
      "Epoch 193/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9564 - val_loss: 0.9235\n",
      "Epoch 194/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9591 - val_loss: 0.9236\n",
      "Epoch 195/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9581 - val_loss: 0.9360\n",
      "Epoch 196/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9627 - val_loss: 0.9225\n",
      "Epoch 197/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9564 - val_loss: 0.9226\n",
      "Epoch 198/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9515 - val_loss: 0.9261\n",
      "Epoch 199/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9535 - val_loss: 0.9201\n",
      "Epoch 200/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9526 - val_loss: 0.9286\n",
      "Epoch 201/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9449 - val_loss: 0.9121\n",
      "Epoch 202/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9544 - val_loss: 0.9402\n",
      "Epoch 203/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9468 - val_loss: 0.9246\n",
      "Epoch 204/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9494 - val_loss: 0.9261\n",
      "Epoch 205/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9482 - val_loss: 0.9331\n",
      "Epoch 206/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9563 - val_loss: 0.9170\n",
      "Epoch 207/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9635 - val_loss: 0.9224\n",
      "Epoch 208/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9587 - val_loss: 0.9190\n",
      "Epoch 209/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9490 - val_loss: 0.9339\n",
      "Epoch 210/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9497 - val_loss: 0.9270\n",
      "Epoch 211/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9485 - val_loss: 0.9260\n",
      "Epoch 212/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9481 - val_loss: 0.9238\n",
      "Epoch 213/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9462 - val_loss: 0.9322\n",
      "Epoch 214/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9469 - val_loss: 0.9387\n",
      "Epoch 215/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9437 - val_loss: 0.9274\n",
      "Epoch 216/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9472 - val_loss: 0.9280\n",
      "Epoch 217/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9416 - val_loss: 0.9423\n",
      "Epoch 218/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9424 - val_loss: 0.9260\n",
      "Epoch 219/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9496 - val_loss: 0.9321\n",
      "Epoch 220/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9641 - val_loss: 0.9262\n",
      "Epoch 221/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9492 - val_loss: 0.9332\n",
      "Epoch 222/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9442 - val_loss: 0.9306\n",
      "Epoch 223/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9414 - val_loss: 0.9391\n",
      "Epoch 224/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9472 - val_loss: 0.9226\n",
      "Epoch 225/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9443 - val_loss: 0.9403\n",
      "Epoch 226/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9546 - val_loss: 0.9429\n",
      "Epoch 227/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9382 - val_loss: 0.9351\n",
      "Epoch 228/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9389 - val_loss: 0.9415\n",
      "Epoch 229/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9366 - val_loss: 0.9380\n",
      "Epoch 230/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9400 - val_loss: 0.9147\n",
      "Epoch 231/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9418 - val_loss: 0.9350\n",
      "Epoch 232/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9401 - val_loss: 0.9270\n",
      "Epoch 233/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9393 - val_loss: 0.9304\n",
      "Epoch 234/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9357 - val_loss: 0.9459\n",
      "Epoch 235/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9499 - val_loss: 0.9240\n",
      "Epoch 236/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9480 - val_loss: 0.9307\n",
      "Epoch 237/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9498 - val_loss: 0.9159\n",
      "Epoch 238/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9364 - val_loss: 0.9384\n",
      "Epoch 239/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9375 - val_loss: 0.9240\n",
      "Epoch 240/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9349 - val_loss: 0.9437\n",
      "Epoch 241/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9302 - val_loss: 0.9500\n",
      "Epoch 242/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9345 - val_loss: 0.9451\n",
      "Epoch 243/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9277 - val_loss: 0.9329\n",
      "Epoch 244/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9327 - val_loss: 0.9254\n",
      "Epoch 245/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9313 - val_loss: 0.9369\n",
      "Epoch 246/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9281 - val_loss: 0.9341\n",
      "Epoch 247/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9254 - val_loss: 0.9495\n",
      "Epoch 248/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9346 - val_loss: 0.9417\n",
      "Epoch 249/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9355 - val_loss: 0.9660\n",
      "Epoch 250/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9322 - val_loss: 0.9260\n",
      "Epoch 251/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9223 - val_loss: 0.9506\n",
      "Epoch 252/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9194 - val_loss: 0.9362\n",
      "Epoch 253/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9336 - val_loss: 0.9374\n",
      "Epoch 254/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9223 - val_loss: 0.9491\n",
      "Epoch 255/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9262 - val_loss: 0.9483\n",
      "Epoch 256/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9224 - val_loss: 0.9302\n",
      "Epoch 257/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9400 - val_loss: 0.9205\n",
      "Epoch 258/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9346 - val_loss: 0.9780\n",
      "Epoch 259/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9263 - val_loss: 0.9409\n",
      "Epoch 260/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9251 - val_loss: 0.9586\n",
      "Epoch 261/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9318 - val_loss: 0.9370\n",
      "Epoch 262/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9277 - val_loss: 0.9369\n",
      "Epoch 263/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9260 - val_loss: 0.9713\n",
      "Epoch 264/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9378 - val_loss: 0.9603\n",
      "Epoch 265/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9148 - val_loss: 0.9374\n",
      "Epoch 266/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9136 - val_loss: 0.9463\n",
      "Epoch 267/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9076 - val_loss: 0.9559\n",
      "Epoch 268/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9110 - val_loss: 0.9263\n",
      "Epoch 269/500\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.9246 - val_loss: 0.9203\n",
      "Epoch 270/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9244 - val_loss: 0.9734\n",
      "Epoch 271/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9115 - val_loss: 0.9350\n",
      "Epoch 272/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9161 - val_loss: 0.9469\n",
      "Epoch 273/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9044 - val_loss: 0.9415\n",
      "Epoch 274/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9172 - val_loss: 0.9575\n",
      "Epoch 275/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9133 - val_loss: 0.9639\n",
      "Epoch 276/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9048 - val_loss: 0.9541\n",
      "Epoch 277/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8996 - val_loss: 0.9377\n",
      "Epoch 278/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8985 - val_loss: 0.9883\n",
      "Epoch 279/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9102 - val_loss: 0.9488\n",
      "Epoch 280/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9086 - val_loss: 0.9735\n",
      "Epoch 281/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9164 - val_loss: 0.9484\n",
      "Epoch 282/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9410 - val_loss: 0.9118\n",
      "Epoch 283/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9441 - val_loss: 0.9332\n",
      "Epoch 284/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9095 - val_loss: 0.9504\n",
      "Epoch 285/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9021 - val_loss: 0.9501\n",
      "Epoch 286/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8965 - val_loss: 0.9494\n",
      "Epoch 287/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9003 - val_loss: 0.9592\n",
      "Epoch 288/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8981 - val_loss: 0.9512\n",
      "Epoch 289/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8955 - val_loss: 0.9476\n",
      "Epoch 290/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9025 - val_loss: 0.9729\n",
      "Epoch 291/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9011 - val_loss: 0.9330\n",
      "Epoch 292/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8993 - val_loss: 0.9738\n",
      "Epoch 293/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9025 - val_loss: 0.9468\n",
      "Epoch 294/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8944 - val_loss: 0.9642\n",
      "Epoch 295/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8927 - val_loss: 0.9900\n",
      "Epoch 296/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8952 - val_loss: 0.9449\n",
      "Epoch 297/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8890 - val_loss: 0.9650\n",
      "Epoch 298/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8876 - val_loss: 0.9352\n",
      "Epoch 299/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8963 - val_loss: 0.9513\n",
      "Epoch 300/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8859 - val_loss: 0.9602\n",
      "Epoch 301/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8967 - val_loss: 0.9414\n",
      "Epoch 302/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9104 - val_loss: 0.9770\n",
      "Epoch 303/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9204 - val_loss: 0.9342\n",
      "Epoch 304/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8964 - val_loss: 0.9629\n",
      "Epoch 305/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8878 - val_loss: 0.9506\n",
      "Epoch 306/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8862 - val_loss: 0.9507\n",
      "Epoch 307/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8854 - val_loss: 0.9370\n",
      "Epoch 308/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8907 - val_loss: 0.9563\n",
      "Epoch 309/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8867 - val_loss: 0.9446\n",
      "Epoch 310/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8782 - val_loss: 0.9752\n",
      "Epoch 311/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8754 - val_loss: 0.9666\n",
      "Epoch 312/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8797 - val_loss: 0.9700\n",
      "Epoch 313/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8778 - val_loss: 0.9248\n",
      "Epoch 314/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8812 - val_loss: 1.0056\n",
      "Epoch 315/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8884 - val_loss: 0.9596\n",
      "Epoch 316/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8736 - val_loss: 0.9960\n",
      "Epoch 317/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8795 - val_loss: 0.9277\n",
      "Epoch 318/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8792 - val_loss: 0.9658\n",
      "Epoch 319/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8832 - val_loss: 0.9440\n",
      "Epoch 320/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8906 - val_loss: 0.9902\n",
      "Epoch 321/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8851 - val_loss: 0.9363\n",
      "Epoch 322/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8740 - val_loss: 0.9985\n",
      "Epoch 323/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8791 - val_loss: 0.9456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 324/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8880 - val_loss: 0.9477\n",
      "Epoch 325/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8745 - val_loss: 0.9537\n",
      "Epoch 326/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8670 - val_loss: 0.9587\n",
      "Epoch 327/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8753 - val_loss: 0.9970\n",
      "Epoch 328/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8769 - val_loss: 0.9741\n",
      "Epoch 329/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8781 - val_loss: 0.9541\n",
      "Epoch 330/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8650 - val_loss: 0.9973\n",
      "Epoch 331/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8642 - val_loss: 0.9431\n",
      "Epoch 332/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8682 - val_loss: 1.0190\n",
      "Epoch 333/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8773 - val_loss: 0.9074\n",
      "Epoch 334/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8725 - val_loss: 0.9757\n",
      "Epoch 335/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8611 - val_loss: 0.9418\n",
      "Epoch 336/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8684 - val_loss: 0.9550\n",
      "Epoch 337/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8661 - val_loss: 0.9375\n",
      "Epoch 338/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8585 - val_loss: 0.9854\n",
      "Epoch 339/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8666 - val_loss: 0.9199\n",
      "Epoch 340/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8649 - val_loss: 0.9754\n",
      "Epoch 341/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8594 - val_loss: 0.9353\n",
      "Epoch 342/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8615 - val_loss: 0.9732\n",
      "Epoch 343/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8569 - val_loss: 0.9479\n",
      "Epoch 344/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8732 - val_loss: 0.9591\n",
      "Epoch 345/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8624 - val_loss: 0.9558\n",
      "Epoch 346/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8547 - val_loss: 0.9426\n",
      "Epoch 347/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8517 - val_loss: 0.9403\n",
      "Epoch 348/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8711 - val_loss: 0.9782\n",
      "Epoch 349/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8597 - val_loss: 0.9654\n",
      "Epoch 350/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8535 - val_loss: 0.9649\n",
      "Epoch 351/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8472 - val_loss: 0.9556\n",
      "Epoch 352/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8558 - val_loss: 0.9841\n",
      "Epoch 353/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8520 - val_loss: 0.9501\n",
      "Epoch 354/500\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.8477 - val_loss: 0.9718\n",
      "Epoch 355/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8543 - val_loss: 1.0182\n",
      "Epoch 356/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8444 - val_loss: 0.9465\n",
      "Epoch 357/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8497 - val_loss: 0.9306\n",
      "Epoch 358/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8669 - val_loss: 1.0155\n",
      "Epoch 359/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8513 - val_loss: 0.9155\n",
      "Epoch 360/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8585 - val_loss: 0.9554\n",
      "Epoch 361/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8466 - val_loss: 0.9879\n",
      "Epoch 362/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8410 - val_loss: 0.9402\n",
      "Epoch 363/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8438 - val_loss: 0.9534\n",
      "Epoch 364/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8486 - val_loss: 0.9486\n",
      "Epoch 365/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8635 - val_loss: 0.9496\n",
      "Epoch 366/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8387 - val_loss: 0.9308\n",
      "Epoch 367/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8566 - val_loss: 0.9857\n",
      "Epoch 368/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8341 - val_loss: 0.9258\n",
      "Epoch 369/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8478 - val_loss: 0.9190\n",
      "Epoch 370/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8393 - val_loss: 0.9597\n",
      "Epoch 371/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8481 - val_loss: 0.9970\n",
      "Epoch 372/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8424 - val_loss: 0.9112\n",
      "Epoch 373/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8330 - val_loss: 0.9652\n",
      "Epoch 374/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8325 - val_loss: 0.9931\n",
      "Epoch 375/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8279 - val_loss: 0.9382\n",
      "Epoch 376/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8367 - val_loss: 0.9678\n",
      "Epoch 377/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8359 - val_loss: 1.0423\n",
      "Epoch 378/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8295 - val_loss: 0.9524\n",
      "Epoch 379/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8220 - val_loss: 0.9528\n",
      "Epoch 380/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8326 - val_loss: 1.0164\n",
      "Epoch 381/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8339 - val_loss: 0.9242\n",
      "Epoch 382/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8342 - val_loss: 1.0378\n",
      "Epoch 383/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8370 - val_loss: 0.9442\n",
      "Epoch 384/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8312 - val_loss: 0.9759\n",
      "Epoch 385/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.8291 - val_loss: 1.0151\n",
      "Epoch 386/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.8174 - val_loss: 0.9525\n",
      "Epoch 387/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8141 - val_loss: 0.9693\n",
      "Epoch 388/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8173 - val_loss: 0.9793\n",
      "Epoch 389/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8220 - val_loss: 1.0493\n",
      "Epoch 390/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8239 - val_loss: 0.9946\n",
      "Epoch 391/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8202 - val_loss: 0.9400\n",
      "Epoch 392/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8245 - val_loss: 0.9478\n",
      "Epoch 393/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8334 - val_loss: 0.9452\n",
      "Epoch 394/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8141 - val_loss: 0.9741\n",
      "Epoch 395/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8089 - val_loss: 0.9646\n",
      "Epoch 396/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8113 - val_loss: 1.0157\n",
      "Epoch 397/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8110 - val_loss: 0.9509\n",
      "Epoch 398/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8134 - val_loss: 0.9703\n",
      "Epoch 399/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8120 - val_loss: 0.9721\n",
      "Epoch 400/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8057 - val_loss: 0.9865\n",
      "Epoch 401/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8077 - val_loss: 0.9990\n",
      "Epoch 402/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8159 - val_loss: 0.9695\n",
      "Epoch 403/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8137 - val_loss: 0.9755\n",
      "Epoch 404/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8035 - val_loss: 0.9656\n",
      "Epoch 405/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8027 - val_loss: 0.9936\n",
      "Epoch 406/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8139 - val_loss: 0.9379\n",
      "Epoch 407/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8167 - val_loss: 0.9282\n",
      "Epoch 408/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8366 - val_loss: 1.0561\n",
      "Epoch 409/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8194 - val_loss: 0.9670\n",
      "Epoch 410/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8183 - val_loss: 0.9616\n",
      "Epoch 411/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8206 - val_loss: 0.9742\n",
      "Epoch 412/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8105 - val_loss: 1.0015\n",
      "Epoch 413/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7948 - val_loss: 0.9626\n",
      "Epoch 414/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8137 - val_loss: 1.0368\n",
      "Epoch 415/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8143 - val_loss: 0.9651\n",
      "Epoch 416/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7910 - val_loss: 0.9546\n",
      "Epoch 417/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7977 - val_loss: 0.9887\n",
      "Epoch 418/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7939 - val_loss: 0.9507\n",
      "Epoch 419/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7926 - val_loss: 0.9704\n",
      "Epoch 420/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7958 - val_loss: 0.9965\n",
      "Epoch 421/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7924 - val_loss: 0.9763\n",
      "Epoch 422/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7926 - val_loss: 1.0457\n",
      "Epoch 423/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7864 - val_loss: 0.9633\n",
      "Epoch 424/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7947 - val_loss: 1.0218\n",
      "Epoch 425/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7936 - val_loss: 0.9810\n",
      "Epoch 426/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7932 - val_loss: 1.0374\n",
      "Epoch 427/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7895 - val_loss: 1.0087\n",
      "Epoch 428/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7874 - val_loss: 1.0019\n",
      "Epoch 429/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7899 - val_loss: 0.9987\n",
      "Epoch 430/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7724 - val_loss: 1.0380\n",
      "Epoch 431/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7964 - val_loss: 0.9366\n",
      "Epoch 432/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8263 - val_loss: 0.9924\n",
      "Epoch 433/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7922 - val_loss: 1.0252\n",
      "Epoch 434/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7841 - val_loss: 0.9789\n",
      "Epoch 435/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7822 - val_loss: 1.0102\n",
      "Epoch 436/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7825 - val_loss: 1.0382\n",
      "Epoch 437/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8068 - val_loss: 0.9797\n",
      "Epoch 438/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8173 - val_loss: 0.9156\n",
      "Epoch 439/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8052 - val_loss: 1.0081\n",
      "Epoch 440/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7962 - val_loss: 1.0493\n",
      "Epoch 441/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7823 - val_loss: 0.9584\n",
      "Epoch 442/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7829 - val_loss: 0.9959\n",
      "Epoch 443/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7720 - val_loss: 0.9518\n",
      "Epoch 444/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7781 - val_loss: 1.0526\n",
      "Epoch 445/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7751 - val_loss: 0.9548\n",
      "Epoch 446/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7803 - val_loss: 0.9921\n",
      "Epoch 447/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7755 - val_loss: 1.0144\n",
      "Epoch 448/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8047 - val_loss: 0.9919\n",
      "Epoch 449/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7924 - val_loss: 0.9948\n",
      "Epoch 450/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7774 - val_loss: 0.9775\n",
      "Epoch 451/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8060 - val_loss: 0.9398\n",
      "Epoch 452/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7901 - val_loss: 0.9991\n",
      "Epoch 453/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7664 - val_loss: 0.9813\n",
      "Epoch 454/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7566 - val_loss: 1.0007\n",
      "Epoch 455/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7590 - val_loss: 1.0144\n",
      "Epoch 456/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7622 - val_loss: 1.0001\n",
      "Epoch 457/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7775 - val_loss: 1.0044\n",
      "Epoch 458/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7823 - val_loss: 1.0118\n",
      "Epoch 459/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7595 - val_loss: 0.9783\n",
      "Epoch 460/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7608 - val_loss: 1.0499\n",
      "Epoch 461/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7590 - val_loss: 0.9766\n",
      "Epoch 462/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7529 - val_loss: 1.0599\n",
      "Epoch 463/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7831 - val_loss: 0.9599\n",
      "Epoch 464/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8000 - val_loss: 1.0483\n",
      "Epoch 465/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7573 - val_loss: 0.9833\n",
      "Epoch 466/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7505 - val_loss: 0.9997\n",
      "Epoch 467/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7552 - val_loss: 1.0154\n",
      "Epoch 468/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7517 - val_loss: 1.0202\n",
      "Epoch 469/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7524 - val_loss: 1.0259\n",
      "Epoch 470/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7499 - val_loss: 1.0081\n",
      "Epoch 471/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7586 - val_loss: 1.0427\n",
      "Epoch 472/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7689 - val_loss: 0.9800\n",
      "Epoch 473/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.7638 - val_loss: 1.0581\n",
      "Epoch 474/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7570 - val_loss: 1.0199\n",
      "Epoch 475/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7564 - val_loss: 0.9879\n",
      "Epoch 476/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7497 - val_loss: 1.0307\n",
      "Epoch 477/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7591 - val_loss: 1.0460\n",
      "Epoch 478/500\n",
      " 8/16 [==============>...............] - ETA: 0s - loss: 0.8959"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_uni_stacked.fit(india_cases_train_X,\n",
    "          india_cases_train_y, \n",
    "          validation_data=(india_cases_test_X, india_cases_test_y),\n",
    "          epochs=500,\n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acc2a67",
   "metadata": {},
   "source": [
    "### Univariate stacked "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a274610",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d40ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = np.array([india_cases_test_X[25]])\n",
    "print(x_input)\n",
    "yhat = model.predict(x_input)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3261c5d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
