{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b680a614",
   "metadata": {},
   "source": [
    "# LSTM "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdb1a44",
   "metadata": {},
   "source": [
    "## Outline\n",
    "1. [Imports](#imports)\n",
    "2. [Data](#data)\n",
    "3. [Model](#model)\n",
    "5. [Train](#train)\n",
    "6. [Predict](#predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b23d55",
   "metadata": {},
   "source": [
    "<a name=imports></a>\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0baed98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc101f4",
   "metadata": {},
   "source": [
    "<a name=data></a>\n",
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7846f422",
   "metadata": {},
   "source": [
    "### Ingestion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d4e3df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent Directory /deep-co-training\n",
      "Path: /deep-co-training/cleaned_datasets/india/daily_cases_india.csv\n",
      "Exists: True\n"
     ]
    }
   ],
   "source": [
    "# Get the path of the current working directory\n",
    "curPath = os.getcwd()\n",
    "# Appened the parent directory to the current path to step out of the current folder\n",
    "parentDir = os.path.abspath(os.path.join(curPath, os.pardir))\n",
    "print(\"Parent Directory\", parentDir)\n",
    "# Save the path to all of the datasets\n",
    "india_cases_path = os.path.join(parentDir, \"cleaned_datasets/india/daily_cases_india.csv\")\n",
    "india_vacc_path = os.path.join(parentDir, \"cleaned_datasets/india/daily_vacc_india.csv\")\n",
    "usa_cases_path = os.path.join(parentDir, \"cleaned_datasets/usa/daily_cases_usa.csv\")\n",
    "usa_vacc_path = os.path.join(parentDir, \"cleaned_datasets/usa/vacc_usa.csv\")\n",
    "\n",
    "# Quick check to make sure the path exists\n",
    "print(\"Path:\", india_cases_path)\n",
    "print(\"Exists:\", os.path.exists(india_cases_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8bb50f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India Cases:\n",
      "    Unnamed: 0        Date  Confirmed  Deaths  Recovered  Active\n",
      "0           0  2020-01-30        1.0     0.0        0.0     0.0\n",
      "1           1  2020-01-31        0.0     0.0        0.0     0.0\n",
      "2           2  2020-02-01        0.0     0.0        0.0     0.0\n",
      "3           3  2020-02-02        1.0     0.0        0.0     0.0\n",
      "4           4  2020-02-03        1.0     0.0        0.0     0.0 \n",
      "\n",
      "India Vacc:\n",
      "    Updated On  Total_Doses  First_Dose  Second_Dose\n",
      "0  2021-01-16          NaN         NaN          NaN\n",
      "1  2021-01-17      20656.0     20656.0          0.0\n",
      "2  2021-01-18      81690.0     81690.0          0.0\n",
      "3  2021-01-19     192152.0    192152.0          0.0\n",
      "4  2021-01-20     111510.0    111510.0          0.0 \n",
      "\n",
      "USA Cases:\n",
      "          Date  Confirmed  Deaths  Recovered\n",
      "0  2020-04-12        NaN     NaN        NaN\n",
      "1  2020-04-13    25322.0  1546.0    11785.0\n",
      "2  2020-04-14    26713.0  2305.0     6484.0\n",
      "3  2020-04-15    29380.0  2478.0     6093.0\n",
      "4  2020-04-16    31542.0  4616.0     5234.0 \n",
      "\n",
      "USA Vacc:\n",
      "          date  total_doses  people_vacc  people_fully_vacc  daily_vacc\n",
      "0  2020-12-20     556208.0          0.0                0.0         0.0\n",
      "1  2020-12-21     614117.0          0.0                0.0     57909.0\n",
      "2  2020-12-22          0.0          0.0                0.0    127432.0\n",
      "3  2020-12-23    1008025.0          0.0                0.0    150606.0\n",
      "4  2020-12-24          0.0          0.0                0.0    191001.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the data as a pandas dataframe\n",
    "india_cases_df = pd.read_csv(india_cases_path)\n",
    "india_vacc_df =  pd.read_csv(india_vacc_path)\n",
    "\n",
    "usa_cases_df = pd.read_csv(usa_cases_path)\n",
    "usa_vacc_df = pd.read_csv(usa_vacc_path)\n",
    "\n",
    "# Visualize the datasets\n",
    "print('India Cases:\\n',india_cases_df.head(),'\\n')\n",
    "print('India Vacc:\\n',india_vacc_df.head(),'\\n')\n",
    "\n",
    "print('USA Cases:\\n',usa_cases_df.head(),'\\n')\n",
    "print('USA Vacc:\\n',usa_vacc_df.head(),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01edd055",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "438c9fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India Cases:\n",
      "    Confirmed\n",
      "1        0.0\n",
      "2        0.0\n",
      "3        1.0\n",
      "4        1.0\n",
      "5        0.0 \n",
      "\n",
      "USA Cases:\n",
      "    Confirmed\n",
      "1    25322.0\n",
      "2    26713.0\n",
      "3    29380.0\n",
      "4    31542.0\n",
      "5    32022.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select only the Confirmed column for univariate analysis\n",
    "# Selecting from the first index because the 0th index is NaN\n",
    "india_cases_df = india_cases_df[[\"Confirmed\"]][1:]\n",
    "usa_cases_df = usa_cases_df[[\"Confirmed\"]][1:]\n",
    "\n",
    "# Visualize the datasets\n",
    "print('India Cases:\\n',india_cases_df.head(),'\\n')\n",
    "print('USA Cases:\\n',usa_cases_df.head(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55483a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India Cases:\n",
      "    Confirmed\n",
      "1  -0.687995\n",
      "2  -0.687995\n",
      "3  -0.687983\n",
      "4  -0.687983\n",
      "5  -0.687995 \n",
      "\n",
      "USA Cases:\n",
      "    Confirmed\n",
      "1  -0.817861\n",
      "2  -0.797170\n",
      "3  -0.757499\n",
      "4  -0.725340\n",
      "5  -0.718200 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalize the data\n",
    "india_cases_mean = india_cases_df.mean()\n",
    "india_cases_std = india_cases_df.std()\n",
    "\n",
    "usa_cases_mean = usa_cases_df.mean()\n",
    "usa_cases_std = usa_cases_df.std()\n",
    "\n",
    "\n",
    "india_cases_df = (india_cases_df-india_cases_mean)/india_cases_std\n",
    "usa_cases_df = (usa_cases_df-usa_cases_mean)/usa_cases_std\n",
    "\n",
    "# Visualize the datasets\n",
    "print('India Cases:\\n',india_cases_df.head(),'\\n')\n",
    "print('USA Cases:\\n',usa_cases_df.head(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b5cc363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India Cases:\n",
      "      Confirmed\n",
      "249   0.081848\n",
      "135  -0.544021\n",
      "68   -0.681298\n",
      "433   0.905157\n",
      "427   0.335657\n",
      "..         ...\n",
      "601  -0.286870\n",
      "552  -0.147910\n",
      "370  -0.525914\n",
      "452   3.370913\n",
      "491   0.826498\n",
      "\n",
      "[496 rows x 1 columns] \n",
      "\n",
      "USA Cases:\n",
      "      Confirmed\n",
      "146  -0.542292\n",
      "507   1.810341\n",
      "209   0.764469\n",
      "39   -0.815258\n",
      "69   -0.702091\n",
      "..         ...\n",
      "258   2.055507\n",
      "445  -0.979431\n",
      "249   2.338424\n",
      "162  -0.439359\n",
      "87   -0.314009\n",
      "\n",
      "[438 rows x 1 columns] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create train test splits\n",
    "india_cases_train, india_cases_test = train_test_split(india_cases_df, test_size=0.2)\n",
    "india_vacc_train, india_vacc_test = train_test_split(india_vacc_df, test_size=0.2)\n",
    "\n",
    "usa_cases_train, usa_cases_test = train_test_split(usa_cases_df, test_size=0.2)\n",
    "usa_vacc_train, usa_vacc_test = train_test_split(usa_vacc_df, test_size=0.2)\n",
    "\n",
    "# Visualize splits\n",
    "print('India Cases:\\n',india_cases_train,'\\n')\n",
    "print('USA Cases:\\n',usa_cases_train,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01face15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India Cases:\n",
      " [ 8.18483055e-02 -5.44020939e-01 -6.81297767e-01  9.05157440e-01\n",
      "  3.35656510e-01 -5.99646258e-02  1.08285875e-01 -4.83041128e-01\n",
      "  7.39840137e-03  4.75622330e-01 -4.26647681e-01 -6.87530198e-01\n",
      " -6.87944855e-01 -2.47564180e-02  4.05419526e-01 -3.72955793e-01\n",
      "  3.27715187e-01 -4.52620331e-01 -5.51484778e-01 -6.87743809e-01\n",
      "  2.84772821e+00 -2.27448638e-01 -1.26523468e-01 -5.17281801e-01\n",
      " -3.77479331e-01 -4.80616008e-01 -4.06140973e-01  3.23166518e-01\n",
      " -1.37053261e-01 -1.60060480e-01  3.74747258e+00 -3.09864996e-01\n",
      "  9.78189094e-02 -1.98585949e-01 -5.25612651e-01 -1.62636384e-01\n",
      " -6.77339671e-01 -6.26877086e-01 -2.48520788e-01  5.53552846e-01\n",
      "  2.41114556e-01 -3.59850096e-01 -6.64208844e-01 -6.87982552e-01\n",
      " -3.71774646e-01 -3.54937031e-01 -5.26140397e-01  2.59652104e+00\n",
      " -6.76409832e-01 -1.87943069e-01 -6.87969986e-01 -6.66546005e-01\n",
      " -3.80243716e-01  3.66950437e+00 -9.41299067e-02 -6.50763882e-01\n",
      " -6.42923082e-01 -4.87351055e-01 -1.93346184e-01 -1.11206265e-01\n",
      "  4.36320156e+00  6.13250989e-01  1.90589144e-01 -1.52483553e-01\n",
      "  4.50830826e-01 -6.77025536e-01 -4.43485297e-01 -6.40711574e-01\n",
      " -6.87995117e-01 -4.60159562e-01 -6.87995117e-01 -4.10739904e-01\n",
      " -4.39954424e-01 -6.87995117e-01 -5.29872316e-01  1.83177206e-02\n",
      "  5.25205338e-01 -4.51074789e-01 -5.22219997e-01 -1.97065538e-01\n",
      "  3.32721674e-03  1.96871837e-01 -2.06615230e-01 -6.87995117e-01\n",
      " -6.10114863e-01 -3.71950562e-01 -6.87919725e-01  1.25387363e-01\n",
      "  1.16566463e-01 -6.87995117e-01 -6.73042309e-01 -2.24747080e-01\n",
      "  2.64574129e-01  4.84820192e-01 -1.14083738e-01  2.47397249e-01\n",
      " -6.86474705e-01 -3.83309670e-01 -6.87982552e-01 -2.09593226e-01\n",
      " -6.57310448e-01 -6.87995117e-01 -6.72313517e-01 -3.15858685e-01\n",
      " -4.13793293e-01  2.20859156e-01 -6.70780540e-01 -1.63603919e-01\n",
      " -5.59602016e-01  7.80284286e-02 -2.68612838e-01  7.50238812e-01\n",
      "  4.25297965e-01  1.00030417e-01 -3.75380912e-01 -4.82236943e-01\n",
      " -4.94324843e-01  5.30621019e-01 -6.68380551e-01 -4.60712439e-01\n",
      " -6.75002509e-01 -4.62810858e-01 -6.87995117e-01  4.02944145e-01\n",
      " -4.99652566e-01 -3.46518223e-01  7.66272243e-01 -5.14391762e-01\n",
      " -6.57913586e-01 -6.74072671e-01 -3.68784085e-01  1.49574308e+00\n",
      " -5.14743593e-01 -6.33297997e-01 -3.56495139e-01  9.37225940e-02\n",
      "  3.02077869e+00 -1.41287795e-01  9.70901171e-02 -5.42111001e-01\n",
      "  7.81792132e-02 -5.73386243e-01 -1.13505730e-01 -6.83959989e-02\n",
      " -5.55003086e-01 -6.87932290e-01  3.86980917e+00 -2.74518569e-01\n",
      "  3.69004877e+00 -2.84897577e-01 -2.28164865e-01 -2.15624611e-01\n",
      " -4.57470570e-01 -7.33467605e-02 -4.24725177e-01 -1.83871884e-01\n",
      "  1.23138160e-01 -5.41407339e-01 -1.52395595e-01 -3.24817804e-01\n",
      "  1.36433975e-02  2.26100014e+00 -1.20693130e-01 -5.28552951e-01\n",
      " -6.48879074e-01 -4.87351055e-01 -4.59732339e-01 -1.98598515e-01\n",
      " -3.18912073e-01 -3.04135181e-01 -6.77829721e-01  7.53896978e-02\n",
      " -6.12211643e-02 -6.07400740e-01 -3.69839577e-01 -1.70841580e-01\n",
      " -6.87995117e-01 -3.30962277e-01 -7.60860144e-02 -3.03871308e-01\n",
      "  1.82019552e-01 -6.04699182e-01 -5.71023951e-01 -6.24539924e-01\n",
      " -5.86152674e-01 -5.35413651e-01  3.10464553e-02 -1.06808380e-01\n",
      "  3.02647245e-01 -2.09517834e-01 -4.55259062e-01  3.99011180e-01\n",
      "  2.98689149e-01 -5.90475167e-01 -3.45538123e-01 -3.33601008e-01\n",
      " -1.63878718e-02 -2.41283127e-01 -6.87995117e-01 -1.50623876e-01\n",
      " -6.10755697e-01 -6.85984655e-01 -6.86160571e-01  1.90023702e-01\n",
      "  8.15341709e-02 -8.94053221e-02  4.35170423e+00 -4.70664224e-01\n",
      " -2.45970015e-01  1.98982821e-01 -6.82504044e-01 -6.87819202e-01\n",
      " -6.87995117e-01  3.45785303e+00  2.62339840e+00 -8.59733331e-03\n",
      " -4.00825816e-01 -6.87995117e-01 -1.71557807e-01  1.62781948e-01\n",
      " -1.44378880e-01 -2.37262204e-01 -3.54170542e-01 -5.08146766e-01\n",
      " -4.42266454e-01 -7.71792028e-02 -6.87995117e-01  1.65374023e+00\n",
      " -6.87706113e-01 -8.91146793e-03 -3.75343216e-01 -6.18219536e-01\n",
      " -4.36423550e-01 -6.87995117e-01 -6.87995117e-01  1.92098630e-02\n",
      " -2.95678677e-01 -3.26966485e-01 -3.49182085e-01 -6.87995117e-01\n",
      " -1.76395480e-01 -4.51828712e-01 -1.73805372e-02 -5.08762470e-01\n",
      " -4.77499793e-01 -2.00810023e-01  4.11381637e+00 -6.77503021e-01\n",
      "  1.49752737e+00 -1.52207115e-01  3.49528531e+00  8.52409594e-02\n",
      " -5.83250070e-01 -1.24073218e-01 -2.35503050e-01  1.21203090e-01\n",
      " -5.50102586e-01 -6.87957421e-01 -5.00356228e-01 -1.31587318e-01\n",
      "  4.51643643e+00 -4.00976600e-01  4.71676799e-01 -9.80377414e-02\n",
      " -6.81146982e-01  4.86403430e-01 -6.38588024e-01 -4.47393131e-01\n",
      " -2.77408608e-01 -6.14575574e-01 -6.87995117e-01 -6.49256036e-01\n",
      " -5.33616801e-01 -5.61713001e-01  1.39224201e+00  1.23298833e+00\n",
      "  3.40955169e+00 -1.19650203e-01 -1.42745380e-01 -6.78457990e-01\n",
      " -4.58249624e-01 -5.05231597e-01 -5.79493020e-01 -2.73576165e-01\n",
      " -6.87831767e-01 -6.87693548e-01 -6.87995117e-01 -8.60503644e-02\n",
      "  9.97701498e-01 -6.87919725e-01 -6.87530198e-01  5.42080650e-01\n",
      " -3.75745308e-01 -6.87165802e-01 -4.08126304e-01  9.38733787e-02\n",
      " -2.23815603e-02 -1.26850168e-01 -3.28738204e-01 -4.69269466e-01\n",
      "  1.34427994e+00 -1.96512661e-01 -1.70364096e-01 -1.48827026e-01\n",
      "  2.56853793e+00 -4.90704373e-02 -6.87706113e-01 -1.62372511e-01\n",
      " -5.77155859e-01  2.54506579e+00 -5.44297378e-01 -6.66156478e-01\n",
      " -6.05465670e-01 -3.04147746e-01 -2.21417253e-01 -2.01752426e-01\n",
      " -4.93106001e-01 -2.91921627e-01  3.59291999e-01  5.48452939e-02\n",
      " -4.32339800e-01 -5.08862993e-01  2.32695748e-01 -1.18129791e-01\n",
      " -5.32083824e-01 -6.81523944e-01 -3.10644050e-01 -1.60486064e-02\n",
      " -5.48934005e-01 -2.06074919e-01 -6.87995117e-01 -5.56837632e-01\n",
      " -6.45976471e-01 -1.30393607e-01 -6.86914494e-01  3.88694999e-01\n",
      " -4.96448393e-01 -1.21509880e-01  3.98056211e-01 -2.28787125e-04\n",
      "  5.23609534e-01 -5.12167689e-01 -4.54291527e-01 -4.02094920e-01\n",
      "  9.80537182e-01 -3.39668450e-02 -1.32228153e-01 -6.67802544e-01\n",
      " -2.30602550e-01 -3.60779935e-01 -5.68988359e-01  4.36369708e-02\n",
      " -4.25340881e-01  2.78143324e+00 -2.55029657e-01 -5.49285836e-01\n",
      "  1.96704553e+00 -5.81918140e-01  4.83714438e-01  2.04312894e+00\n",
      " -2.41936527e-01 -7.11341602e-01 -4.54882101e-01  3.91714297e+00\n",
      "  1.43444914e+00 -5.36469143e-01  1.66702348e-01 -6.87995117e-01\n",
      " -2.10824634e-01 -6.87969986e-01 -2.08449776e-01 -6.87995117e-01\n",
      " -6.80443321e-01 -3.18208412e-01 -1.10603126e-01 -7.49802605e-02\n",
      " -2.55708188e-01  2.33827726e+00 -4.37868570e-01 -3.47297277e-01\n",
      "  3.93682036e+00  1.93717761e+00 -4.86541407e-03 -4.29789027e-01\n",
      " -4.00373462e-01  4.16921715e+00 -1.60688749e-01 -6.40410005e-01\n",
      "  4.75509242e-01  3.59216607e-01 -6.87995117e-01  1.50518133e-01\n",
      " -8.14891298e-02  2.46655891e-01  7.18462593e-02 -2.34598342e-01\n",
      " -1.99641442e-01 -9.62660221e-02 -1.00450295e-01  4.97825365e-01\n",
      " -2.45040177e-01  1.56499256e-01 -5.39334051e-01 -1.68001803e-01\n",
      " -3.29906785e-01 -1.93409011e-01 -6.87995117e-01 -4.20892735e-01\n",
      " -3.32068031e-01 -2.15963876e-01 -2.28466434e-01 -1.28973718e-01\n",
      " -1.07926699e-01 -3.67992466e-01 -1.15315145e-01 -4.08402743e-01\n",
      " -4.51351227e-01 -6.87995117e-01 -2.28051777e-01 -6.80393059e-01\n",
      " -5.09579220e-01 -5.73624986e-01 -4.09433104e-01 -2.39888369e-01\n",
      " -4.92100770e-01 -2.02619438e-01 -2.99209550e-01  3.84719148e+00\n",
      "  8.94377979e-02 -1.83695969e-01  5.38009465e-01  2.98538364e-01\n",
      " -4.97529016e-01  3.70600681e+00  1.21732475e-02 -1.36462688e-01\n",
      " -2.13777500e-01 -3.07816838e-01 -6.87995117e-01  1.62870999e+00\n",
      "  1.13881077e+00 -4.89173035e-01 -6.87882028e-01 -4.79698735e-01\n",
      " -9.78869568e-02 -7.67142836e-02  9.18629171e-02 -4.76419170e-01\n",
      " -4.81520716e-01 -3.53454316e-01  2.67115943e+00 -6.87995117e-01\n",
      " -5.70871527e-02 -6.87995117e-01 -1.44779333e-02 -3.95410135e-01\n",
      " -4.01868743e-01  4.38094389e+00 -1.11344484e-01 -5.81049489e-02\n",
      " -1.68768292e-01 -4.80817055e-01  3.71668903e-01 -5.15321601e-01\n",
      " -6.45725163e-01 -3.92344181e-01 -3.47837589e-01 -6.87517632e-01\n",
      " -5.98617536e-01 -5.05595993e-01 -1.80039442e-01 -2.29735538e-01\n",
      " -6.87995117e-01  3.80181787e+00 -3.28549723e-01 -5.24092239e-01\n",
      " -1.46791434e-01 -5.01411720e-01 -2.99121592e-01 -2.48545919e-01\n",
      " -5.96355767e-01  1.77759887e-01  3.71657976e-02 -1.00060768e-01\n",
      " -1.19926641e-01  2.57336304e+00  9.43006017e-02 -2.43909292e-01\n",
      " -6.65364859e-01 -2.21140815e-01 -4.77675708e-01 -2.86870342e-01\n",
      " -1.47909753e-01 -5.25914220e-01  3.37091313e+00  8.26498132e-01] \n",
      "\n",
      "USA Cases:\n",
      " [-5.42291769e-01  1.81034141e+00  7.64468836e-01 -8.15258086e-01\n",
      " -7.02091079e-01  1.85892231e+00 -2.98732810e-01 -8.32706153e-01\n",
      "  2.46024719e-01 -3.33674845e-02 -3.80365372e-01 -5.92761755e-01\n",
      " -1.03393161e+00 -5.43570997e-01 -7.85449111e-01  1.80686072e+00\n",
      " -3.36291523e-01 -5.26063430e-01 -7.88037315e-01 -2.64119295e-01\n",
      "  8.52110792e-01 -7.06434503e-01  2.14497839e+00 -5.73841088e-01\n",
      " -2.22574151e-01  5.96205800e-01 -5.22286522e-02 -6.64115873e-01\n",
      " -5.11203567e-01  1.52238136e+00 -5.95245836e-01  3.44852478e-01\n",
      " -1.12591699e+00 -2.91191318e-01 -9.22638819e-01  1.16516453e+00\n",
      "  2.28994759e+00 -8.98392996e-01  1.55504628e+00 -1.66853383e-01\n",
      "  1.23248632e-01  9.60116264e-01 -5.45430339e-01  2.08858826e+00\n",
      " -6.08573602e-01 -7.64029488e-01 -1.91352076e-01 -4.69524551e-01\n",
      " -4.58591619e-01 -3.36440271e-01 -7.69339770e-01 -7.57499478e-01\n",
      "  1.11548290e+00 -6.93746351e-01 -4.30850232e-01 -3.66323619e-01\n",
      "  2.31194605e-01 -2.59984117e-01 -4.44966359e-01 -1.76878956e-01\n",
      " -5.61257060e-01  7.11767639e-01  2.46752221e+00 -1.73234646e-01\n",
      "  1.96932261e+00  1.13767601e+00 -3.70116677e-01 -2.45674619e-01\n",
      " -1.02060384e+00 -7.14154492e-01  4.35826375e-01 -4.36279512e-01\n",
      " -7.67703548e-01  2.07860731e+00 -4.22625778e-02 -8.18322282e-01\n",
      " -3.52222368e-01 -1.94951763e-01 -9.07213716e-01 -7.88438933e-01\n",
      " -1.92066064e-01 -5.93550116e-01  1.13620341e+00 -4.25078834e-01\n",
      "  1.79744501e+00 -7.11060546e-01 -5.03096835e-01  2.22411200e+00\n",
      " -3.23857518e-02  7.33722752e-01 -3.19393821e-01 -8.37674316e-01\n",
      "  1.82595989e+00 -3.97858064e-01 -9.09370553e-01  1.39732944e+00\n",
      " -7.34027142e-01 -6.78782365e-01 -7.25340295e-01 -8.34654744e-01\n",
      "  4.50537491e-01 -4.81736711e-01 -6.02608832e-01  1.38065486e+00\n",
      "  6.45575055e-01 -9.08254947e-01 -6.44124225e-01  1.55754524e+00\n",
      "  1.66638242e-01  2.06621538e-01 -4.90527681e-01 -8.64389345e-01\n",
      "  1.66120729e+00 -1.12823745e+00 -6.79823596e-01  1.65060160e+00\n",
      " -5.98265408e-01 -6.55875268e-01 -9.02528173e-01  6.39045045e-01\n",
      " -9.00817578e-01 -9.43165957e-01 -1.76209593e-01 -9.17804529e-01\n",
      " -7.87263829e-01 -5.54474180e-01 -9.97116632e-01 -5.68471308e-01\n",
      " -9.13907348e-01 -2.35946541e-01 -9.81751028e-01  6.49189616e-01\n",
      "  1.68877018e+00  1.34684458e+00 -7.56978862e-01 -3.61816573e-01\n",
      "  8.99114964e-01  4.64058628e-01 -7.08011225e-01  2.67224323e+00\n",
      " -5.77916767e-01 -5.34720528e-01  1.90049720e+00 -8.17861165e-01\n",
      "  4.50818835e-02 -1.24624002e-01  1.28591765e+00 -7.75483036e-01\n",
      "  2.24486226e+00  2.34595098e+00 -9.28082973e-01  1.46196017e+00\n",
      " -6.15609353e-01 -1.74068907e-02 -3.14157913e-01 -1.64756045e-01\n",
      " -2.97051964e-01 -1.39811109e-01 -5.65555859e-01  3.31264532e+00\n",
      "  2.19607184e-01 -9.77734848e-01  3.86784364e-01  1.19635685e+00\n",
      " -5.16781594e-01  1.85448963e+00 -3.50258902e-01 -4.34330921e-01\n",
      " -6.84330642e-01  2.42020567e+00 -6.60322815e-01 -6.15311858e-01\n",
      " -5.73350222e-01 -8.16135695e-01 -4.83447306e-01 -2.16713505e-01\n",
      "  9.79379050e-01 -6.78038628e-01  2.99425028e-01 -6.97064693e-02\n",
      " -7.18200420e-01 -3.72883379e-01 -3.78089537e-01 -1.00318553e+00\n",
      " -3.14232287e-01 -1.00785619e+00 -1.78143309e-01 -2.50598158e-01\n",
      " -7.99029746e-01 -4.01026383e-01 -9.06767474e-01 -1.01083114e+00\n",
      " -9.69062876e-01 -1.06528756e+00 -7.29817591e-01 -8.17266175e-01\n",
      " -3.78387032e-01 -2.23258389e-01 -3.22816286e-02  9.57870179e-01\n",
      " -6.94817332e-01  1.80541787e+00 -5.89563686e-01 -4.35744021e-01\n",
      " -1.98462201e-01 -8.91297746e-01 -7.86564716e-01  1.11002387e+00\n",
      " -6.11340303e-01 -4.51793863e-01  1.56171017e+00  9.62734218e-01\n",
      " -2.38742992e-01 -7.03950421e-01 -3.41378684e-01 -4.21137028e-01\n",
      " -3.74311353e-01  1.42997949e+00  2.50173411e+00 -1.85179060e-01\n",
      "  1.30385658e+00 -6.59460080e-01  4.67627289e-02 -4.83551429e-01\n",
      " -8.57204847e-01 -3.02972110e-01 -5.32310820e-01  1.15991374e+00\n",
      " -4.44088749e-01 -1.02980417e-03 -6.95620568e-01 -1.99027441e-01\n",
      "  1.00725431e+00  2.90396062e-01 -6.76610653e-01 -6.00719740e-01\n",
      "  3.72281495e-01 -2.00395917e-01 -8.65504950e-01 -5.68025066e-01\n",
      "  9.31155149e-01 -8.52400306e-01  1.80938943e+00  2.59974376e+00\n",
      " -3.14024040e-01 -5.93713738e-01  9.88838107e-02  2.09958069e+00\n",
      " -8.19452762e-01 -1.13128677e+00 -9.43241607e-02 -7.90180553e-02\n",
      " -4.57684260e-01 -1.07149032e+00  3.33845172e-01 -8.86314709e-01\n",
      " -1.02067822e+00 -5.06964267e-01  2.10074092e+00 -4.87136241e-01\n",
      " -3.72675132e-01  5.18396046e-01 -4.49458530e-01 -2.90849199e-01\n",
      " -3.14500032e-01 -8.22442584e-01  2.65954020e+00 -8.25834024e-01\n",
      " -7.61323561e-02  2.45849324e+00  1.28325507e+00 -1.84539446e-01\n",
      " -2.56384431e-01 -4.11170954e-01 -8.36469462e-01  4.11729300e-01\n",
      "  6.36025473e-01 -1.10665420e+00 -5.17436082e-01 -1.77548320e-01\n",
      " -6.53108567e-01  1.34928403e+00 -8.79264083e-01  5.04532790e-01\n",
      " -9.89828010e-01 -8.04563149e-01  1.61882916e+00 -8.38730422e-01\n",
      " -7.72493214e-01  9.16191163e-01 -5.69096047e-01 -4.03525339e-01\n",
      "  1.44241477e+00 -8.52654453e-02 -2.72761517e-01  1.79841187e+00\n",
      " -7.99401615e-01  3.09953495e+00 -5.07068390e-01  1.50067784e-01\n",
      "  2.23446482e+00 -4.85827264e-01 -2.83039961e-01 -6.83988523e-01\n",
      " -2.73416006e-01  1.24075795e+00 -2.05602075e-01 -8.81867162e-01\n",
      "  1.32239051e+00  1.62208673e+00  1.38490903e+00 -5.88909198e-01\n",
      " -9.53057658e-01 -2.83977070e-01  6.04312532e-01 -9.42972586e-01\n",
      "  1.42083152e+00  2.65531578e+00 -1.03338124e+00 -8.67096547e-01\n",
      "  1.32961963e+00 -1.09149684e+00  2.29667097e+00  2.19709947e+00\n",
      " -5.45236967e-01 -5.92644033e-02 -9.45679788e-01 -1.13863489e+00\n",
      "  2.29872368e+00 -1.03634132e+00 -3.84173305e-01 -4.97503933e-01\n",
      "  7.39018159e-01  5.77551603e-02 -5.00716877e-01 -1.16206176e-02\n",
      " -6.52291732e-02 -2.88707236e-01  9.78277043e-02 -4.77512285e-01\n",
      " -7.47622652e-01 -7.56978862e-01 -8.51582196e-01 -6.56604130e-01\n",
      " -8.07880216e-01 -9.38346542e-01 -6.54075425e-01 -8.64389345e-01\n",
      " -1.00126668e+00  1.26600038e+00 -8.72555576e-01 -6.61676416e-01\n",
      " -5.67697822e-01  2.10407286e+00 -7.71035490e-01 -1.84807192e-01\n",
      " -1.00904617e+00 -1.33816590e-01  2.05461436e+00 -5.16945216e-01\n",
      "  8.34438327e-02 -8.26667010e-01  6.54007756e-02 -2.12518829e-01\n",
      " -4.98619539e-01 -3.36975761e-01 -9.23888297e-01 -8.30698064e-01\n",
      " -8.52192060e-01 -3.47908693e-01 -3.96653210e-01 -3.62277690e-01\n",
      " -1.06830713e+00  8.41281983e-01 -2.08353902e-01 -4.34851537e-01\n",
      "  1.06877622e+00 -2.38774017e-02 -8.70562361e-01 -8.19869254e-01\n",
      " -3.29151649e-01 -8.58082456e-01  1.95904417e+00 -3.85512031e-01\n",
      "  2.61798018e+00 -1.76730209e-01 -4.17343970e-01 -7.45391441e-01\n",
      "  2.94189120e-01 -4.62191305e-01  1.65626888e+00 -1.32105995e-01\n",
      " -8.03789663e-01 -4.67204092e-01 -6.85832991e-01  1.58525688e+00\n",
      "  2.35499482e+00 -2.90209585e-01 -8.49618730e-01 -7.77506001e-01\n",
      " -1.03552321e+00 -2.73029263e-01 -9.13327233e-01 -2.46195235e-01\n",
      " -8.08922723e-02 -5.09701218e-01  1.14185581e+00 -1.52871129e-01\n",
      "  1.65626760e-01 -7.59329071e-01 -3.24361983e-01  1.23089599e+00\n",
      " -9.79966059e-01  9.52678895e-01 -6.51011229e-01  2.07842881e+00\n",
      " -7.25786537e-01  2.68414302e+00 -7.93927711e-01 -6.61126051e-01\n",
      " -7.87949342e-02 -6.02370836e-01  1.84208410e+00 -7.97170404e-01\n",
      " -8.18233033e-01  1.54894764e+00  1.17932528e+00  1.56200766e+00\n",
      "  1.25509719e+00 -8.69387257e-01 -2.75527377e-03 -5.29975486e-01\n",
      " -4.00580141e-01  2.05550684e+00 -9.79430569e-01  2.33842436e+00\n",
      " -4.39358583e-01 -3.14009166e-01] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to numpy array\n",
    "india_cases_train, india_cases_test = india_cases_train.to_numpy().flatten(), india_cases_test.to_numpy()\n",
    "usa_cases_train, usa_cases_test = usa_cases_train.to_numpy().flatten(), usa_cases_test.to_numpy()\n",
    "\n",
    "# Visualize outputs\n",
    "print('India Cases:\\n',india_cases_train,'\\n')\n",
    "print('USA Cases:\\n',usa_cases_train,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "613a19ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate data preparation\n",
    "from numpy import array\n",
    "\n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66ca8149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.08184831 -0.54402094 -0.68129777] 0.9051574397524952\n",
      "[-0.54402094 -0.68129777  0.90515744] 0.3356565104389887\n",
      "[-0.68129777  0.90515744  0.33565651] -0.05996462583698051\n",
      "[ 0.90515744  0.33565651 -0.05996463] 0.10828587486923602\n",
      "[ 0.33565651 -0.05996463  0.10828587] -0.48304112761280654\n"
     ]
    }
   ],
   "source": [
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "# split into samples\n",
    "india_cases_train_X, india_cases_train_y = split_sequence(india_cases_train, n_steps)\n",
    "india_cases_test_X, india_cases_test_y = split_sequence(india_cases_test, n_steps)\n",
    "\n",
    "\n",
    "# summarize the data\n",
    "for i in range(0,5):\n",
    "    print(india_cases_train_X[i], india_cases_train_y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "409c73a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India Cases:\n",
      " [[[ 0.08184831]\n",
      "  [-0.54402094]\n",
      "  [-0.68129777]]\n",
      "\n",
      " [[-0.54402094]\n",
      "  [-0.68129777]\n",
      "  [ 0.90515744]]\n",
      "\n",
      " [[-0.68129777]\n",
      "  [ 0.90515744]\n",
      "  [ 0.33565651]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.47767571]\n",
      "  [-0.28687034]\n",
      "  [-0.14790975]]\n",
      "\n",
      " [[-0.28687034]\n",
      "  [-0.14790975]\n",
      "  [-0.52591422]]\n",
      "\n",
      " [[-0.14790975]\n",
      "  [-0.52591422]\n",
      "  [ 3.37091313]]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reshape the data into [samples, timesteps, features]\n",
    "# univariate\n",
    "n_features = 1\n",
    "\n",
    "india_cases_train_X = india_cases_train_X.reshape((india_cases_train_X.shape[0], \n",
    "                                                   india_cases_train_X.shape[1], n_features))\n",
    "india_cases_test_X = india_cases_test_X.reshape((india_cases_test_X.shape[0], \n",
    "                                                 india_cases_test_X.shape[1], n_features))\n",
    "\n",
    "# Visualize outputs\n",
    "print('India Cases:\\n',india_cases_train_X,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4e40f6",
   "metadata": {},
   "source": [
    "<a name=model></a>\n",
    "## Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6119e325",
   "metadata": {},
   "source": [
    "### Univariate non-stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7cc198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.losses import MeanAbsolutePercentageError, MeanAbsoluteError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "14d80147",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_31 (LSTM)               (None, 150)               91200     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 151       \n",
      "=================================================================\n",
      "Total params: 91,351\n",
      "Trainable params: 91,351\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model_uni = Sequential()\n",
    "model_uni.add(LSTM(150, activation='relu', input_shape=(n_steps, n_features)))\n",
    "model_uni.add(Dense(1))\n",
    "model_uni.compile(optimizer='adam',loss=MeanAbsoluteError())\n",
    "model_uni.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acc2a67",
   "metadata": {},
   "source": [
    "### Univariate stacked "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a8645ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_33 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_32 (LSTM)               (None, 3, 100)            40800     \n",
      "_________________________________________________________________\n",
      "lstm_33 (LSTM)               (None, 150)               150600    \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 151       \n",
      "=================================================================\n",
      "Total params: 191,551\n",
      "Trainable params: 191,551\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model_uni_stacked = Sequential()\n",
    "model_uni_stacked.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "model_uni_stacked.add(LSTM(150, activation='relu'))\n",
    "model_uni_stacked.add(Dense(1))\n",
    "model_uni_stacked.compile(optimizer='adam',loss=MeanAbsoluteError())\n",
    "model_uni_stacked.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b14488",
   "metadata": {},
   "source": [
    "<a name=train></a>\n",
    "## Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "38071f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard callback\n",
    "logdir = os.path.join(parentDir+\"/logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8190a5d2",
   "metadata": {},
   "source": [
    "### Univariate non-stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a7019359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "16/16 [==============================] - 1s 28ms/step - loss: 0.5644 - val_loss: 0.5934\n",
      "Epoch 2/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5322 - val_loss: 0.5941\n",
      "Epoch 3/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5243 - val_loss: 0.5954\n",
      "Epoch 4/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5229 - val_loss: 0.5970\n",
      "Epoch 5/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5223 - val_loss: 0.5971\n",
      "Epoch 6/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5216 - val_loss: 0.5946\n",
      "Epoch 7/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5217 - val_loss: 0.5968\n",
      "Epoch 8/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5211 - val_loss: 0.5966\n",
      "Epoch 9/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5193 - val_loss: 0.5968\n",
      "Epoch 10/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5197 - val_loss: 0.5956\n",
      "Epoch 11/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5201 - val_loss: 0.5952\n",
      "Epoch 12/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5171 - val_loss: 0.5969\n",
      "Epoch 13/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5175 - val_loss: 0.5977\n",
      "Epoch 14/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5164 - val_loss: 0.5950\n",
      "Epoch 15/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5166 - val_loss: 0.5959\n",
      "Epoch 16/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5151 - val_loss: 0.5966\n",
      "Epoch 17/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5157 - val_loss: 0.5961\n",
      "Epoch 18/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5153 - val_loss: 0.5952\n",
      "Epoch 19/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5146 - val_loss: 0.5958\n",
      "Epoch 20/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5156 - val_loss: 0.5961\n",
      "Epoch 21/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5145 - val_loss: 0.5964\n",
      "Epoch 22/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5139 - val_loss: 0.5956\n",
      "Epoch 23/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5136 - val_loss: 0.5961\n",
      "Epoch 24/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5147 - val_loss: 0.5955\n",
      "Epoch 25/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5146 - val_loss: 0.5958\n",
      "Epoch 26/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5139 - val_loss: 0.5968\n",
      "Epoch 27/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5132 - val_loss: 0.5968\n",
      "Epoch 28/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5157 - val_loss: 0.5953\n",
      "Epoch 29/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5145 - val_loss: 0.5961\n",
      "Epoch 30/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5141 - val_loss: 0.5968\n",
      "Epoch 31/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5135 - val_loss: 0.5949\n",
      "Epoch 32/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5150 - val_loss: 0.5961\n",
      "Epoch 33/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5145 - val_loss: 0.5952\n",
      "Epoch 34/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5141 - val_loss: 0.5957\n",
      "Epoch 35/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5124 - val_loss: 0.5959\n",
      "Epoch 36/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5123 - val_loss: 0.5959\n",
      "Epoch 37/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5121 - val_loss: 0.5963\n",
      "Epoch 38/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5128 - val_loss: 0.5958\n",
      "Epoch 39/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5123 - val_loss: 0.5965\n",
      "Epoch 40/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5125 - val_loss: 0.5958\n",
      "Epoch 41/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5131 - val_loss: 0.5972\n",
      "Epoch 42/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5122 - val_loss: 0.5947\n",
      "Epoch 43/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5133 - val_loss: 0.5975\n",
      "Epoch 44/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5126 - val_loss: 0.5956\n",
      "Epoch 45/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5112 - val_loss: 0.5958\n",
      "Epoch 46/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5121 - val_loss: 0.5963\n",
      "Epoch 47/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5137 - val_loss: 0.5961\n",
      "Epoch 48/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5150 - val_loss: 0.5969\n",
      "Epoch 49/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5131 - val_loss: 0.5959\n",
      "Epoch 50/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5128 - val_loss: 0.5957\n",
      "Epoch 51/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5125 - val_loss: 0.5965\n",
      "Epoch 52/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5109 - val_loss: 0.5959\n",
      "Epoch 53/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5109 - val_loss: 0.5966\n",
      "Epoch 54/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5126 - val_loss: 0.5964\n",
      "Epoch 55/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5142 - val_loss: 0.5974\n",
      "Epoch 56/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5122 - val_loss: 0.5958\n",
      "Epoch 57/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5122 - val_loss: 0.5961\n",
      "Epoch 58/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5108 - val_loss: 0.5966\n",
      "Epoch 59/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5102 - val_loss: 0.5954\n",
      "Epoch 60/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5109 - val_loss: 0.5961\n",
      "Epoch 61/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5108 - val_loss: 0.5965\n",
      "Epoch 62/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5103 - val_loss: 0.5976\n",
      "Epoch 63/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5103 - val_loss: 0.5960\n",
      "Epoch 64/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5104 - val_loss: 0.5978\n",
      "Epoch 65/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5102 - val_loss: 0.5968\n",
      "Epoch 66/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5102 - val_loss: 0.5965\n",
      "Epoch 67/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5113 - val_loss: 0.5967\n",
      "Epoch 68/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5110 - val_loss: 0.5962\n",
      "Epoch 69/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5106 - val_loss: 0.5961\n",
      "Epoch 70/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5103 - val_loss: 0.5968\n",
      "Epoch 71/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5114 - val_loss: 0.5953\n",
      "Epoch 72/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5122 - val_loss: 0.5964\n",
      "Epoch 73/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5101 - val_loss: 0.5963\n",
      "Epoch 74/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5097 - val_loss: 0.5961\n",
      "Epoch 75/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5107 - val_loss: 0.5962\n",
      "Epoch 76/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5116 - val_loss: 0.5970\n",
      "Epoch 77/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5100 - val_loss: 0.5981\n",
      "Epoch 78/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5092 - val_loss: 0.5960\n",
      "Epoch 79/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5099 - val_loss: 0.5961\n",
      "Epoch 80/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5108 - val_loss: 0.5954\n",
      "Epoch 81/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5104 - val_loss: 0.5973\n",
      "Epoch 82/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5105 - val_loss: 0.5967\n",
      "Epoch 83/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5085 - val_loss: 0.5988\n",
      "Epoch 84/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5109 - val_loss: 0.5972\n",
      "Epoch 85/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5089 - val_loss: 0.5985\n",
      "Epoch 86/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5090 - val_loss: 0.5967\n",
      "Epoch 87/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5092 - val_loss: 0.5967\n",
      "Epoch 88/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5087 - val_loss: 0.5982\n",
      "Epoch 89/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5089 - val_loss: 0.5973\n",
      "Epoch 90/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5087 - val_loss: 0.5965\n",
      "Epoch 91/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5105 - val_loss: 0.5983\n",
      "Epoch 92/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5114 - val_loss: 0.5974\n",
      "Epoch 93/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5088 - val_loss: 0.5976\n",
      "Epoch 94/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5086 - val_loss: 0.5980\n",
      "Epoch 95/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5084 - val_loss: 0.5975\n",
      "Epoch 96/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5104 - val_loss: 0.5970\n",
      "Epoch 97/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5096 - val_loss: 0.5983\n",
      "Epoch 98/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5084 - val_loss: 0.5963\n",
      "Epoch 99/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5098 - val_loss: 0.5973\n",
      "Epoch 100/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5089 - val_loss: 0.5974\n",
      "Epoch 101/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5092 - val_loss: 0.5969\n",
      "Epoch 102/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5089 - val_loss: 0.5984\n",
      "Epoch 103/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5083 - val_loss: 0.5968\n",
      "Epoch 104/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5084 - val_loss: 0.5986\n",
      "Epoch 105/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5077 - val_loss: 0.5976\n",
      "Epoch 106/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5083 - val_loss: 0.5969\n",
      "Epoch 107/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5110 - val_loss: 0.5971\n",
      "Epoch 108/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5098 - val_loss: 0.5979\n",
      "Epoch 109/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5091 - val_loss: 0.5985\n",
      "Epoch 110/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5092 - val_loss: 0.5979\n",
      "Epoch 111/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5080 - val_loss: 0.5982\n",
      "Epoch 112/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5078 - val_loss: 0.5971\n",
      "Epoch 113/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5078 - val_loss: 0.5992\n",
      "Epoch 114/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5074 - val_loss: 0.5989\n",
      "Epoch 115/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5072 - val_loss: 0.5978\n",
      "Epoch 116/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5068 - val_loss: 0.5984\n",
      "Epoch 117/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5069 - val_loss: 0.5978\n",
      "Epoch 118/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5081 - val_loss: 0.5999\n",
      "Epoch 119/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5073 - val_loss: 0.5983\n",
      "Epoch 120/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5065 - val_loss: 0.5987\n",
      "Epoch 121/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5072 - val_loss: 0.5993\n",
      "Epoch 122/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5077 - val_loss: 0.6001\n",
      "Epoch 123/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5068 - val_loss: 0.5993\n",
      "Epoch 124/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5085 - val_loss: 0.5988\n",
      "Epoch 125/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5081 - val_loss: 0.5983\n",
      "Epoch 126/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5066 - val_loss: 0.5973\n",
      "Epoch 127/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5061 - val_loss: 0.6001\n",
      "Epoch 128/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5089 - val_loss: 0.6005\n",
      "Epoch 129/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5081 - val_loss: 0.5969\n",
      "Epoch 130/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5076 - val_loss: 0.5992\n",
      "Epoch 131/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5069 - val_loss: 0.5991\n",
      "Epoch 132/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5062 - val_loss: 0.5992\n",
      "Epoch 133/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5070 - val_loss: 0.5984\n",
      "Epoch 134/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5069 - val_loss: 0.6002\n",
      "Epoch 135/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5064 - val_loss: 0.5981\n",
      "Epoch 136/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5063 - val_loss: 0.5995\n",
      "Epoch 137/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5063 - val_loss: 0.6009\n",
      "Epoch 138/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5078 - val_loss: 0.6007\n",
      "Epoch 139/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5080 - val_loss: 0.6009\n",
      "Epoch 140/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5065 - val_loss: 0.5997\n",
      "Epoch 141/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5066 - val_loss: 0.6001\n",
      "Epoch 142/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5073 - val_loss: 0.6014\n",
      "Epoch 143/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5081 - val_loss: 0.6004\n",
      "Epoch 144/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5052 - val_loss: 0.6005\n",
      "Epoch 145/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5051 - val_loss: 0.5999\n",
      "Epoch 146/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5074 - val_loss: 0.6004\n",
      "Epoch 147/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5058 - val_loss: 0.5996\n",
      "Epoch 148/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5052 - val_loss: 0.5985\n",
      "Epoch 149/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5053 - val_loss: 0.6009\n",
      "Epoch 150/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5050 - val_loss: 0.5997\n",
      "Epoch 151/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5056 - val_loss: 0.6006\n",
      "Epoch 152/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5050 - val_loss: 0.6010\n",
      "Epoch 153/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5061 - val_loss: 0.6007\n",
      "Epoch 154/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5048 - val_loss: 0.6008\n",
      "Epoch 155/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5050 - val_loss: 0.6008\n",
      "Epoch 156/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5047 - val_loss: 0.6017\n",
      "Epoch 157/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5048 - val_loss: 0.6015\n",
      "Epoch 158/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5052 - val_loss: 0.6013\n",
      "Epoch 159/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5061 - val_loss: 0.6027\n",
      "Epoch 160/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5046 - val_loss: 0.5996\n",
      "Epoch 161/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5049 - val_loss: 0.6036\n",
      "Epoch 162/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5079 - val_loss: 0.6011\n",
      "Epoch 163/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5044 - val_loss: 0.6018\n",
      "Epoch 164/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5065 - val_loss: 0.6008\n",
      "Epoch 165/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5066 - val_loss: 0.6007\n",
      "Epoch 166/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5062 - val_loss: 0.6007\n",
      "Epoch 167/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5051 - val_loss: 0.5999\n",
      "Epoch 168/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5040 - val_loss: 0.6022\n",
      "Epoch 169/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5032 - val_loss: 0.6014\n",
      "Epoch 170/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5039 - val_loss: 0.6015\n",
      "Epoch 171/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5041 - val_loss: 0.6014\n",
      "Epoch 172/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5045 - val_loss: 0.6026\n",
      "Epoch 173/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5045 - val_loss: 0.6039\n",
      "Epoch 174/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5050 - val_loss: 0.6032\n",
      "Epoch 175/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5053 - val_loss: 0.6017\n",
      "Epoch 176/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5040 - val_loss: 0.6046\n",
      "Epoch 177/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5034 - val_loss: 0.6027\n",
      "Epoch 178/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5042 - val_loss: 0.6038\n",
      "Epoch 179/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5048 - val_loss: 0.6020\n",
      "Epoch 180/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5041 - val_loss: 0.6026\n",
      "Epoch 181/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5042 - val_loss: 0.6027\n",
      "Epoch 182/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5029 - val_loss: 0.6035\n",
      "Epoch 183/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5036 - val_loss: 0.6023\n",
      "Epoch 184/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5033 - val_loss: 0.6043\n",
      "Epoch 185/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5023 - val_loss: 0.6034\n",
      "Epoch 186/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5021 - val_loss: 0.6055\n",
      "Epoch 187/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5032 - val_loss: 0.6037\n",
      "Epoch 188/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5030 - val_loss: 0.6079\n",
      "Epoch 189/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5037 - val_loss: 0.6058\n",
      "Epoch 190/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5044 - val_loss: 0.6042\n",
      "Epoch 191/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5027 - val_loss: 0.6030\n",
      "Epoch 192/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5020 - val_loss: 0.6049\n",
      "Epoch 193/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5032 - val_loss: 0.6064\n",
      "Epoch 194/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5034 - val_loss: 0.6025\n",
      "Epoch 195/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5022 - val_loss: 0.6054\n",
      "Epoch 196/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5020 - val_loss: 0.6044\n",
      "Epoch 197/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5012 - val_loss: 0.6034\n",
      "Epoch 198/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5040 - val_loss: 0.6051\n",
      "Epoch 199/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5016 - val_loss: 0.6045\n",
      "Epoch 200/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5019 - val_loss: 0.6061\n",
      "Epoch 201/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5005 - val_loss: 0.6050\n",
      "Epoch 202/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5032 - val_loss: 0.6038\n",
      "Epoch 203/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5018 - val_loss: 0.6037\n",
      "Epoch 204/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5014 - val_loss: 0.6071\n",
      "Epoch 205/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5016 - val_loss: 0.6038\n",
      "Epoch 206/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5011 - val_loss: 0.6042\n",
      "Epoch 207/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5001 - val_loss: 0.6066\n",
      "Epoch 208/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4996 - val_loss: 0.6073\n",
      "Epoch 209/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4997 - val_loss: 0.6060\n",
      "Epoch 210/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4996 - val_loss: 0.6080\n",
      "Epoch 211/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4997 - val_loss: 0.6071\n",
      "Epoch 212/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5033 - val_loss: 0.6062\n",
      "Epoch 213/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5023 - val_loss: 0.6065\n",
      "Epoch 214/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5055 - val_loss: 0.6041\n",
      "Epoch 215/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5017 - val_loss: 0.6038\n",
      "Epoch 216/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5013 - val_loss: 0.6064\n",
      "Epoch 217/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4998 - val_loss: 0.6036\n",
      "Epoch 218/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5011 - val_loss: 0.6060\n",
      "Epoch 219/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4991 - val_loss: 0.6052\n",
      "Epoch 220/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4988 - val_loss: 0.6082\n",
      "Epoch 221/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5003 - val_loss: 0.6071\n",
      "Epoch 222/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4985 - val_loss: 0.6022\n",
      "Epoch 223/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4993 - val_loss: 0.6065\n",
      "Epoch 224/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4982 - val_loss: 0.6061\n",
      "Epoch 225/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4989 - val_loss: 0.6065\n",
      "Epoch 226/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4989 - val_loss: 0.6086\n",
      "Epoch 227/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4991 - val_loss: 0.6073\n",
      "Epoch 228/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5000 - val_loss: 0.6051\n",
      "Epoch 229/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4983 - val_loss: 0.6087\n",
      "Epoch 230/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4987 - val_loss: 0.6072\n",
      "Epoch 231/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4978 - val_loss: 0.6065\n",
      "Epoch 232/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5001 - val_loss: 0.6048\n",
      "Epoch 233/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4979 - val_loss: 0.6071\n",
      "Epoch 234/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4981 - val_loss: 0.6072\n",
      "Epoch 235/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4992 - val_loss: 0.6069\n",
      "Epoch 236/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4982 - val_loss: 0.6061\n",
      "Epoch 237/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4976 - val_loss: 0.6073\n",
      "Epoch 238/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5005 - val_loss: 0.6041\n",
      "Epoch 239/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4984 - val_loss: 0.6047\n",
      "Epoch 240/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4991 - val_loss: 0.6125\n",
      "Epoch 241/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5032 - val_loss: 0.6082\n",
      "Epoch 242/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5026 - val_loss: 0.6064\n",
      "Epoch 243/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5021 - val_loss: 0.6066\n",
      "Epoch 244/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4995 - val_loss: 0.6051\n",
      "Epoch 245/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4988 - val_loss: 0.6098\n",
      "Epoch 246/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4976 - val_loss: 0.6063\n",
      "Epoch 247/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4978 - val_loss: 0.6068\n",
      "Epoch 248/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4967 - val_loss: 0.6087\n",
      "Epoch 249/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4986 - val_loss: 0.6051\n",
      "Epoch 250/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4976 - val_loss: 0.6075\n",
      "Epoch 251/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4973 - val_loss: 0.6081\n",
      "Epoch 252/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4985 - val_loss: 0.6070\n",
      "Epoch 253/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4968 - val_loss: 0.6073\n",
      "Epoch 254/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4988 - val_loss: 0.6042\n",
      "Epoch 255/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4972 - val_loss: 0.6091\n",
      "Epoch 256/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4961 - val_loss: 0.6077\n",
      "Epoch 257/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4973 - val_loss: 0.6080\n",
      "Epoch 258/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4958 - val_loss: 0.6064\n",
      "Epoch 259/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4965 - val_loss: 0.6074\n",
      "Epoch 260/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4964 - val_loss: 0.6079\n",
      "Epoch 261/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4969 - val_loss: 0.6079\n",
      "Epoch 262/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4973 - val_loss: 0.6053\n",
      "Epoch 263/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5001 - val_loss: 0.6097\n",
      "Epoch 264/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4984 - val_loss: 0.6064\n",
      "Epoch 265/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4961 - val_loss: 0.6086\n",
      "Epoch 266/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4973 - val_loss: 0.6056\n",
      "Epoch 267/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4967 - val_loss: 0.6092\n",
      "Epoch 268/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4974 - val_loss: 0.6089\n",
      "Epoch 269/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4961 - val_loss: 0.6054\n",
      "Epoch 270/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4961 - val_loss: 0.6065\n",
      "Epoch 271/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4952 - val_loss: 0.6088\n",
      "Epoch 272/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4963 - val_loss: 0.6077\n",
      "Epoch 273/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4969 - val_loss: 0.6091\n",
      "Epoch 274/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4948 - val_loss: 0.6072\n",
      "Epoch 275/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4949 - val_loss: 0.6062\n",
      "Epoch 276/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4955 - val_loss: 0.6071\n",
      "Epoch 277/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4948 - val_loss: 0.6094\n",
      "Epoch 278/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4956 - val_loss: 0.6086\n",
      "Epoch 279/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4971 - val_loss: 0.6072\n",
      "Epoch 280/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4964 - val_loss: 0.6130\n",
      "Epoch 281/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4988 - val_loss: 0.6088\n",
      "Epoch 282/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4964 - val_loss: 0.6092\n",
      "Epoch 283/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4956 - val_loss: 0.6080\n",
      "Epoch 284/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4963 - val_loss: 0.6070\n",
      "Epoch 285/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4947 - val_loss: 0.6084\n",
      "Epoch 286/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4948 - val_loss: 0.6097\n",
      "Epoch 287/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4950 - val_loss: 0.6097\n",
      "Epoch 288/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4995 - val_loss: 0.6094\n",
      "Epoch 289/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4968 - val_loss: 0.6083\n",
      "Epoch 290/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4946 - val_loss: 0.6063\n",
      "Epoch 291/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4941 - val_loss: 0.6079\n",
      "Epoch 292/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4961 - val_loss: 0.6076\n",
      "Epoch 293/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4954 - val_loss: 0.6071\n",
      "Epoch 294/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4953 - val_loss: 0.6093\n",
      "Epoch 295/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4949 - val_loss: 0.6101\n",
      "Epoch 296/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4940 - val_loss: 0.6072\n",
      "Epoch 297/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4942 - val_loss: 0.6119\n",
      "Epoch 298/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4953 - val_loss: 0.6095\n",
      "Epoch 299/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4943 - val_loss: 0.6093\n",
      "Epoch 300/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4935 - val_loss: 0.6102\n",
      "Epoch 301/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4945 - val_loss: 0.6083\n",
      "Epoch 302/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4940 - val_loss: 0.6084\n",
      "Epoch 303/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4938 - val_loss: 0.6089\n",
      "Epoch 304/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4935 - val_loss: 0.6111\n",
      "Epoch 305/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4934 - val_loss: 0.6104\n",
      "Epoch 306/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4938 - val_loss: 0.6092\n",
      "Epoch 307/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4941 - val_loss: 0.6087\n",
      "Epoch 308/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4942 - val_loss: 0.6102\n",
      "Epoch 309/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4946 - val_loss: 0.6113\n",
      "Epoch 310/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4941 - val_loss: 0.6094\n",
      "Epoch 311/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4929 - val_loss: 0.6111\n",
      "Epoch 312/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4930 - val_loss: 0.6097\n",
      "Epoch 313/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4924 - val_loss: 0.6088\n",
      "Epoch 314/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4936 - val_loss: 0.6115\n",
      "Epoch 315/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4928 - val_loss: 0.6079\n",
      "Epoch 316/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4940 - val_loss: 0.6120\n",
      "Epoch 317/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4934 - val_loss: 0.6088\n",
      "Epoch 318/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4930 - val_loss: 0.6090\n",
      "Epoch 319/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4929 - val_loss: 0.6092\n",
      "Epoch 320/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4925 - val_loss: 0.6118\n",
      "Epoch 321/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4961 - val_loss: 0.6083\n",
      "Epoch 322/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4940 - val_loss: 0.6096\n",
      "Epoch 323/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4931 - val_loss: 0.6106\n",
      "Epoch 324/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4928 - val_loss: 0.6117\n",
      "Epoch 325/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4932 - val_loss: 0.6108\n",
      "Epoch 326/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4930 - val_loss: 0.6097\n",
      "Epoch 327/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4939 - val_loss: 0.6085\n",
      "Epoch 328/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4931 - val_loss: 0.6108\n",
      "Epoch 329/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4926 - val_loss: 0.6100\n",
      "Epoch 330/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4927 - val_loss: 0.6091\n",
      "Epoch 331/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4916 - val_loss: 0.6107\n",
      "Epoch 332/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4920 - val_loss: 0.6093\n",
      "Epoch 333/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4934 - val_loss: 0.6121\n",
      "Epoch 334/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4944 - val_loss: 0.6097\n",
      "Epoch 335/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4925 - val_loss: 0.6100\n",
      "Epoch 336/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4918 - val_loss: 0.6108\n",
      "Epoch 337/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4916 - val_loss: 0.6105\n",
      "Epoch 338/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4932 - val_loss: 0.6110\n",
      "Epoch 339/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4924 - val_loss: 0.6090\n",
      "Epoch 340/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4960 - val_loss: 0.6128\n",
      "Epoch 341/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4956 - val_loss: 0.6117\n",
      "Epoch 342/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4933 - val_loss: 0.6073\n",
      "Epoch 343/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4911 - val_loss: 0.6101\n",
      "Epoch 344/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4920 - val_loss: 0.6098\n",
      "Epoch 345/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4922 - val_loss: 0.6111\n",
      "Epoch 346/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4928 - val_loss: 0.6085\n",
      "Epoch 347/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4905 - val_loss: 0.6090\n",
      "Epoch 348/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4908 - val_loss: 0.6095\n",
      "Epoch 349/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4925 - val_loss: 0.6114\n",
      "Epoch 350/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4915 - val_loss: 0.6087\n",
      "Epoch 351/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4918 - val_loss: 0.6105\n",
      "Epoch 352/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4925 - val_loss: 0.6090\n",
      "Epoch 353/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4924 - val_loss: 0.6078\n",
      "Epoch 354/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4931 - val_loss: 0.6116\n",
      "Epoch 355/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4923 - val_loss: 0.6090\n",
      "Epoch 356/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4929 - val_loss: 0.6079\n",
      "Epoch 357/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4929 - val_loss: 0.6085\n",
      "Epoch 358/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4909 - val_loss: 0.6104\n",
      "Epoch 359/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4911 - val_loss: 0.6099\n",
      "Epoch 360/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4904 - val_loss: 0.6081\n",
      "Epoch 361/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4906 - val_loss: 0.6105\n",
      "Epoch 362/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4926 - val_loss: 0.6076\n",
      "Epoch 363/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4910 - val_loss: 0.6110\n",
      "Epoch 364/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4897 - val_loss: 0.6102\n",
      "Epoch 365/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4901 - val_loss: 0.6111\n",
      "Epoch 366/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4906 - val_loss: 0.6112\n",
      "Epoch 367/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4902 - val_loss: 0.6085\n",
      "Epoch 368/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4897 - val_loss: 0.6120\n",
      "Epoch 369/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4917 - val_loss: 0.6089\n",
      "Epoch 370/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4924 - val_loss: 0.6105\n",
      "Epoch 371/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4940 - val_loss: 0.6094\n",
      "Epoch 372/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4901 - val_loss: 0.6096\n",
      "Epoch 373/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4907 - val_loss: 0.6119\n",
      "Epoch 374/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4910 - val_loss: 0.6114\n",
      "Epoch 375/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4914 - val_loss: 0.6110\n",
      "Epoch 376/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4883 - val_loss: 0.6117\n",
      "Epoch 377/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4889 - val_loss: 0.6108\n",
      "Epoch 378/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4897 - val_loss: 0.6098\n",
      "Epoch 379/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4889 - val_loss: 0.6099\n",
      "Epoch 380/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4897 - val_loss: 0.6120\n",
      "Epoch 381/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4904 - val_loss: 0.6083\n",
      "Epoch 382/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4898 - val_loss: 0.6129\n",
      "Epoch 383/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4900 - val_loss: 0.6087\n",
      "Epoch 384/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4914 - val_loss: 0.6132\n",
      "Epoch 385/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4933 - val_loss: 0.6105\n",
      "Epoch 386/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4900 - val_loss: 0.6110\n",
      "Epoch 387/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4895 - val_loss: 0.6115\n",
      "Epoch 388/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4886 - val_loss: 0.6113\n",
      "Epoch 389/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4887 - val_loss: 0.6111\n",
      "Epoch 390/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4901 - val_loss: 0.6111\n",
      "Epoch 391/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4902 - val_loss: 0.6107\n",
      "Epoch 392/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4902 - val_loss: 0.6098\n",
      "Epoch 393/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4899 - val_loss: 0.6123\n",
      "Epoch 394/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4903 - val_loss: 0.6097\n",
      "Epoch 395/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4913 - val_loss: 0.6143\n",
      "Epoch 396/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4890 - val_loss: 0.6087\n",
      "Epoch 397/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4903 - val_loss: 0.6128\n",
      "Epoch 398/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4900 - val_loss: 0.6126\n",
      "Epoch 399/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4881 - val_loss: 0.6109\n",
      "Epoch 400/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4890 - val_loss: 0.6129\n",
      "Epoch 401/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4900 - val_loss: 0.6141\n",
      "Epoch 402/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4879 - val_loss: 0.6106\n",
      "Epoch 403/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4899 - val_loss: 0.6111\n",
      "Epoch 404/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4881 - val_loss: 0.6139\n",
      "Epoch 405/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4888 - val_loss: 0.6117\n",
      "Epoch 406/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4880 - val_loss: 0.6112\n",
      "Epoch 407/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4874 - val_loss: 0.6117\n",
      "Epoch 408/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4869 - val_loss: 0.6137\n",
      "Epoch 409/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4880 - val_loss: 0.6088\n",
      "Epoch 410/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4894 - val_loss: 0.6123\n",
      "Epoch 411/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4903 - val_loss: 0.6107\n",
      "Epoch 412/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4891 - val_loss: 0.6113\n",
      "Epoch 413/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4885 - val_loss: 0.6108\n",
      "Epoch 414/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4899 - val_loss: 0.6102\n",
      "Epoch 415/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4921 - val_loss: 0.6107\n",
      "Epoch 416/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4922 - val_loss: 0.6103\n",
      "Epoch 417/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4890 - val_loss: 0.6126\n",
      "Epoch 418/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4886 - val_loss: 0.6079\n",
      "Epoch 419/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4872 - val_loss: 0.6115\n",
      "Epoch 420/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4882 - val_loss: 0.6095\n",
      "Epoch 421/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4883 - val_loss: 0.6140\n",
      "Epoch 422/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4871 - val_loss: 0.6109\n",
      "Epoch 423/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4888 - val_loss: 0.6098\n",
      "Epoch 424/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4877 - val_loss: 0.6124\n",
      "Epoch 425/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4875 - val_loss: 0.6108\n",
      "Epoch 426/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4866 - val_loss: 0.6106\n",
      "Epoch 427/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4881 - val_loss: 0.6111\n",
      "Epoch 428/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4896 - val_loss: 0.6124\n",
      "Epoch 429/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4887 - val_loss: 0.6075\n",
      "Epoch 430/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4873 - val_loss: 0.6114\n",
      "Epoch 431/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4893 - val_loss: 0.6091\n",
      "Epoch 432/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4880 - val_loss: 0.6110\n",
      "Epoch 433/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4883 - val_loss: 0.6141\n",
      "Epoch 434/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4884 - val_loss: 0.6114\n",
      "Epoch 435/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4868 - val_loss: 0.6131\n",
      "Epoch 436/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4867 - val_loss: 0.6116\n",
      "Epoch 437/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4868 - val_loss: 0.6141\n",
      "Epoch 438/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4878 - val_loss: 0.6144\n",
      "Epoch 439/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4862 - val_loss: 0.6131\n",
      "Epoch 440/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4878 - val_loss: 0.6120\n",
      "Epoch 441/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4882 - val_loss: 0.6161\n",
      "Epoch 442/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4858 - val_loss: 0.6105\n",
      "Epoch 443/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4863 - val_loss: 0.6147\n",
      "Epoch 444/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4868 - val_loss: 0.6148\n",
      "Epoch 445/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4861 - val_loss: 0.6140\n",
      "Epoch 446/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4870 - val_loss: 0.6139\n",
      "Epoch 447/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4889 - val_loss: 0.6115\n",
      "Epoch 448/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4863 - val_loss: 0.6165\n",
      "Epoch 449/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4864 - val_loss: 0.6150\n",
      "Epoch 450/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4888 - val_loss: 0.6136\n",
      "Epoch 451/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4863 - val_loss: 0.6118\n",
      "Epoch 452/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4852 - val_loss: 0.6153\n",
      "Epoch 453/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4889 - val_loss: 0.6143\n",
      "Epoch 454/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4866 - val_loss: 0.6123\n",
      "Epoch 455/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4916 - val_loss: 0.6160\n",
      "Epoch 456/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4893 - val_loss: 0.6137\n",
      "Epoch 457/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4854 - val_loss: 0.6144\n",
      "Epoch 458/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4852 - val_loss: 0.6145\n",
      "Epoch 459/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4852 - val_loss: 0.6154\n",
      "Epoch 460/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4887 - val_loss: 0.6096\n",
      "Epoch 461/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4847 - val_loss: 0.6152\n",
      "Epoch 462/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4861 - val_loss: 0.6117\n",
      "Epoch 463/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4850 - val_loss: 0.6146\n",
      "Epoch 464/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4859 - val_loss: 0.6115\n",
      "Epoch 465/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4855 - val_loss: 0.6196\n",
      "Epoch 466/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4861 - val_loss: 0.6128\n",
      "Epoch 467/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4851 - val_loss: 0.6152\n",
      "Epoch 468/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4839 - val_loss: 0.6161\n",
      "Epoch 469/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4872 - val_loss: 0.6141\n",
      "Epoch 470/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4860 - val_loss: 0.6144\n",
      "Epoch 471/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4850 - val_loss: 0.6150\n",
      "Epoch 472/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4865 - val_loss: 0.6175\n",
      "Epoch 473/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4839 - val_loss: 0.6172\n",
      "Epoch 474/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4868 - val_loss: 0.6124\n",
      "Epoch 475/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4882 - val_loss: 0.6160\n",
      "Epoch 476/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4852 - val_loss: 0.6150\n",
      "Epoch 477/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4844 - val_loss: 0.6160\n",
      "Epoch 478/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4844 - val_loss: 0.6149\n",
      "Epoch 479/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4861 - val_loss: 0.6123\n",
      "Epoch 480/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4841 - val_loss: 0.6175\n",
      "Epoch 481/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4855 - val_loss: 0.6181\n",
      "Epoch 482/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4857 - val_loss: 0.6129\n",
      "Epoch 483/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4860 - val_loss: 0.6153\n",
      "Epoch 484/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4848 - val_loss: 0.6177\n",
      "Epoch 485/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4837 - val_loss: 0.6163\n",
      "Epoch 486/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4836 - val_loss: 0.6178\n",
      "Epoch 487/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4841 - val_loss: 0.6136\n",
      "Epoch 488/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4834 - val_loss: 0.6141\n",
      "Epoch 489/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4833 - val_loss: 0.6156\n",
      "Epoch 490/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4829 - val_loss: 0.6179\n",
      "Epoch 491/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4833 - val_loss: 0.6151\n",
      "Epoch 492/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4838 - val_loss: 0.6164\n",
      "Epoch 493/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4828 - val_loss: 0.6158\n",
      "Epoch 494/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4836 - val_loss: 0.6177\n",
      "Epoch 495/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4855 - val_loss: 0.6142\n",
      "Epoch 496/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4838 - val_loss: 0.6170\n",
      "Epoch 497/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.4850 - val_loss: 0.6221\n",
      "Epoch 498/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4878 - val_loss: 0.6177\n",
      "Epoch 499/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4867 - val_loss: 0.6141\n",
      "Epoch 500/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4832 - val_loss: 0.6141\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc05019df60>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model|\n",
    "model_uni.fit(india_cases_train_X,\n",
    "          india_cases_train_y, \n",
    "          validation_data=(india_cases_test_X, india_cases_test_y),\n",
    "          epochs=500,\n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ae07aa",
   "metadata": {},
   "source": [
    "### Univariate stacked "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d582c6a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "16/16 [==============================] - 2s 41ms/step - loss: 0.5688 - val_loss: 0.5914\n",
      "Epoch 2/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5349 - val_loss: 0.5939\n",
      "Epoch 3/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5274 - val_loss: 0.5932\n",
      "Epoch 4/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5249 - val_loss: 0.5937\n",
      "Epoch 5/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5243 - val_loss: 0.5936\n",
      "Epoch 6/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5241 - val_loss: 0.5935\n",
      "Epoch 7/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5256 - val_loss: 0.5945\n",
      "Epoch 8/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5210 - val_loss: 0.5937\n",
      "Epoch 9/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5212 - val_loss: 0.5935\n",
      "Epoch 10/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5212 - val_loss: 0.5937\n",
      "Epoch 11/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5199 - val_loss: 0.5944\n",
      "Epoch 12/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5194 - val_loss: 0.5940\n",
      "Epoch 13/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5188 - val_loss: 0.5943\n",
      "Epoch 14/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5179 - val_loss: 0.5949\n",
      "Epoch 15/500\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.5192 - val_loss: 0.5948\n",
      "Epoch 16/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5195 - val_loss: 0.5930\n",
      "Epoch 17/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5169 - val_loss: 0.5961\n",
      "Epoch 18/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5186 - val_loss: 0.5938\n",
      "Epoch 19/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5173 - val_loss: 0.5949\n",
      "Epoch 20/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5154 - val_loss: 0.5933\n",
      "Epoch 21/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5168 - val_loss: 0.5939\n",
      "Epoch 22/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5159 - val_loss: 0.5937\n",
      "Epoch 23/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5178 - val_loss: 0.5963\n",
      "Epoch 24/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5162 - val_loss: 0.5937\n",
      "Epoch 25/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5152 - val_loss: 0.5935\n",
      "Epoch 26/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5145 - val_loss: 0.5943\n",
      "Epoch 27/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5150 - val_loss: 0.5944\n",
      "Epoch 28/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5153 - val_loss: 0.5945\n",
      "Epoch 29/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5153 - val_loss: 0.5938\n",
      "Epoch 30/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5151 - val_loss: 0.5940\n",
      "Epoch 31/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5147 - val_loss: 0.5944\n",
      "Epoch 32/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5146 - val_loss: 0.5932\n",
      "Epoch 33/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5144 - val_loss: 0.5947\n",
      "Epoch 34/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5140 - val_loss: 0.5944\n",
      "Epoch 35/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5140 - val_loss: 0.5946\n",
      "Epoch 36/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5153 - val_loss: 0.5946\n",
      "Epoch 37/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5141 - val_loss: 0.5944\n",
      "Epoch 38/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5134 - val_loss: 0.5949\n",
      "Epoch 39/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5144 - val_loss: 0.5937\n",
      "Epoch 40/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5140 - val_loss: 0.5954\n",
      "Epoch 41/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5164 - val_loss: 0.5972\n",
      "Epoch 42/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5189 - val_loss: 0.5930\n",
      "Epoch 43/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5140 - val_loss: 0.5934\n",
      "Epoch 44/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5136 - val_loss: 0.5954\n",
      "Epoch 45/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5129 - val_loss: 0.5933\n",
      "Epoch 46/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5129 - val_loss: 0.5939\n",
      "Epoch 47/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5138 - val_loss: 0.5928\n",
      "Epoch 48/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5141 - val_loss: 0.5967\n",
      "Epoch 49/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5125 - val_loss: 0.5934\n",
      "Epoch 50/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5127 - val_loss: 0.5945\n",
      "Epoch 51/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5134 - val_loss: 0.5938\n",
      "Epoch 52/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5125 - val_loss: 0.5939\n",
      "Epoch 53/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5137 - val_loss: 0.5940\n",
      "Epoch 54/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5124 - val_loss: 0.5939\n",
      "Epoch 55/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5125 - val_loss: 0.5936\n",
      "Epoch 56/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5131 - val_loss: 0.5943\n",
      "Epoch 57/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5135 - val_loss: 0.5935\n",
      "Epoch 58/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5118 - val_loss: 0.5944\n",
      "Epoch 59/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5125 - val_loss: 0.5930\n",
      "Epoch 60/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5134 - val_loss: 0.5956\n",
      "Epoch 61/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5138 - val_loss: 0.5928\n",
      "Epoch 62/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5135 - val_loss: 0.5945\n",
      "Epoch 63/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5133 - val_loss: 0.5952\n",
      "Epoch 64/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5119 - val_loss: 0.5938\n",
      "Epoch 65/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5137 - val_loss: 0.5950\n",
      "Epoch 66/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5130 - val_loss: 0.5960\n",
      "Epoch 67/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5116 - val_loss: 0.5930\n",
      "Epoch 68/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5118 - val_loss: 0.5946\n",
      "Epoch 69/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5118 - val_loss: 0.5938\n",
      "Epoch 70/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5115 - val_loss: 0.5953\n",
      "Epoch 71/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5112 - val_loss: 0.5929\n",
      "Epoch 72/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5128 - val_loss: 0.5954\n",
      "Epoch 73/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5118 - val_loss: 0.5958\n",
      "Epoch 74/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5116 - val_loss: 0.5945\n",
      "Epoch 75/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5126 - val_loss: 0.5940\n",
      "Epoch 76/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5117 - val_loss: 0.5952\n",
      "Epoch 77/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5126 - val_loss: 0.5949\n",
      "Epoch 78/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5117 - val_loss: 0.5947\n",
      "Epoch 79/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5118 - val_loss: 0.5934\n",
      "Epoch 80/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5118 - val_loss: 0.5940\n",
      "Epoch 81/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5120 - val_loss: 0.5928\n",
      "Epoch 82/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5108 - val_loss: 0.6021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5138 - val_loss: 0.5943\n",
      "Epoch 84/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5126 - val_loss: 0.5970\n",
      "Epoch 85/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5107 - val_loss: 0.5938\n",
      "Epoch 86/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5107 - val_loss: 0.5938\n",
      "Epoch 87/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5103 - val_loss: 0.5943\n",
      "Epoch 88/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5101 - val_loss: 0.5970\n",
      "Epoch 89/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5110 - val_loss: 0.5953\n",
      "Epoch 90/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5108 - val_loss: 0.5942\n",
      "Epoch 91/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5107 - val_loss: 0.5970\n",
      "Epoch 92/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5116 - val_loss: 0.5946\n",
      "Epoch 93/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5105 - val_loss: 0.5986\n",
      "Epoch 94/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5107 - val_loss: 0.5940\n",
      "Epoch 95/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5099 - val_loss: 0.5984\n",
      "Epoch 96/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5129 - val_loss: 0.5934\n",
      "Epoch 97/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5093 - val_loss: 0.5998\n",
      "Epoch 98/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5106 - val_loss: 0.5938\n",
      "Epoch 99/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5105 - val_loss: 0.5980\n",
      "Epoch 100/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5109 - val_loss: 0.5956\n",
      "Epoch 101/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5093 - val_loss: 0.5977\n",
      "Epoch 102/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5127 - val_loss: 0.5949\n",
      "Epoch 103/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5099 - val_loss: 0.5970\n",
      "Epoch 104/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5097 - val_loss: 0.5984\n",
      "Epoch 105/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5101 - val_loss: 0.5972\n",
      "Epoch 106/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5093 - val_loss: 0.5967\n",
      "Epoch 107/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5093 - val_loss: 0.5975\n",
      "Epoch 108/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5086 - val_loss: 0.5967\n",
      "Epoch 109/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5092 - val_loss: 0.5990\n",
      "Epoch 110/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5092 - val_loss: 0.5999\n",
      "Epoch 111/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5085 - val_loss: 0.5975\n",
      "Epoch 112/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5078 - val_loss: 0.5957\n",
      "Epoch 113/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5086 - val_loss: 0.5971\n",
      "Epoch 114/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5086 - val_loss: 0.5984\n",
      "Epoch 115/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5088 - val_loss: 0.5965\n",
      "Epoch 116/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5080 - val_loss: 0.5983\n",
      "Epoch 117/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5075 - val_loss: 0.5963\n",
      "Epoch 118/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5084 - val_loss: 0.5986\n",
      "Epoch 119/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5074 - val_loss: 0.5985\n",
      "Epoch 120/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5080 - val_loss: 0.6002\n",
      "Epoch 121/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5089 - val_loss: 0.5982\n",
      "Epoch 122/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5081 - val_loss: 0.5958\n",
      "Epoch 123/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5131 - val_loss: 0.5992\n",
      "Epoch 124/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5069 - val_loss: 0.6023\n",
      "Epoch 125/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5078 - val_loss: 0.5970\n",
      "Epoch 126/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5072 - val_loss: 0.6021\n",
      "Epoch 127/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5067 - val_loss: 0.5964\n",
      "Epoch 128/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5067 - val_loss: 0.5984\n",
      "Epoch 129/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5055 - val_loss: 0.5999\n",
      "Epoch 130/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5072 - val_loss: 0.5994\n",
      "Epoch 131/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5052 - val_loss: 0.6005\n",
      "Epoch 132/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5072 - val_loss: 0.6034\n",
      "Epoch 133/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5064 - val_loss: 0.6010\n",
      "Epoch 134/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5050 - val_loss: 0.6031\n",
      "Epoch 135/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5048 - val_loss: 0.6026\n",
      "Epoch 136/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5060 - val_loss: 0.6021\n",
      "Epoch 137/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5057 - val_loss: 0.5979\n",
      "Epoch 138/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5048 - val_loss: 0.6010\n",
      "Epoch 139/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5040 - val_loss: 0.6021\n",
      "Epoch 140/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5035 - val_loss: 0.6049\n",
      "Epoch 141/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5041 - val_loss: 0.6041\n",
      "Epoch 142/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5039 - val_loss: 0.6077\n",
      "Epoch 143/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5031 - val_loss: 0.6063\n",
      "Epoch 144/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5038 - val_loss: 0.6047\n",
      "Epoch 145/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5035 - val_loss: 0.6086\n",
      "Epoch 146/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5047 - val_loss: 0.6037\n",
      "Epoch 147/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5019 - val_loss: 0.6041\n",
      "Epoch 148/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5029 - val_loss: 0.6061\n",
      "Epoch 149/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5027 - val_loss: 0.6076\n",
      "Epoch 150/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5069 - val_loss: 0.6022\n",
      "Epoch 151/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5044 - val_loss: 0.6089\n",
      "Epoch 152/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5029 - val_loss: 0.6046\n",
      "Epoch 153/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5020 - val_loss: 0.6083\n",
      "Epoch 154/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5034 - val_loss: 0.6019\n",
      "Epoch 155/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5023 - val_loss: 0.6069\n",
      "Epoch 156/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5022 - val_loss: 0.6051\n",
      "Epoch 157/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5032 - val_loss: 0.6054\n",
      "Epoch 158/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5033 - val_loss: 0.6105\n",
      "Epoch 159/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5032 - val_loss: 0.6070\n",
      "Epoch 160/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5017 - val_loss: 0.6052\n",
      "Epoch 161/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4995 - val_loss: 0.6124\n",
      "Epoch 162/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5010 - val_loss: 0.6068\n",
      "Epoch 163/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5022 - val_loss: 0.6081\n",
      "Epoch 164/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5009 - val_loss: 0.6090\n",
      "Epoch 165/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5010 - val_loss: 0.6085\n",
      "Epoch 166/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5035 - val_loss: 0.6113\n",
      "Epoch 167/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5002 - val_loss: 0.6088\n",
      "Epoch 168/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5000 - val_loss: 0.6097\n",
      "Epoch 169/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5013 - val_loss: 0.6048\n",
      "Epoch 170/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5031 - val_loss: 0.6102\n",
      "Epoch 171/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5015 - val_loss: 0.6110\n",
      "Epoch 172/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4997 - val_loss: 0.6122\n",
      "Epoch 173/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5001 - val_loss: 0.6100\n",
      "Epoch 174/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4996 - val_loss: 0.6136\n",
      "Epoch 175/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5040 - val_loss: 0.6084\n",
      "Epoch 176/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5006 - val_loss: 0.6112\n",
      "Epoch 177/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4989 - val_loss: 0.6092\n",
      "Epoch 178/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4992 - val_loss: 0.6071\n",
      "Epoch 179/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5001 - val_loss: 0.6102\n",
      "Epoch 180/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4991 - val_loss: 0.6138\n",
      "Epoch 181/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4991 - val_loss: 0.6136\n",
      "Epoch 182/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4967 - val_loss: 0.6108\n",
      "Epoch 183/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4980 - val_loss: 0.6102\n",
      "Epoch 184/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4983 - val_loss: 0.6176\n",
      "Epoch 185/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4998 - val_loss: 0.6114\n",
      "Epoch 186/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5000 - val_loss: 0.6103\n",
      "Epoch 187/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5017 - val_loss: 0.6235\n",
      "Epoch 188/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5003 - val_loss: 0.6168\n",
      "Epoch 189/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4986 - val_loss: 0.6171\n",
      "Epoch 190/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4995 - val_loss: 0.6083\n",
      "Epoch 191/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5014 - val_loss: 0.6170\n",
      "Epoch 192/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4985 - val_loss: 0.6105\n",
      "Epoch 193/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4978 - val_loss: 0.6128\n",
      "Epoch 194/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4974 - val_loss: 0.6116\n",
      "Epoch 195/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4967 - val_loss: 0.6155\n",
      "Epoch 196/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4974 - val_loss: 0.6149\n",
      "Epoch 197/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4968 - val_loss: 0.6146\n",
      "Epoch 198/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4958 - val_loss: 0.6190\n",
      "Epoch 199/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4972 - val_loss: 0.6150\n",
      "Epoch 200/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4964 - val_loss: 0.6141\n",
      "Epoch 201/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4969 - val_loss: 0.6197\n",
      "Epoch 202/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4976 - val_loss: 0.6166\n",
      "Epoch 203/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4971 - val_loss: 0.6140\n",
      "Epoch 204/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4961 - val_loss: 0.6137\n",
      "Epoch 205/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4962 - val_loss: 0.6121\n",
      "Epoch 206/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4997 - val_loss: 0.6237\n",
      "Epoch 207/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5011 - val_loss: 0.6148\n",
      "Epoch 208/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5000 - val_loss: 0.6109\n",
      "Epoch 209/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4952 - val_loss: 0.6179\n",
      "Epoch 210/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4958 - val_loss: 0.6187\n",
      "Epoch 211/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4972 - val_loss: 0.6142\n",
      "Epoch 212/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4964 - val_loss: 0.6133\n",
      "Epoch 213/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4961 - val_loss: 0.6143\n",
      "Epoch 214/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4949 - val_loss: 0.6141\n",
      "Epoch 215/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4962 - val_loss: 0.6156\n",
      "Epoch 216/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4970 - val_loss: 0.6145\n",
      "Epoch 217/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4958 - val_loss: 0.6225\n",
      "Epoch 218/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4953 - val_loss: 0.6148\n",
      "Epoch 219/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4965 - val_loss: 0.6139\n",
      "Epoch 220/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4950 - val_loss: 0.6157\n",
      "Epoch 221/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4943 - val_loss: 0.6164\n",
      "Epoch 222/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4961 - val_loss: 0.6122\n",
      "Epoch 223/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4965 - val_loss: 0.6140\n",
      "Epoch 224/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4984 - val_loss: 0.6189\n",
      "Epoch 225/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4974 - val_loss: 0.6170\n",
      "Epoch 226/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4964 - val_loss: 0.6159\n",
      "Epoch 227/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4959 - val_loss: 0.6139\n",
      "Epoch 228/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4952 - val_loss: 0.6174\n",
      "Epoch 229/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4946 - val_loss: 0.6197\n",
      "Epoch 230/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4936 - val_loss: 0.6203\n",
      "Epoch 231/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4945 - val_loss: 0.6175\n",
      "Epoch 232/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4972 - val_loss: 0.6173\n",
      "Epoch 233/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4943 - val_loss: 0.6191\n",
      "Epoch 234/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4942 - val_loss: 0.6161\n",
      "Epoch 235/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4940 - val_loss: 0.6186\n",
      "Epoch 236/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4950 - val_loss: 0.6187\n",
      "Epoch 237/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4938 - val_loss: 0.6198\n",
      "Epoch 238/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4929 - val_loss: 0.6239\n",
      "Epoch 239/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4976 - val_loss: 0.6209\n",
      "Epoch 240/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4954 - val_loss: 0.6150\n",
      "Epoch 241/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4932 - val_loss: 0.6180\n",
      "Epoch 242/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4934 - val_loss: 0.6212\n",
      "Epoch 243/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4941 - val_loss: 0.6197\n",
      "Epoch 244/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4932 - val_loss: 0.6167\n",
      "Epoch 245/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4918 - val_loss: 0.6242\n",
      "Epoch 246/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4944 - val_loss: 0.6174\n",
      "Epoch 247/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4930 - val_loss: 0.6224\n",
      "Epoch 248/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4968 - val_loss: 0.6187\n",
      "Epoch 249/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4944 - val_loss: 0.6172\n",
      "Epoch 250/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4955 - val_loss: 0.6260\n",
      "Epoch 251/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4950 - val_loss: 0.6151\n",
      "Epoch 252/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4946 - val_loss: 0.6248\n",
      "Epoch 253/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4920 - val_loss: 0.6190\n",
      "Epoch 254/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4919 - val_loss: 0.6193\n",
      "Epoch 255/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4918 - val_loss: 0.6184\n",
      "Epoch 256/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4919 - val_loss: 0.6163\n",
      "Epoch 257/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4939 - val_loss: 0.6164\n",
      "Epoch 258/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4941 - val_loss: 0.6228\n",
      "Epoch 259/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4920 - val_loss: 0.6179\n",
      "Epoch 260/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4923 - val_loss: 0.6171\n",
      "Epoch 261/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4928 - val_loss: 0.6192\n",
      "Epoch 262/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4976 - val_loss: 0.6230\n",
      "Epoch 263/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4953 - val_loss: 0.6140\n",
      "Epoch 264/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4938 - val_loss: 0.6246\n",
      "Epoch 265/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4932 - val_loss: 0.6161\n",
      "Epoch 266/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4920 - val_loss: 0.6200\n",
      "Epoch 267/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4909 - val_loss: 0.6198\n",
      "Epoch 268/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4929 - val_loss: 0.6165\n",
      "Epoch 269/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4912 - val_loss: 0.6188\n",
      "Epoch 270/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4903 - val_loss: 0.6178\n",
      "Epoch 271/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4912 - val_loss: 0.6206\n",
      "Epoch 272/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4908 - val_loss: 0.6146\n",
      "Epoch 273/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4919 - val_loss: 0.6173\n",
      "Epoch 274/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4902 - val_loss: 0.6174\n",
      "Epoch 275/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4916 - val_loss: 0.6193\n",
      "Epoch 276/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4910 - val_loss: 0.6177\n",
      "Epoch 277/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4910 - val_loss: 0.6176\n",
      "Epoch 278/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4903 - val_loss: 0.6159\n",
      "Epoch 279/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4913 - val_loss: 0.6141\n",
      "Epoch 280/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4898 - val_loss: 0.6243\n",
      "Epoch 281/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4908 - val_loss: 0.6177\n",
      "Epoch 282/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4949 - val_loss: 0.6194\n",
      "Epoch 283/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4929 - val_loss: 0.6178\n",
      "Epoch 284/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4904 - val_loss: 0.6232\n",
      "Epoch 285/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4904 - val_loss: 0.6193\n",
      "Epoch 286/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4896 - val_loss: 0.6173\n",
      "Epoch 287/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4887 - val_loss: 0.6186\n",
      "Epoch 288/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4893 - val_loss: 0.6185\n",
      "Epoch 289/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4903 - val_loss: 0.6282\n",
      "Epoch 290/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4918 - val_loss: 0.6172\n",
      "Epoch 291/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4904 - val_loss: 0.6203\n",
      "Epoch 292/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4886 - val_loss: 0.6266\n",
      "Epoch 293/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4928 - val_loss: 0.6186\n",
      "Epoch 294/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4915 - val_loss: 0.6188\n",
      "Epoch 295/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4914 - val_loss: 0.6272\n",
      "Epoch 296/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4905 - val_loss: 0.6228\n",
      "Epoch 297/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4912 - val_loss: 0.6163\n",
      "Epoch 298/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4889 - val_loss: 0.6179\n",
      "Epoch 299/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4905 - val_loss: 0.6234\n",
      "Epoch 300/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4890 - val_loss: 0.6185\n",
      "Epoch 301/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4896 - val_loss: 0.6173\n",
      "Epoch 302/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4886 - val_loss: 0.6195\n",
      "Epoch 303/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4884 - val_loss: 0.6185\n",
      "Epoch 304/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4885 - val_loss: 0.6230\n",
      "Epoch 305/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4879 - val_loss: 0.6196\n",
      "Epoch 306/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4890 - val_loss: 0.6195\n",
      "Epoch 307/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4878 - val_loss: 0.6178\n",
      "Epoch 308/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4880 - val_loss: 0.6207\n",
      "Epoch 309/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4885 - val_loss: 0.6199\n",
      "Epoch 310/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4895 - val_loss: 0.6243\n",
      "Epoch 311/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4895 - val_loss: 0.6197\n",
      "Epoch 312/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4887 - val_loss: 0.6163\n",
      "Epoch 313/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4899 - val_loss: 0.6257\n",
      "Epoch 314/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4891 - val_loss: 0.6265\n",
      "Epoch 315/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4933 - val_loss: 0.6161\n",
      "Epoch 316/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4930 - val_loss: 0.6239\n",
      "Epoch 317/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4891 - val_loss: 0.6210\n",
      "Epoch 318/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4878 - val_loss: 0.6203\n",
      "Epoch 319/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4881 - val_loss: 0.6176\n",
      "Epoch 320/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4880 - val_loss: 0.6197\n",
      "Epoch 321/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4886 - val_loss: 0.6196\n",
      "Epoch 322/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4867 - val_loss: 0.6220\n",
      "Epoch 323/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4870 - val_loss: 0.6191\n",
      "Epoch 324/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4889 - val_loss: 0.6172\n",
      "Epoch 325/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4879 - val_loss: 0.6227\n",
      "Epoch 326/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4896 - val_loss: 0.6230\n",
      "Epoch 327/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4877 - val_loss: 0.6242\n",
      "Epoch 328/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4882 - val_loss: 0.6175\n",
      "Epoch 329/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4871 - val_loss: 0.6252\n",
      "Epoch 330/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4875 - val_loss: 0.6189\n",
      "Epoch 331/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4878 - val_loss: 0.6235\n",
      "Epoch 332/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4891 - val_loss: 0.6221\n",
      "Epoch 333/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4877 - val_loss: 0.6210\n",
      "Epoch 334/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4883 - val_loss: 0.6186\n",
      "Epoch 335/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4888 - val_loss: 0.6210\n",
      "Epoch 336/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4859 - val_loss: 0.6242\n",
      "Epoch 337/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4865 - val_loss: 0.6235\n",
      "Epoch 338/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4877 - val_loss: 0.6218\n",
      "Epoch 339/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4869 - val_loss: 0.6233\n",
      "Epoch 340/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4857 - val_loss: 0.6204\n",
      "Epoch 341/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4856 - val_loss: 0.6212\n",
      "Epoch 342/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4866 - val_loss: 0.6219\n",
      "Epoch 343/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4897 - val_loss: 0.6274\n",
      "Epoch 344/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4878 - val_loss: 0.6185\n",
      "Epoch 345/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4859 - val_loss: 0.6238\n",
      "Epoch 346/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4854 - val_loss: 0.6222\n",
      "Epoch 347/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4854 - val_loss: 0.6188\n",
      "Epoch 348/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4847 - val_loss: 0.6242\n",
      "Epoch 349/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4838 - val_loss: 0.6196\n",
      "Epoch 350/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4869 - val_loss: 0.6247\n",
      "Epoch 351/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4872 - val_loss: 0.6199\n",
      "Epoch 352/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4845 - val_loss: 0.6248\n",
      "Epoch 353/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4867 - val_loss: 0.6240\n",
      "Epoch 354/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4856 - val_loss: 0.6220\n",
      "Epoch 355/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4849 - val_loss: 0.6212\n",
      "Epoch 356/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4866 - val_loss: 0.6272\n",
      "Epoch 357/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4853 - val_loss: 0.6241\n",
      "Epoch 358/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4892 - val_loss: 0.6239\n",
      "Epoch 359/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4877 - val_loss: 0.6169\n",
      "Epoch 360/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4871 - val_loss: 0.6250\n",
      "Epoch 361/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4896 - val_loss: 0.6247\n",
      "Epoch 362/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4862 - val_loss: 0.6206\n",
      "Epoch 363/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4869 - val_loss: 0.6275\n",
      "Epoch 364/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4846 - val_loss: 0.6228\n",
      "Epoch 365/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4871 - val_loss: 0.6269\n",
      "Epoch 366/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4858 - val_loss: 0.6226\n",
      "Epoch 367/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4837 - val_loss: 0.6266\n",
      "Epoch 368/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4836 - val_loss: 0.6258\n",
      "Epoch 369/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4836 - val_loss: 0.6247\n",
      "Epoch 370/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4864 - val_loss: 0.6265\n",
      "Epoch 371/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4837 - val_loss: 0.6246\n",
      "Epoch 372/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4842 - val_loss: 0.6224\n",
      "Epoch 373/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4828 - val_loss: 0.6279\n",
      "Epoch 374/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4825 - val_loss: 0.6255\n",
      "Epoch 375/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4882 - val_loss: 0.6318\n",
      "Epoch 376/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4886 - val_loss: 0.6189\n",
      "Epoch 377/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4865 - val_loss: 0.6260\n",
      "Epoch 378/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4833 - val_loss: 0.6259\n",
      "Epoch 379/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4831 - val_loss: 0.6288\n",
      "Epoch 380/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4828 - val_loss: 0.6287\n",
      "Epoch 381/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4825 - val_loss: 0.6265\n",
      "Epoch 382/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4852 - val_loss: 0.6318\n",
      "Epoch 383/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4848 - val_loss: 0.6292\n",
      "Epoch 384/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4833 - val_loss: 0.6233\n",
      "Epoch 385/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4847 - val_loss: 0.6281\n",
      "Epoch 386/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4822 - val_loss: 0.6281\n",
      "Epoch 387/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4818 - val_loss: 0.6294\n",
      "Epoch 388/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4857 - val_loss: 0.6305\n",
      "Epoch 389/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4823 - val_loss: 0.6241\n",
      "Epoch 390/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4847 - val_loss: 0.6313\n",
      "Epoch 391/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4879 - val_loss: 0.6285\n",
      "Epoch 392/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4840 - val_loss: 0.6295\n",
      "Epoch 393/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4825 - val_loss: 0.6301\n",
      "Epoch 394/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4806 - val_loss: 0.6277\n",
      "Epoch 395/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4826 - val_loss: 0.6295\n",
      "Epoch 396/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4820 - val_loss: 0.6261\n",
      "Epoch 397/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4826 - val_loss: 0.6323\n",
      "Epoch 398/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4846 - val_loss: 0.6266\n",
      "Epoch 399/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4839 - val_loss: 0.6374\n",
      "Epoch 400/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4806 - val_loss: 0.6246\n",
      "Epoch 401/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4834 - val_loss: 0.6340\n",
      "Epoch 402/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4801 - val_loss: 0.6300\n",
      "Epoch 403/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4822 - val_loss: 0.6302\n",
      "Epoch 404/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4833 - val_loss: 0.6298\n",
      "Epoch 405/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4818 - val_loss: 0.6293\n",
      "Epoch 406/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4820 - val_loss: 0.6354\n",
      "Epoch 407/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4798 - val_loss: 0.6253\n",
      "Epoch 408/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4809 - val_loss: 0.6332\n",
      "Epoch 409/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4845 - val_loss: 0.6341\n",
      "Epoch 410/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4815 - val_loss: 0.6286\n",
      "Epoch 411/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4798 - val_loss: 0.6359\n",
      "Epoch 412/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4802 - val_loss: 0.6241\n",
      "Epoch 413/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4799 - val_loss: 0.6365\n",
      "Epoch 414/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4785 - val_loss: 0.6314\n",
      "Epoch 415/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4792 - val_loss: 0.6346\n",
      "Epoch 416/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4817 - val_loss: 0.6345\n",
      "Epoch 417/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4841 - val_loss: 0.6298\n",
      "Epoch 418/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4835 - val_loss: 0.6351\n",
      "Epoch 419/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4845 - val_loss: 0.6300\n",
      "Epoch 420/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4825 - val_loss: 0.6291\n",
      "Epoch 421/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4781 - val_loss: 0.6301\n",
      "Epoch 422/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4779 - val_loss: 0.6371\n",
      "Epoch 423/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4790 - val_loss: 0.6353\n",
      "Epoch 424/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4770 - val_loss: 0.6328\n",
      "Epoch 425/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4751 - val_loss: 0.6447\n",
      "Epoch 426/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4794 - val_loss: 0.6371\n",
      "Epoch 427/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4783 - val_loss: 0.6330\n",
      "Epoch 428/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4780 - val_loss: 0.6352\n",
      "Epoch 429/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4819 - val_loss: 0.6363\n",
      "Epoch 430/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4778 - val_loss: 0.6400\n",
      "Epoch 431/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4796 - val_loss: 0.6308\n",
      "Epoch 432/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4809 - val_loss: 0.6399\n",
      "Epoch 433/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4806 - val_loss: 0.6370\n",
      "Epoch 434/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4778 - val_loss: 0.6370\n",
      "Epoch 435/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4783 - val_loss: 0.6383\n",
      "Epoch 436/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4771 - val_loss: 0.6453\n",
      "Epoch 437/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4758 - val_loss: 0.6340\n",
      "Epoch 438/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4776 - val_loss: 0.6437\n",
      "Epoch 439/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4769 - val_loss: 0.6356\n",
      "Epoch 440/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4804 - val_loss: 0.6389\n",
      "Epoch 441/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4770 - val_loss: 0.6316\n",
      "Epoch 442/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4766 - val_loss: 0.6364\n",
      "Epoch 443/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4764 - val_loss: 0.6451\n",
      "Epoch 444/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4773 - val_loss: 0.6308\n",
      "Epoch 445/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4763 - val_loss: 0.6374\n",
      "Epoch 446/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4768 - val_loss: 0.6357\n",
      "Epoch 447/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4759 - val_loss: 0.6451\n",
      "Epoch 448/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4757 - val_loss: 0.6369\n",
      "Epoch 449/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4770 - val_loss: 0.6311\n",
      "Epoch 450/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4787 - val_loss: 0.6458\n",
      "Epoch 451/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4774 - val_loss: 0.6281\n",
      "Epoch 452/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4768 - val_loss: 0.6433\n",
      "Epoch 453/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4757 - val_loss: 0.6381\n",
      "Epoch 454/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4761 - val_loss: 0.6439\n",
      "Epoch 455/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4728 - val_loss: 0.6380\n",
      "Epoch 456/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4730 - val_loss: 0.6391\n",
      "Epoch 457/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4713 - val_loss: 0.6438\n",
      "Epoch 458/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4729 - val_loss: 0.6391\n",
      "Epoch 459/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4705 - val_loss: 0.6383\n",
      "Epoch 460/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4719 - val_loss: 0.6452\n",
      "Epoch 461/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4750 - val_loss: 0.6468\n",
      "Epoch 462/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4733 - val_loss: 0.6416\n",
      "Epoch 463/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4734 - val_loss: 0.6392\n",
      "Epoch 464/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4712 - val_loss: 0.6481\n",
      "Epoch 465/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4788 - val_loss: 0.6399\n",
      "Epoch 466/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4717 - val_loss: 0.6432\n",
      "Epoch 467/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4753 - val_loss: 0.6412\n",
      "Epoch 468/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4694 - val_loss: 0.6442\n",
      "Epoch 469/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4724 - val_loss: 0.6477\n",
      "Epoch 470/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4714 - val_loss: 0.6524\n",
      "Epoch 471/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4780 - val_loss: 0.6422\n",
      "Epoch 472/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4723 - val_loss: 0.6529\n",
      "Epoch 473/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4706 - val_loss: 0.6455\n",
      "Epoch 474/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4760 - val_loss: 0.6533\n",
      "Epoch 475/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4721 - val_loss: 0.6451\n",
      "Epoch 476/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4684 - val_loss: 0.6473\n",
      "Epoch 477/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4683 - val_loss: 0.6491\n",
      "Epoch 478/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4703 - val_loss: 0.6495\n",
      "Epoch 479/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4684 - val_loss: 0.6491\n",
      "Epoch 480/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4711 - val_loss: 0.6500\n",
      "Epoch 481/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4705 - val_loss: 0.6491\n",
      "Epoch 482/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4697 - val_loss: 0.6488\n",
      "Epoch 483/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4704 - val_loss: 0.6438\n",
      "Epoch 484/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4680 - val_loss: 0.6462\n",
      "Epoch 485/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4652 - val_loss: 0.6515\n",
      "Epoch 486/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4671 - val_loss: 0.6489\n",
      "Epoch 487/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4682 - val_loss: 0.6492\n",
      "Epoch 488/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4708 - val_loss: 0.6479\n",
      "Epoch 489/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4671 - val_loss: 0.6522\n",
      "Epoch 490/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4659 - val_loss: 0.6564\n",
      "Epoch 491/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4626 - val_loss: 0.6546\n",
      "Epoch 492/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4644 - val_loss: 0.6527\n",
      "Epoch 493/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4671 - val_loss: 0.6511\n",
      "Epoch 494/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4650 - val_loss: 0.6452\n",
      "Epoch 495/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4646 - val_loss: 0.6517\n",
      "Epoch 496/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4660 - val_loss: 0.6612\n",
      "Epoch 497/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4666 - val_loss: 0.6447\n",
      "Epoch 498/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4678 - val_loss: 0.6489\n",
      "Epoch 499/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4647 - val_loss: 0.6605\n",
      "Epoch 500/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4658 - val_loss: 0.6458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc03855af60>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_uni_stacked.fit(india_cases_train_X,\n",
    "          india_cases_train_y, \n",
    "          validation_data=(india_cases_test_X, india_cases_test_y),\n",
    "          epochs=500,\n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a274610",
   "metadata": {},
   "source": [
    "<a name=predict></a>\n",
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdf2acf",
   "metadata": {},
   "source": [
    "### Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d6d40ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.83436764]\n",
      "  [-0.43624764]\n",
      "  [-0.10223458]]\n",
      "\n",
      " [[-0.43624764]\n",
      "  [-0.10223458]\n",
      "  [-0.47536368]]\n",
      "\n",
      " [[-0.10223458]\n",
      "  [-0.47536368]\n",
      "  [-0.21208117]]\n",
      "\n",
      " [[-0.47536368]\n",
      "  [-0.21208117]\n",
      "  [-0.52361475]]\n",
      "\n",
      " [[-0.21208117]\n",
      "  [-0.52361475]\n",
      "  [ 0.45296694]]\n",
      "\n",
      " [[-0.52361475]\n",
      "  [ 0.45296694]\n",
      "  [ 3.62374124]]\n",
      "\n",
      " [[ 0.45296694]\n",
      "  [ 3.62374124]\n",
      "  [ 0.39328136]]\n",
      "\n",
      " [[ 3.62374124]\n",
      "  [ 0.39328136]\n",
      "  [-0.12703865]]\n",
      "\n",
      " [[ 0.39328136]\n",
      "  [-0.12703865]\n",
      "  [ 2.10547838]]\n",
      "\n",
      " [[-0.12703865]\n",
      "  [ 2.10547838]\n",
      "  [-0.15062388]]\n",
      "\n",
      " [[ 2.10547838]\n",
      "  [-0.15062388]\n",
      "  [-0.45088631]]\n",
      "\n",
      " [[-0.15062388]\n",
      "  [-0.45088631]\n",
      "  [-0.18882265]]\n",
      "\n",
      " [[-0.45088631]\n",
      "  [-0.18882265]\n",
      "  [ 0.1233769 ]]\n",
      "\n",
      " [[-0.18882265]\n",
      "  [ 0.1233769 ]\n",
      "  [ 3.22197563]]\n",
      "\n",
      " [[ 0.1233769 ]\n",
      "  [ 3.22197563]\n",
      "  [-0.68799512]]\n",
      "\n",
      " [[ 3.22197563]\n",
      "  [-0.68799512]\n",
      "  [-0.38197774]]\n",
      "\n",
      " [[-0.68799512]\n",
      "  [-0.38197774]\n",
      "  [-0.29698548]]\n",
      "\n",
      " [[-0.38197774]\n",
      "  [-0.29698548]\n",
      "  [ 0.1100199 ]]\n",
      "\n",
      " [[-0.29698548]\n",
      "  [ 0.1100199 ]\n",
      "  [ 0.43194505]]\n",
      "\n",
      " [[ 0.1100199 ]\n",
      "  [ 0.43194505]\n",
      "  [-0.13437683]]\n",
      "\n",
      " [[ 0.43194505]\n",
      "  [-0.13437683]\n",
      "  [-0.13946581]]\n",
      "\n",
      " [[-0.13437683]\n",
      "  [-0.13946581]\n",
      "  [ 4.07815581]]\n",
      "\n",
      " [[-0.13946581]\n",
      "  [ 4.07815581]\n",
      "  [ 0.08362002]]\n",
      "\n",
      " [[ 4.07815581]\n",
      "  [ 0.08362002]\n",
      "  [-0.57352446]]\n",
      "\n",
      " [[ 0.08362002]\n",
      "  [-0.57352446]\n",
      "  [ 0.51495198]]\n",
      "\n",
      " [[-0.57352446]\n",
      "  [ 0.51495198]\n",
      "  [ 0.4047033 ]]\n",
      "\n",
      " [[ 0.51495198]\n",
      "  [ 0.4047033 ]\n",
      "  [-0.38705415]]\n",
      "\n",
      " [[ 0.4047033 ]\n",
      "  [-0.38705415]\n",
      "  [-0.68799512]]\n",
      "\n",
      " [[-0.38705415]\n",
      "  [-0.68799512]\n",
      "  [-0.50176355]]\n",
      "\n",
      " [[-0.68799512]\n",
      "  [-0.50176355]\n",
      "  [-0.35763859]]\n",
      "\n",
      " [[-0.50176355]\n",
      "  [-0.35763859]\n",
      "  [ 0.27290498]]\n",
      "\n",
      " [[-0.35763859]\n",
      "  [ 0.27290498]\n",
      "  [ 0.28288189]]\n",
      "\n",
      " [[ 0.27290498]\n",
      "  [ 0.28288189]\n",
      "  [-0.67178577]]\n",
      "\n",
      " [[ 0.28288189]\n",
      "  [-0.67178577]\n",
      "  [ 1.7801857 ]]\n",
      "\n",
      " [[-0.67178577]\n",
      "  [ 1.7801857 ]\n",
      "  [-0.54162095]]\n",
      "\n",
      " [[ 1.7801857 ]\n",
      "  [-0.54162095]\n",
      "  [-0.68673858]]\n",
      "\n",
      " [[-0.54162095]\n",
      "  [-0.68673858]\n",
      "  [-0.53477282]]\n",
      "\n",
      " [[-0.68673858]\n",
      "  [-0.53477282]\n",
      "  [ 0.46891241]]\n",
      "\n",
      " [[-0.53477282]\n",
      "  [ 0.46891241]\n",
      "  [-0.56373603]]\n",
      "\n",
      " [[ 0.46891241]\n",
      "  [-0.56373603]\n",
      "  [-0.65273665]]\n",
      "\n",
      " [[-0.56373603]\n",
      "  [-0.65273665]\n",
      "  [-0.30784197]]\n",
      "\n",
      " [[-0.65273665]\n",
      "  [-0.30784197]\n",
      "  [-0.11147014]]\n",
      "\n",
      " [[-0.30784197]\n",
      "  [-0.11147014]\n",
      "  [-0.66973761]]\n",
      "\n",
      " [[-0.11147014]\n",
      "  [-0.66973761]\n",
      "  [-0.6763847 ]]\n",
      "\n",
      " [[-0.66973761]\n",
      "  [-0.6763847 ]\n",
      "  [-0.45234389]]\n",
      "\n",
      " [[-0.6763847 ]\n",
      "  [-0.45234389]\n",
      "  [ 4.24376758]]\n",
      "\n",
      " [[-0.45234389]\n",
      "  [ 4.24376758]\n",
      "  [-0.66863186]]\n",
      "\n",
      " [[ 4.24376758]\n",
      "  [-0.66863186]\n",
      "  [-0.453613  ]]\n",
      "\n",
      " [[-0.66863186]\n",
      "  [-0.453613  ]\n",
      "  [-0.68799512]]\n",
      "\n",
      " [[-0.453613  ]\n",
      "  [-0.68799512]\n",
      "  [-0.03877939]]\n",
      "\n",
      " [[-0.68799512]\n",
      "  [-0.03877939]\n",
      "  [-0.27412904]]\n",
      "\n",
      " [[-0.03877939]\n",
      "  [-0.27412904]\n",
      "  [ 0.36602705]]\n",
      "\n",
      " [[-0.27412904]\n",
      "  [ 0.36602705]\n",
      "  [-0.68799512]]\n",
      "\n",
      " [[ 0.36602705]\n",
      "  [-0.68799512]\n",
      "  [ 4.49435905]]\n",
      "\n",
      " [[-0.68799512]\n",
      "  [ 4.49435905]\n",
      "  [-0.05525261]]\n",
      "\n",
      " [[ 4.49435905]\n",
      "  [-0.05525261]\n",
      "  [-0.68799512]]\n",
      "\n",
      " [[-0.05525261]\n",
      "  [-0.68799512]\n",
      "  [ 0.30166714]]\n",
      "\n",
      " [[-0.68799512]\n",
      "  [ 0.30166714]\n",
      "  [ 0.21732828]]\n",
      "\n",
      " [[ 0.30166714]\n",
      "  [ 0.21732828]\n",
      "  [-0.56695277]]\n",
      "\n",
      " [[ 0.21732828]\n",
      "  [-0.56695277]\n",
      "  [-0.6878569 ]]\n",
      "\n",
      " [[-0.56695277]\n",
      "  [-0.6878569 ]\n",
      "  [-0.62981739]]\n",
      "\n",
      " [[-0.6878569 ]\n",
      "  [-0.62981739]\n",
      "  [ 0.97520946]]\n",
      "\n",
      " [[-0.62981739]\n",
      "  [ 0.97520946]\n",
      "  [-0.66446015]]\n",
      "\n",
      " [[ 0.97520946]\n",
      "  [-0.66446015]\n",
      "  [-0.10812775]]\n",
      "\n",
      " [[-0.66446015]\n",
      "  [-0.10812775]\n",
      "  [-0.68799512]]\n",
      "\n",
      " [[-0.10812775]\n",
      "  [-0.68799512]\n",
      "  [ 1.23116634]]\n",
      "\n",
      " [[-0.68799512]\n",
      "  [ 1.23116634]\n",
      "  [ 0.19781424]]\n",
      "\n",
      " [[ 1.23116634]\n",
      "  [ 0.19781424]\n",
      "  [-0.16645626]]\n",
      "\n",
      " [[ 0.19781424]\n",
      "  [-0.16645626]\n",
      "  [ 0.29659073]]\n",
      "\n",
      " [[-0.16645626]\n",
      "  [ 0.29659073]\n",
      "  [-0.68514277]]\n",
      "\n",
      " [[ 0.29659073]\n",
      "  [-0.68514277]\n",
      "  [-0.13372343]]\n",
      "\n",
      " [[-0.68514277]\n",
      "  [-0.13372343]\n",
      "  [-0.14665321]]\n",
      "\n",
      " [[-0.13372343]\n",
      "  [-0.14665321]\n",
      "  [-0.68736685]]\n",
      "\n",
      " [[-0.14665321]\n",
      "  [-0.68736685]\n",
      "  [-0.12937581]]\n",
      "\n",
      " [[-0.68736685]\n",
      "  [-0.12937581]\n",
      "  [-0.59626781]]\n",
      "\n",
      " [[-0.12937581]\n",
      "  [-0.59626781]\n",
      "  [-0.68670088]]\n",
      "\n",
      " [[-0.59626781]\n",
      "  [-0.68670088]\n",
      "  [ 0.15387309]]\n",
      "\n",
      " [[-0.68670088]\n",
      "  [ 0.15387309]\n",
      "  [-0.6878569 ]]\n",
      "\n",
      " [[ 0.15387309]\n",
      "  [-0.6878569 ]\n",
      "  [ 0.43919528]]\n",
      "\n",
      " [[-0.6878569 ]\n",
      "  [ 0.43919528]\n",
      "  [-0.05922327]]\n",
      "\n",
      " [[ 0.43919528]\n",
      "  [-0.05922327]\n",
      "  [-0.68798255]]\n",
      "\n",
      " [[-0.05922327]\n",
      "  [-0.68798255]\n",
      "  [ 0.38461125]]\n",
      "\n",
      " [[-0.68798255]\n",
      "  [ 0.38461125]\n",
      "  [-0.47506211]]\n",
      "\n",
      " [[ 0.38461125]\n",
      "  [-0.47506211]\n",
      "  [ 0.91421708]]\n",
      "\n",
      " [[-0.47506211]\n",
      "  [ 0.91421708]\n",
      "  [ 0.25545166]]\n",
      "\n",
      " [[ 0.91421708]\n",
      "  [ 0.25545166]\n",
      "  [-0.25193857]]\n",
      "\n",
      " [[ 0.25545166]\n",
      "  [-0.25193857]\n",
      "  [-0.64267177]]\n",
      "\n",
      " [[-0.25193857]\n",
      "  [-0.64267177]\n",
      "  [-0.38630023]]\n",
      "\n",
      " [[-0.64267177]\n",
      "  [-0.38630023]\n",
      "  [-0.31494141]]\n",
      "\n",
      " [[-0.38630023]\n",
      "  [-0.31494141]\n",
      "  [-0.16465941]]\n",
      "\n",
      " [[-0.31494141]\n",
      "  [-0.16465941]\n",
      "  [ 0.57653493]]\n",
      "\n",
      " [[-0.16465941]\n",
      "  [ 0.57653493]\n",
      "  [ 0.34450254]]\n",
      "\n",
      " [[ 0.57653493]\n",
      "  [ 0.34450254]\n",
      "  [-0.665968  ]]\n",
      "\n",
      " [[ 0.34450254]\n",
      "  [-0.665968  ]\n",
      "  [-0.02819933]]\n",
      "\n",
      " [[-0.665968  ]\n",
      "  [-0.02819933]\n",
      "  [-0.49770493]]\n",
      "\n",
      " [[-0.02819933]\n",
      "  [-0.49770493]\n",
      "  [-0.53810264]]\n",
      "\n",
      " [[-0.49770493]\n",
      "  [-0.53810264]\n",
      "  [-0.38219135]]\n",
      "\n",
      " [[-0.53810264]\n",
      "  [-0.38219135]\n",
      "  [ 3.26562778]]\n",
      "\n",
      " [[-0.38219135]\n",
      "  [ 3.26562778]\n",
      "  [-0.4600716 ]]\n",
      "\n",
      " [[ 3.26562778]\n",
      "  [-0.4600716 ]\n",
      "  [-0.39814939]]\n",
      "\n",
      " [[-0.4600716 ]\n",
      "  [-0.39814939]\n",
      "  [ 0.18746036]]\n",
      "\n",
      " [[-0.39814939]\n",
      "  [ 0.18746036]\n",
      "  [-0.29366822]]\n",
      "\n",
      " [[ 0.18746036]\n",
      "  [-0.29366822]\n",
      "  [ 2.75243234]]\n",
      "\n",
      " [[-0.29366822]\n",
      "  [ 2.75243234]\n",
      "  [-0.6437147 ]]\n",
      "\n",
      " [[ 2.75243234]\n",
      "  [-0.6437147 ]\n",
      "  [-0.20212939]]\n",
      "\n",
      " [[-0.6437147 ]\n",
      "  [-0.20212939]\n",
      "  [-0.30132053]]\n",
      "\n",
      " [[-0.20212939]\n",
      "  [-0.30132053]\n",
      "  [-0.10707225]]\n",
      "\n",
      " [[-0.30132053]\n",
      "  [-0.10707225]\n",
      "  [-0.68799512]]\n",
      "\n",
      " [[-0.10707225]\n",
      "  [-0.68799512]\n",
      "  [-0.01599834]]\n",
      "\n",
      " [[-0.68799512]\n",
      "  [-0.01599834]\n",
      "  [ 0.39888553]]\n",
      "\n",
      " [[-0.01599834]\n",
      "  [ 0.39888553]\n",
      "  [-0.5261404 ]]\n",
      "\n",
      " [[ 0.39888553]\n",
      "  [-0.5261404 ]\n",
      "  [-0.63846237]]\n",
      "\n",
      " [[-0.5261404 ]\n",
      "  [-0.63846237]\n",
      "  [-0.17340492]]\n",
      "\n",
      " [[-0.63846237]\n",
      "  [-0.17340492]\n",
      "  [-0.13847315]]\n",
      "\n",
      " [[-0.17340492]\n",
      "  [-0.13847315]\n",
      "  [-0.68711554]]\n",
      "\n",
      " [[-0.13847315]\n",
      "  [-0.68711554]\n",
      "  [-0.52209434]]\n",
      "\n",
      " [[-0.68711554]\n",
      "  [-0.52209434]\n",
      "  [ 0.26482544]]\n",
      "\n",
      " [[-0.52209434]\n",
      "  [ 0.26482544]\n",
      "  [-0.49463898]]\n",
      "\n",
      " [[ 0.26482544]\n",
      "  [-0.49463898]\n",
      "  [-0.54081677]]\n",
      "\n",
      " [[-0.49463898]\n",
      "  [-0.54081677]\n",
      "  [-0.57764591]]\n",
      "\n",
      " [[-0.54081677]\n",
      "  [-0.57764591]\n",
      "  [-0.35216008]]\n",
      "\n",
      " [[-0.57764591]\n",
      "  [-0.35216008]\n",
      "  [ 0.97023357]]]\n",
      "[[-0.24128035]\n",
      " [-0.19131926]\n",
      " [-0.29773635]\n",
      " [-0.23669791]\n",
      " [-0.40585423]\n",
      " [-0.00168916]\n",
      " [ 0.04779262]\n",
      " [-0.20937702]\n",
      " [-0.23199639]\n",
      " [-0.47323388]\n",
      " [-0.49823526]\n",
      " [-0.27964798]\n",
      " [-0.20678869]\n",
      " [ 0.30242428]\n",
      " [ 0.34145388]\n",
      " [-0.3472874 ]\n",
      " [-0.21236399]\n",
      " [-0.22063667]\n",
      " [-0.22339058]\n",
      " [-0.28373563]\n",
      " [-0.27963215]\n",
      " [ 0.33922356]\n",
      " [ 0.10728704]\n",
      " [ 0.20584509]\n",
      " [-0.46613577]\n",
      " [-0.33843315]\n",
      " [-0.3087639 ]\n",
      " [-0.38805217]\n",
      " [-0.31187907]\n",
      " [-0.16793236]\n",
      " [-0.22520155]\n",
      " [-0.27562046]\n",
      " [-0.37059715]\n",
      " [-0.54119116]\n",
      " [ 0.02154118]\n",
      " [-0.35922885]\n",
      " [-0.27195668]\n",
      " [ 0.00921341]\n",
      " [-0.49388745]\n",
      " [-0.48611835]\n",
      " [-0.16221061]\n",
      " [-0.18693575]\n",
      " [-0.21314701]\n",
      " [-0.4475596 ]\n",
      " [-0.13755515]\n",
      " [ 0.08171818]\n",
      " [-0.32742518]\n",
      " [ 0.21176505]\n",
      " [-0.3626297 ]\n",
      " [-0.191055  ]\n",
      " [-0.2985254 ]\n",
      " [-0.31776047]\n",
      " [-0.45205414]\n",
      " [-0.02566666]\n",
      " [-0.2012591 ]\n",
      " [ 0.42808223]\n",
      " [-0.48276162]\n",
      " [-0.22006515]\n",
      " [-0.31584075]\n",
      " [-0.43789107]\n",
      " [-0.30075455]\n",
      " [-0.00213111]\n",
      " [-0.26021203]\n",
      " [-0.43958715]\n",
      " [-0.39660847]\n",
      " [-0.5892598 ]\n",
      " [ 0.05079647]\n",
      " [-0.2956648 ]\n",
      " [-0.30645338]\n",
      " [-0.39859986]\n",
      " [-0.48347187]\n",
      " [-0.24186456]\n",
      " [-0.24720171]\n",
      " [-0.4240199 ]\n",
      " [-0.38205254]\n",
      " [-0.39067537]\n",
      " [ 0.00615349]\n",
      " [-0.3650921 ]\n",
      " [-0.51817805]\n",
      " [-0.216701  ]\n",
      " [-0.31780723]\n",
      " [-0.49694026]\n",
      " [-0.33652458]\n",
      " [-0.46415114]\n",
      " [-0.2666539 ]\n",
      " [-0.27185935]\n",
      " [-0.29873863]\n",
      " [-0.39492586]\n",
      " [-0.23867616]\n",
      " [-0.19125813]\n",
      " [-0.2659384 ]\n",
      " [-0.32280737]\n",
      " [-0.34740788]\n",
      " [-0.46704227]\n",
      " [-0.35290354]\n",
      " [-0.32584408]\n",
      " [-0.22734779]\n",
      " [-0.2905209 ]\n",
      " [-0.43176377]\n",
      " [-0.28605676]\n",
      " [-0.21914   ]\n",
      " [-0.3002981 ]\n",
      " [-0.13265795]\n",
      " [-0.23645002]\n",
      " [-0.64471203]\n",
      " [-0.2613763 ]\n",
      " [-0.21947265]\n",
      " [-0.21212393]\n",
      " [-0.4225593 ]\n",
      " [-0.30587432]\n",
      " [-0.41530856]\n",
      " [-0.4467296 ]\n",
      " [-0.16467434]\n",
      " [-0.22649416]\n",
      " [-0.23914257]\n",
      " [-0.44964525]\n",
      " [ 0.02830747]\n",
      " [-0.40405408]\n",
      " [-0.3812103 ]\n",
      " [-0.266637  ]\n",
      " [-0.22094595]\n",
      " [-0.30429527]]\n"
     ]
    }
   ],
   "source": [
    "x_input = np.array(india_cases_test_X)\n",
    "print(x_input)\n",
    "yhat = model_uni.predict(x_input)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9cc9c8",
   "metadata": {},
   "source": [
    "### Univariate stacked "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "967264ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.83436764]\n",
      "  [-0.43624764]\n",
      "  [-0.10223458]]\n",
      "\n",
      " [[-0.43624764]\n",
      "  [-0.10223458]\n",
      "  [-0.47536368]]\n",
      "\n",
      " [[-0.10223458]\n",
      "  [-0.47536368]\n",
      "  [-0.21208117]]\n",
      "\n",
      " [[-0.47536368]\n",
      "  [-0.21208117]\n",
      "  [-0.52361475]]\n",
      "\n",
      " [[-0.21208117]\n",
      "  [-0.52361475]\n",
      "  [ 0.45296694]]\n",
      "\n",
      " [[-0.52361475]\n",
      "  [ 0.45296694]\n",
      "  [ 3.62374124]]\n",
      "\n",
      " [[ 0.45296694]\n",
      "  [ 3.62374124]\n",
      "  [ 0.39328136]]\n",
      "\n",
      " [[ 3.62374124]\n",
      "  [ 0.39328136]\n",
      "  [-0.12703865]]\n",
      "\n",
      " [[ 0.39328136]\n",
      "  [-0.12703865]\n",
      "  [ 2.10547838]]\n",
      "\n",
      " [[-0.12703865]\n",
      "  [ 2.10547838]\n",
      "  [-0.15062388]]\n",
      "\n",
      " [[ 2.10547838]\n",
      "  [-0.15062388]\n",
      "  [-0.45088631]]\n",
      "\n",
      " [[-0.15062388]\n",
      "  [-0.45088631]\n",
      "  [-0.18882265]]\n",
      "\n",
      " [[-0.45088631]\n",
      "  [-0.18882265]\n",
      "  [ 0.1233769 ]]\n",
      "\n",
      " [[-0.18882265]\n",
      "  [ 0.1233769 ]\n",
      "  [ 3.22197563]]\n",
      "\n",
      " [[ 0.1233769 ]\n",
      "  [ 3.22197563]\n",
      "  [-0.68799512]]\n",
      "\n",
      " [[ 3.22197563]\n",
      "  [-0.68799512]\n",
      "  [-0.38197774]]\n",
      "\n",
      " [[-0.68799512]\n",
      "  [-0.38197774]\n",
      "  [-0.29698548]]\n",
      "\n",
      " [[-0.38197774]\n",
      "  [-0.29698548]\n",
      "  [ 0.1100199 ]]\n",
      "\n",
      " [[-0.29698548]\n",
      "  [ 0.1100199 ]\n",
      "  [ 0.43194505]]\n",
      "\n",
      " [[ 0.1100199 ]\n",
      "  [ 0.43194505]\n",
      "  [-0.13437683]]\n",
      "\n",
      " [[ 0.43194505]\n",
      "  [-0.13437683]\n",
      "  [-0.13946581]]\n",
      "\n",
      " [[-0.13437683]\n",
      "  [-0.13946581]\n",
      "  [ 4.07815581]]\n",
      "\n",
      " [[-0.13946581]\n",
      "  [ 4.07815581]\n",
      "  [ 0.08362002]]\n",
      "\n",
      " [[ 4.07815581]\n",
      "  [ 0.08362002]\n",
      "  [-0.57352446]]\n",
      "\n",
      " [[ 0.08362002]\n",
      "  [-0.57352446]\n",
      "  [ 0.51495198]]\n",
      "\n",
      " [[-0.57352446]\n",
      "  [ 0.51495198]\n",
      "  [ 0.4047033 ]]\n",
      "\n",
      " [[ 0.51495198]\n",
      "  [ 0.4047033 ]\n",
      "  [-0.38705415]]\n",
      "\n",
      " [[ 0.4047033 ]\n",
      "  [-0.38705415]\n",
      "  [-0.68799512]]\n",
      "\n",
      " [[-0.38705415]\n",
      "  [-0.68799512]\n",
      "  [-0.50176355]]\n",
      "\n",
      " [[-0.68799512]\n",
      "  [-0.50176355]\n",
      "  [-0.35763859]]\n",
      "\n",
      " [[-0.50176355]\n",
      "  [-0.35763859]\n",
      "  [ 0.27290498]]\n",
      "\n",
      " [[-0.35763859]\n",
      "  [ 0.27290498]\n",
      "  [ 0.28288189]]\n",
      "\n",
      " [[ 0.27290498]\n",
      "  [ 0.28288189]\n",
      "  [-0.67178577]]\n",
      "\n",
      " [[ 0.28288189]\n",
      "  [-0.67178577]\n",
      "  [ 1.7801857 ]]\n",
      "\n",
      " [[-0.67178577]\n",
      "  [ 1.7801857 ]\n",
      "  [-0.54162095]]\n",
      "\n",
      " [[ 1.7801857 ]\n",
      "  [-0.54162095]\n",
      "  [-0.68673858]]\n",
      "\n",
      " [[-0.54162095]\n",
      "  [-0.68673858]\n",
      "  [-0.53477282]]\n",
      "\n",
      " [[-0.68673858]\n",
      "  [-0.53477282]\n",
      "  [ 0.46891241]]\n",
      "\n",
      " [[-0.53477282]\n",
      "  [ 0.46891241]\n",
      "  [-0.56373603]]\n",
      "\n",
      " [[ 0.46891241]\n",
      "  [-0.56373603]\n",
      "  [-0.65273665]]\n",
      "\n",
      " [[-0.56373603]\n",
      "  [-0.65273665]\n",
      "  [-0.30784197]]\n",
      "\n",
      " [[-0.65273665]\n",
      "  [-0.30784197]\n",
      "  [-0.11147014]]\n",
      "\n",
      " [[-0.30784197]\n",
      "  [-0.11147014]\n",
      "  [-0.66973761]]\n",
      "\n",
      " [[-0.11147014]\n",
      "  [-0.66973761]\n",
      "  [-0.6763847 ]]\n",
      "\n",
      " [[-0.66973761]\n",
      "  [-0.6763847 ]\n",
      "  [-0.45234389]]\n",
      "\n",
      " [[-0.6763847 ]\n",
      "  [-0.45234389]\n",
      "  [ 4.24376758]]\n",
      "\n",
      " [[-0.45234389]\n",
      "  [ 4.24376758]\n",
      "  [-0.66863186]]\n",
      "\n",
      " [[ 4.24376758]\n",
      "  [-0.66863186]\n",
      "  [-0.453613  ]]\n",
      "\n",
      " [[-0.66863186]\n",
      "  [-0.453613  ]\n",
      "  [-0.68799512]]\n",
      "\n",
      " [[-0.453613  ]\n",
      "  [-0.68799512]\n",
      "  [-0.03877939]]\n",
      "\n",
      " [[-0.68799512]\n",
      "  [-0.03877939]\n",
      "  [-0.27412904]]\n",
      "\n",
      " [[-0.03877939]\n",
      "  [-0.27412904]\n",
      "  [ 0.36602705]]\n",
      "\n",
      " [[-0.27412904]\n",
      "  [ 0.36602705]\n",
      "  [-0.68799512]]\n",
      "\n",
      " [[ 0.36602705]\n",
      "  [-0.68799512]\n",
      "  [ 4.49435905]]\n",
      "\n",
      " [[-0.68799512]\n",
      "  [ 4.49435905]\n",
      "  [-0.05525261]]\n",
      "\n",
      " [[ 4.49435905]\n",
      "  [-0.05525261]\n",
      "  [-0.68799512]]\n",
      "\n",
      " [[-0.05525261]\n",
      "  [-0.68799512]\n",
      "  [ 0.30166714]]\n",
      "\n",
      " [[-0.68799512]\n",
      "  [ 0.30166714]\n",
      "  [ 0.21732828]]\n",
      "\n",
      " [[ 0.30166714]\n",
      "  [ 0.21732828]\n",
      "  [-0.56695277]]\n",
      "\n",
      " [[ 0.21732828]\n",
      "  [-0.56695277]\n",
      "  [-0.6878569 ]]\n",
      "\n",
      " [[-0.56695277]\n",
      "  [-0.6878569 ]\n",
      "  [-0.62981739]]\n",
      "\n",
      " [[-0.6878569 ]\n",
      "  [-0.62981739]\n",
      "  [ 0.97520946]]\n",
      "\n",
      " [[-0.62981739]\n",
      "  [ 0.97520946]\n",
      "  [-0.66446015]]\n",
      "\n",
      " [[ 0.97520946]\n",
      "  [-0.66446015]\n",
      "  [-0.10812775]]\n",
      "\n",
      " [[-0.66446015]\n",
      "  [-0.10812775]\n",
      "  [-0.68799512]]\n",
      "\n",
      " [[-0.10812775]\n",
      "  [-0.68799512]\n",
      "  [ 1.23116634]]\n",
      "\n",
      " [[-0.68799512]\n",
      "  [ 1.23116634]\n",
      "  [ 0.19781424]]\n",
      "\n",
      " [[ 1.23116634]\n",
      "  [ 0.19781424]\n",
      "  [-0.16645626]]\n",
      "\n",
      " [[ 0.19781424]\n",
      "  [-0.16645626]\n",
      "  [ 0.29659073]]\n",
      "\n",
      " [[-0.16645626]\n",
      "  [ 0.29659073]\n",
      "  [-0.68514277]]\n",
      "\n",
      " [[ 0.29659073]\n",
      "  [-0.68514277]\n",
      "  [-0.13372343]]\n",
      "\n",
      " [[-0.68514277]\n",
      "  [-0.13372343]\n",
      "  [-0.14665321]]\n",
      "\n",
      " [[-0.13372343]\n",
      "  [-0.14665321]\n",
      "  [-0.68736685]]\n",
      "\n",
      " [[-0.14665321]\n",
      "  [-0.68736685]\n",
      "  [-0.12937581]]\n",
      "\n",
      " [[-0.68736685]\n",
      "  [-0.12937581]\n",
      "  [-0.59626781]]\n",
      "\n",
      " [[-0.12937581]\n",
      "  [-0.59626781]\n",
      "  [-0.68670088]]\n",
      "\n",
      " [[-0.59626781]\n",
      "  [-0.68670088]\n",
      "  [ 0.15387309]]\n",
      "\n",
      " [[-0.68670088]\n",
      "  [ 0.15387309]\n",
      "  [-0.6878569 ]]\n",
      "\n",
      " [[ 0.15387309]\n",
      "  [-0.6878569 ]\n",
      "  [ 0.43919528]]\n",
      "\n",
      " [[-0.6878569 ]\n",
      "  [ 0.43919528]\n",
      "  [-0.05922327]]\n",
      "\n",
      " [[ 0.43919528]\n",
      "  [-0.05922327]\n",
      "  [-0.68798255]]\n",
      "\n",
      " [[-0.05922327]\n",
      "  [-0.68798255]\n",
      "  [ 0.38461125]]\n",
      "\n",
      " [[-0.68798255]\n",
      "  [ 0.38461125]\n",
      "  [-0.47506211]]\n",
      "\n",
      " [[ 0.38461125]\n",
      "  [-0.47506211]\n",
      "  [ 0.91421708]]\n",
      "\n",
      " [[-0.47506211]\n",
      "  [ 0.91421708]\n",
      "  [ 0.25545166]]\n",
      "\n",
      " [[ 0.91421708]\n",
      "  [ 0.25545166]\n",
      "  [-0.25193857]]\n",
      "\n",
      " [[ 0.25545166]\n",
      "  [-0.25193857]\n",
      "  [-0.64267177]]\n",
      "\n",
      " [[-0.25193857]\n",
      "  [-0.64267177]\n",
      "  [-0.38630023]]\n",
      "\n",
      " [[-0.64267177]\n",
      "  [-0.38630023]\n",
      "  [-0.31494141]]\n",
      "\n",
      " [[-0.38630023]\n",
      "  [-0.31494141]\n",
      "  [-0.16465941]]\n",
      "\n",
      " [[-0.31494141]\n",
      "  [-0.16465941]\n",
      "  [ 0.57653493]]\n",
      "\n",
      " [[-0.16465941]\n",
      "  [ 0.57653493]\n",
      "  [ 0.34450254]]\n",
      "\n",
      " [[ 0.57653493]\n",
      "  [ 0.34450254]\n",
      "  [-0.665968  ]]\n",
      "\n",
      " [[ 0.34450254]\n",
      "  [-0.665968  ]\n",
      "  [-0.02819933]]\n",
      "\n",
      " [[-0.665968  ]\n",
      "  [-0.02819933]\n",
      "  [-0.49770493]]\n",
      "\n",
      " [[-0.02819933]\n",
      "  [-0.49770493]\n",
      "  [-0.53810264]]\n",
      "\n",
      " [[-0.49770493]\n",
      "  [-0.53810264]\n",
      "  [-0.38219135]]\n",
      "\n",
      " [[-0.53810264]\n",
      "  [-0.38219135]\n",
      "  [ 3.26562778]]\n",
      "\n",
      " [[-0.38219135]\n",
      "  [ 3.26562778]\n",
      "  [-0.4600716 ]]\n",
      "\n",
      " [[ 3.26562778]\n",
      "  [-0.4600716 ]\n",
      "  [-0.39814939]]\n",
      "\n",
      " [[-0.4600716 ]\n",
      "  [-0.39814939]\n",
      "  [ 0.18746036]]\n",
      "\n",
      " [[-0.39814939]\n",
      "  [ 0.18746036]\n",
      "  [-0.29366822]]\n",
      "\n",
      " [[ 0.18746036]\n",
      "  [-0.29366822]\n",
      "  [ 2.75243234]]\n",
      "\n",
      " [[-0.29366822]\n",
      "  [ 2.75243234]\n",
      "  [-0.6437147 ]]\n",
      "\n",
      " [[ 2.75243234]\n",
      "  [-0.6437147 ]\n",
      "  [-0.20212939]]\n",
      "\n",
      " [[-0.6437147 ]\n",
      "  [-0.20212939]\n",
      "  [-0.30132053]]\n",
      "\n",
      " [[-0.20212939]\n",
      "  [-0.30132053]\n",
      "  [-0.10707225]]\n",
      "\n",
      " [[-0.30132053]\n",
      "  [-0.10707225]\n",
      "  [-0.68799512]]\n",
      "\n",
      " [[-0.10707225]\n",
      "  [-0.68799512]\n",
      "  [-0.01599834]]\n",
      "\n",
      " [[-0.68799512]\n",
      "  [-0.01599834]\n",
      "  [ 0.39888553]]\n",
      "\n",
      " [[-0.01599834]\n",
      "  [ 0.39888553]\n",
      "  [-0.5261404 ]]\n",
      "\n",
      " [[ 0.39888553]\n",
      "  [-0.5261404 ]\n",
      "  [-0.63846237]]\n",
      "\n",
      " [[-0.5261404 ]\n",
      "  [-0.63846237]\n",
      "  [-0.17340492]]\n",
      "\n",
      " [[-0.63846237]\n",
      "  [-0.17340492]\n",
      "  [-0.13847315]]\n",
      "\n",
      " [[-0.17340492]\n",
      "  [-0.13847315]\n",
      "  [-0.68711554]]\n",
      "\n",
      " [[-0.13847315]\n",
      "  [-0.68711554]\n",
      "  [-0.52209434]]\n",
      "\n",
      " [[-0.68711554]\n",
      "  [-0.52209434]\n",
      "  [ 0.26482544]]\n",
      "\n",
      " [[-0.52209434]\n",
      "  [ 0.26482544]\n",
      "  [-0.49463898]]\n",
      "\n",
      " [[ 0.26482544]\n",
      "  [-0.49463898]\n",
      "  [-0.54081677]]\n",
      "\n",
      " [[-0.49463898]\n",
      "  [-0.54081677]\n",
      "  [-0.57764591]]\n",
      "\n",
      " [[-0.54081677]\n",
      "  [-0.57764591]\n",
      "  [-0.35216008]]\n",
      "\n",
      " [[-0.57764591]\n",
      "  [-0.35216008]\n",
      "  [ 0.97023357]]]\n",
      "[[-0.24128035]\n",
      " [-0.19131926]\n",
      " [-0.29773635]\n",
      " [-0.23669791]\n",
      " [-0.40585423]\n",
      " [-0.00168916]\n",
      " [ 0.04779262]\n",
      " [-0.20937702]\n",
      " [-0.23199639]\n",
      " [-0.47323388]\n",
      " [-0.49823526]\n",
      " [-0.27964798]\n",
      " [-0.20678869]\n",
      " [ 0.30242428]\n",
      " [ 0.34145388]\n",
      " [-0.3472874 ]\n",
      " [-0.21236399]\n",
      " [-0.22063667]\n",
      " [-0.22339058]\n",
      " [-0.28373563]\n",
      " [-0.27963215]\n",
      " [ 0.33922356]\n",
      " [ 0.10728704]\n",
      " [ 0.20584509]\n",
      " [-0.46613577]\n",
      " [-0.33843315]\n",
      " [-0.3087639 ]\n",
      " [-0.38805217]\n",
      " [-0.31187907]\n",
      " [-0.16793236]\n",
      " [-0.22520155]\n",
      " [-0.27562046]\n",
      " [-0.37059715]\n",
      " [-0.54119116]\n",
      " [ 0.02154118]\n",
      " [-0.35922885]\n",
      " [-0.27195668]\n",
      " [ 0.00921341]\n",
      " [-0.49388745]\n",
      " [-0.48611835]\n",
      " [-0.16221061]\n",
      " [-0.18693575]\n",
      " [-0.21314701]\n",
      " [-0.4475596 ]\n",
      " [-0.13755515]\n",
      " [ 0.08171818]\n",
      " [-0.32742518]\n",
      " [ 0.21176505]\n",
      " [-0.3626297 ]\n",
      " [-0.191055  ]\n",
      " [-0.2985254 ]\n",
      " [-0.31776047]\n",
      " [-0.45205414]\n",
      " [-0.02566666]\n",
      " [-0.2012591 ]\n",
      " [ 0.42808223]\n",
      " [-0.48276162]\n",
      " [-0.22006515]\n",
      " [-0.31584075]\n",
      " [-0.43789107]\n",
      " [-0.30075455]\n",
      " [-0.00213111]\n",
      " [-0.26021203]\n",
      " [-0.43958715]\n",
      " [-0.39660847]\n",
      " [-0.5892598 ]\n",
      " [ 0.05079647]\n",
      " [-0.2956648 ]\n",
      " [-0.30645338]\n",
      " [-0.39859986]\n",
      " [-0.48347187]\n",
      " [-0.24186456]\n",
      " [-0.24720171]\n",
      " [-0.4240199 ]\n",
      " [-0.38205254]\n",
      " [-0.39067537]\n",
      " [ 0.00615349]\n",
      " [-0.3650921 ]\n",
      " [-0.51817805]\n",
      " [-0.216701  ]\n",
      " [-0.31780723]\n",
      " [-0.49694026]\n",
      " [-0.33652458]\n",
      " [-0.46415114]\n",
      " [-0.2666539 ]\n",
      " [-0.27185935]\n",
      " [-0.29873863]\n",
      " [-0.39492586]\n",
      " [-0.23867616]\n",
      " [-0.19125813]\n",
      " [-0.2659384 ]\n",
      " [-0.32280737]\n",
      " [-0.34740788]\n",
      " [-0.46704227]\n",
      " [-0.35290354]\n",
      " [-0.32584408]\n",
      " [-0.22734779]\n",
      " [-0.2905209 ]\n",
      " [-0.43176377]\n",
      " [-0.28605676]\n",
      " [-0.21914   ]\n",
      " [-0.3002981 ]\n",
      " [-0.13265795]\n",
      " [-0.23645002]\n",
      " [-0.64471203]\n",
      " [-0.2613763 ]\n",
      " [-0.21947265]\n",
      " [-0.21212393]\n",
      " [-0.4225593 ]\n",
      " [-0.30587432]\n",
      " [-0.41530856]\n",
      " [-0.4467296 ]\n",
      " [-0.16467434]\n",
      " [-0.22649416]\n",
      " [-0.23914257]\n",
      " [-0.44964525]\n",
      " [ 0.02830747]\n",
      " [-0.40405408]\n",
      " [-0.3812103 ]\n",
      " [-0.266637  ]\n",
      " [-0.22094595]\n",
      " [-0.30429527]]\n"
     ]
    }
   ],
   "source": [
    "x_input = np.array(india_cases_test_X)\n",
    "print(x_input)\n",
    "yhat_stacked = model_uni_stacked.predict(x_input)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf59990c",
   "metadata": {},
   "source": [
    "<a name=evaluate></a>\n",
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f86c583",
   "metadata": {},
   "source": [
    "### Univariate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "86cb9bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129.73888"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape = MeanAbsolutePercentageError()\n",
    "mape(india_cases_test_y, yhat).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9cc9c8",
   "metadata": {},
   "source": [
    "### Univariate stacked "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f7d51527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130.9539"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(india_cases_test_y, yhat_stacked).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fe9dec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
